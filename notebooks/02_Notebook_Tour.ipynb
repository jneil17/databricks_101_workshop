{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fa3998",
   "metadata": {},
   "source": [
    "# ğŸï¸ Databricks Notebook Tour: Build F1 Data Pipeline\n",
    "*Create a complete medallion architecture in 15 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What We'll Build\n",
    "\n",
    "**Complete Formula 1 Data Lakehouse:**\n",
    "```\n",
    "ğŸ“ Volume (Raw Files)    â†’    ğŸ¥‰ Bronze (Raw Tables)    â†’    ğŸ¥ˆ Silver (Clean Tables)    â†’    ğŸ¥‡ Gold (Analytics)\n",
    "â”œâ”€â”€ races.csv                 â”œâ”€â”€ bronze_races              â”œâ”€â”€ silver_races               â”œâ”€â”€ gold_driver_standings\n",
    "â”œâ”€â”€ drivers.csv               â”œâ”€â”€ bronze_drivers            â”œâ”€â”€ silver_drivers             â””â”€â”€ gold_season_stats  \n",
    "â””â”€â”€ results.csv               â””â”€â”€ bronze_results            â””â”€â”€ silver_results\n",
    "```\n",
    "\n",
    "**ğŸ”¥ Key Features:**\n",
    "- âš¡ **Serverless compute** (no cluster management)\n",
    "- ğŸ“ **Volumes** for file storage (no DBFS)\n",
    "- ğŸ”„ **COPY INTO** for production-ready ingestion\n",
    "- ğŸ **Python + SQL** multi-language development\n",
    "- ğŸ“Š **8 tables** across medallion layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c767dd3",
   "metadata": {},
   "source": [
    "## âš¡ Step 1: Serverless Compute Setup\n",
    "\n",
    "**ğŸ“Œ IMPORTANT:** Make sure you're using **Serverless compute** for this workshop!\n",
    "\n",
    "### How to Verify Serverless Compute:\n",
    "1. Look at the top-right of this notebook\n",
    "2. You should see \"Serverless\" in the compute dropdown\n",
    "3. If not, click the dropdown and select \"Serverless\"\n",
    "\n",
    "### Why Serverless?\n",
    "- âœ… **No cluster management** - starts instantly\n",
    "- âœ… **Auto-scaling** - handles any workload size\n",
    "- âœ… **Cost efficient** - pay per second of actual usage\n",
    "- âœ… **Always up-to-date** - latest Databricks runtime\n",
    "\n",
    "*ğŸ¯ Once you see \"Serverless\" in the compute dropdown, continue to the next cell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759e2a",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Step 2: Multi-Language Demo\n",
    "\n",
    "One of Databricks' superpowers is **seamless multi-language support**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf186aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python cell - let's start with some basic info\n",
    "print(\"ğŸï¸ Welcome to the F1 Data Pipeline!\")\n",
    "print(\"ğŸŒ This workspace supports multiple languages seamlessly\")\n",
    "\n",
    "# Check current compute and workspace info\n",
    "print(\"\\nâš¡ Compute Information:\")\n",
    "print(f\"ğŸ“Š Spark version: {spark.version}\")\n",
    "print(f\"ğŸ Python kernel ready for F1 analysis!\")\n",
    "\n",
    "# Quick test that everything works\n",
    "test_data = [(\"Lewis Hamilton\", \"Mercedes\", 103), (\"Max Verstappen\", \"Red Bull\", 56)]\n",
    "print(f\"\\nğŸ Quick test with sample F1 data: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cell - let's check our available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"ğŸ“‹ Available catalogs in your workspace\")\n",
    "print(\"ğŸ’¡ We'll create our F1 tables in the 'main' catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a53415",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 3: Create Volume for F1 Data\n",
    "\n",
    "**Volumes** are Databricks' modern approach to file storage (replacing DBFS). Let's create a volume for our F1 data files:\n",
    "\n",
    "### Why Volumes?\n",
    "- âœ… **Unity Catalog integration** - governance and lineage\n",
    "- âœ… **Better performance** than traditional file systems\n",
    "- âœ… **Cross-cloud compatibility** (AWS, Azure, GCP)\n",
    "- âœ… **Production ready** with security controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for F1 data storage\n",
    "volume_name = \"f1_data_volume\"\n",
    "\n",
    "try:\n",
    "    # Create the volume (this will be our file storage)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS main.default.{volume_name}\")\n",
    "    print(f\"âœ… Volume '{volume_name}' created successfully!\")\n",
    "    \n",
    "    # Show volume info\n",
    "    spark.sql(f\"DESCRIBE VOLUME main.default.{volume_name}\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Volume creation note: {e}\")\n",
    "    print(\"ğŸ’¡ The volume may already exist or you may need admin permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available volumes to confirm our setup\n",
    "spark.sql(\"SHOW VOLUMES IN main.default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc58290",
   "metadata": {},
   "source": [
    "## ğŸŒ Step 4: Download F1 Data from GitHub\n",
    "\n",
    "Now let's download real Formula 1 data from GitHub. We'll use production-ready techniques for data ingestion:\n",
    "\n",
    "### Data Source\n",
    "- **Repository:** `plotly/datasets`\n",
    "- **Files:** `formula1_drivers.csv`, `formula1_race_results.csv`, `formula1_races.csv`\n",
    "- **Coverage:** Historical F1 data from 1950-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d818991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define F1 data URLs from GitHub\n",
    "f1_data_urls = {\n",
    "    \"drivers\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_drivers.csv\",\n",
    "    \"results\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_race_results.csv\", \n",
    "    \"races\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_races.csv\"\n",
    "}\n",
    "\n",
    "# Volume path for storing our files\n",
    "volume_path = \"/Volumes/main/default/f1_data_volume\"\n",
    "\n",
    "print(\"ğŸï¸ Downloading F1 data from GitHub...\")\n",
    "\n",
    "for file_name, url in f1_data_urls.items():\n",
    "    try:\n",
    "        print(f\"\\nğŸ“¥ Downloading {file_name}.csv...\")\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to volume\n",
    "        file_path = f\"{volume_path}/{file_name}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Write file to volume\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"âœ… {file_name}.csv saved to {file_path}\")\n",
    "        \n",
    "        # Quick peek at file size\n",
    "        file_size = len(response.text)\n",
    "        print(f\"ğŸ“Š File size: {file_size:,} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {file_name}: {e}\")\n",
    "        print(f\"ğŸ’¡ You may need to manually upload the files to the volume\")\n",
    "\n",
    "print(\"\\nğŸ‰ F1 data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0261d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our files are in the volume\n",
    "dbutils.fs.ls(\"/Volumes/main/default/f1_data_volume/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500e36e",
   "metadata": {},
   "source": [
    "## ğŸ¥‰ Step 5: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The **Bronze layer** stores raw data exactly as received. Let's create our bronze tables using modern **COPY INTO** commands:\n",
    "\n",
    "### Bronze Layer Benefits:\n",
    "- âœ… **Preserves original data** for auditing\n",
    "- âœ… **Handles schema evolution** automatically\n",
    "- âœ… **Production-ready** error handling\n",
    "- âœ… **Efficient incremental loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4016e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Layer: Create tables from CSV files using COPY INTO\n",
    "bronze_tables = [\"drivers\", \"races\", \"results\"]\n",
    "\n",
    "for table_name in bronze_tables:\n",
    "    try:\n",
    "        print(f\"\\nğŸ¥‰ Creating bronze_{table_name} table...\")\n",
    "        \n",
    "        # Drop table if exists (for demo purposes)\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS main.default.bronze_{table_name}\")\n",
    "        \n",
    "        # Create bronze table with COPY INTO (production approach)\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE main.default.bronze_{table_name}\n",
    "        USING DELTA\n",
    "        AS SELECT * FROM \n",
    "        read_files('/Volumes/main/default/f1_data_volume/{table_name}.csv',\n",
    "                   format => 'csv',\n",
    "                   header => true,\n",
    "                   inferSchema => true)\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(create_table_sql)\n",
    "        \n",
    "        # Show table info\n",
    "        count = spark.table(f\"main.default.bronze_{table_name}\").count()\n",
    "        print(f\"âœ… bronze_{table_name} created with {count:,} rows\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"ğŸ“Š Sample data from bronze_{table_name}:\")\n",
    "        spark.table(f\"main.default.bronze_{table_name}\").limit(3).show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating bronze_{table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a19e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality check for Bronze layer\n",
    "print(\"ğŸ” Bronze Layer Data Quality Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for table_name in [\"drivers\", \"races\", \"results\"]:\n",
    "    df = spark.table(f\"main.default.bronze_{table_name}\")\n",
    "    print(f\"\\nğŸ“Š bronze_{table_name}:\")\n",
    "    print(f\"   Rows: {df.count():,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4a5d7",
   "metadata": {},
   "source": [
    "## ğŸ¥ˆ Step 6: Silver Layer - Cleaned & Validated Data\n",
    "\n",
    "The **Silver layer** contains cleaned, validated, and conformed data. Let's transform our F1 data:\n",
    "\n",
    "### Silver Layer Transformations:\n",
    "- âœ… **Data type corrections** (strings â†’ dates, numbers)\n",
    "- âœ… **Column renaming** for consistency\n",
    "- âœ… **Data quality filters** (remove nulls, invalid values)\n",
    "- âœ… **Schema standardization** across related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 drivers data\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"ğŸ¥ˆ Creating silver_drivers table...\")\n",
    "\n",
    "# Transform bronze_drivers to silver_drivers\n",
    "silver_drivers_df = (\n",
    "    spark.table(\"main.default.bronze_drivers\")\n",
    "    .select(\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"forename\").alias(\"first_name\"),\n",
    "        col(\"surname\").alias(\"last_name\"),\n",
    "        concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"full_name\"),\n",
    "        col(\"nationality\"),\n",
    "        to_date(col(\"dob\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"driver_id\").isNotNull())  # Data quality: remove invalid drivers\n",
    "    .filter(col(\"nationality\").isNotNull())  # Data quality: must have nationality\n",
    ")\n",
    "\n",
    "# Write to Silver table\n",
    "silver_drivers_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_drivers\")\n",
    "\n",
    "print(f\"âœ… silver_drivers created with {silver_drivers_df.count():,} rows\")\n",
    "silver_drivers_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 races data\n",
    "print(\"ğŸ¥ˆ Creating silver_races table...\")\n",
    "\n",
    "silver_races_df = (\n",
    "    spark.table(\"main.default.bronze_races\")\n",
    "    .select(\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"year\").cast(\"integer\").alias(\"race_year\"),\n",
    "        col(\"round\").cast(\"integer\").alias(\"round_number\"),\n",
    "        col(\"name\").alias(\"race_name\"),\n",
    "        col(\"date\").alias(\"race_date\"),\n",
    "        col(\"circuitId\").cast(\"integer\").alias(\"circuit_id\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"race_year\").between(1950, 2024))  # Data quality: reasonable year range\n",
    ")\n",
    "\n",
    "silver_races_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_races\")\n",
    "\n",
    "print(f\"âœ… silver_races created with {silver_races_df.count():,} rows\")\n",
    "silver_races_df.limit(5).show(truncate=False)\n",
    "\n",
    "# Silver Layer: Clean and validate F1 results data  \n",
    "print(\"ğŸ¥ˆ Creating silver_results table...\")\n",
    "\n",
    "silver_results_df = (\n",
    "    spark.table(\"main.default.bronze_results\")\n",
    "    .select(\n",
    "        col(\"resultId\").cast(\"integer\").alias(\"result_id\"),\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"constructorId\").cast(\"integer\").alias(\"constructor_id\"),\n",
    "        col(\"positionOrder\").cast(\"integer\").alias(\"finish_position\"),\n",
    "        col(\"points\").cast(\"double\").alias(\"points_earned\"),\n",
    "        col(\"laps\").cast(\"integer\").alias(\"laps_completed\"),\n",
    "        when(col(\"positionOrder\") == 1, True).otherwise(False).alias(\"race_winner\"),\n",
    "        when(col(\"positionOrder\") <= 3, True).otherwise(False).alias(\"podium_finish\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"result_id\").isNotNull())\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"driver_id\").isNotNull())\n",
    ")\n",
    "\n",
    "silver_results_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_results\")\n",
    "\n",
    "print(f\"âœ… silver_results created with {silver_results_df.count():,} rows\")\n",
    "silver_results_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd5f7d",
   "metadata": {},
   "source": [
    "## ğŸ¥‡ Step 7: Gold Layer - Analytics-Ready Business Data\n",
    "\n",
    "The **Gold layer** contains aggregated, business-ready data optimized for analytics and reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create driver performance analytics table\n",
    "print(\"ğŸ¥‡ Creating gold_driver_standings table...\")\n",
    "\n",
    "# Create comprehensive driver performance metrics\n",
    "gold_driver_standings = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_drivers\").alias(\"d\"), \"driver_id\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\n",
    "        col(\"d.driver_id\"),\n",
    "        col(\"d.full_name\"),\n",
    "        col(\"d.nationality\"),\n",
    "        col(\"d.first_name\"),\n",
    "        col(\"d.last_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_races\"),\n",
    "        sum(\"points_earned\").alias(\"career_points\"),\n",
    "        sum(when(col(\"race_winner\"), 1).otherwise(0)).alias(\"wins\"),\n",
    "        sum(when(col(\"podium_finish\"), 1).otherwise(0)).alias(\"podiums\"),\n",
    "        min(\"race_year\").alias(\"career_start\"),\n",
    "        max(\"race_year\").alias(\"career_end\"),\n",
    "        avg(\"finish_position\").alias(\"avg_finish_position\"),\n",
    "        avg(\"points_earned\").alias(\"points_per_race\")\n",
    "    )\n",
    "    .withColumn(\"career_length\", col(\"career_end\") - col(\"career_start\") + 1)\n",
    "    .withColumn(\"win_percentage\", round(col(\"wins\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"podium_percentage\", round(col(\"podiums\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .filter(col(\"total_races\") >= 5)  # Focus on drivers with meaningful careers\n",
    "    .orderBy(col(\"career_points\").desc())\n",
    ")\n",
    "\n",
    "gold_driver_standings.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_driver_standings\")\n",
    "\n",
    "print(f\"âœ… gold_driver_standings created with {gold_driver_standings.count():,} drivers\")\n",
    "gold_driver_standings.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb26345",
   "metadata": {},
   "source": [
    "## ğŸ† Step 8: Gold Layer - Season Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create season-level analytics\n",
    "print(\"ğŸ¥‡ Creating gold_season_stats table...\")\n",
    "\n",
    "gold_season_stats = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\"race_year\")\n",
    "    .agg(\n",
    "        countDistinct(\"driver_id\").alias(\"unique_drivers\"),\n",
    "        countDistinct(\"constructor_id\").alias(\"unique_constructors\"),\n",
    "        count(\"*\").alias(\"total_race_entries\"),\n",
    "        countDistinct(\"race_id\").alias(\"races_in_season\"),\n",
    "        sum(\"points_earned\").alias(\"total_points_awarded\"),\n",
    "        avg(\"laps_completed\").alias(\"avg_laps_per_race\"),\n",
    "        (count(\"*\") - count(when(col(\"finish_position\").isNull(), 1))) / count(\"*\") * 100).alias(\"completion_rate\")\n",
    "    )\n",
    "    .withColumn(\"avg_drivers_per_race\", round(col(\"total_race_entries\") / col(\"races_in_season\"), 1))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .orderBy(\"race_year\")\n",
    ")\n",
    "\n",
    "gold_season_stats.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_season_stats\")\n",
    "\n",
    "print(f\"âœ… gold_season_stats created with {gold_season_stats.count():,} seasons\")\n",
    "gold_season_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09caa0",
   "metadata": {},
   "source": [
    "## âœ… Mission Accomplished! ğŸ‰\n",
    "\n",
    "**Congratulations! You've built a complete F1 data lakehouse in 15 minutes!**\n",
    "\n",
    "### What You've Created:\n",
    "- ğŸ“ **Volume storage** for modern file management  \n",
    "- ğŸ¥‰ **Bronze layer** - 3 raw data tables (drivers, races, results)\n",
    "- ğŸ¥ˆ **Silver layer** - 3 cleaned & validated tables\n",
    "- ğŸ¥‡ **Gold layer** - 2 analytics-ready business tables\n",
    "\n",
    "### Your F1 Data Lakehouse:\n",
    "```\n",
    "ğŸ—ï¸ Architecture Complete:\n",
    "   â””â”€â”€ main.default (catalog.schema)\n",
    "       â”œâ”€â”€ ğŸ“ f1_data_volume/ (file storage)\n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_drivers (1,500+ drivers)\n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_races (5,000+ races) \n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_results (100,000+ results)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_drivers (cleaned driver data)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_races (validated race data)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_results (processed results)\n",
    "       â”œâ”€â”€ ğŸ¥‡ gold_driver_standings (career analytics)\n",
    "       â””â”€â”€ ğŸ¥‡ gold_season_stats (seasonal insights)\n",
    "```\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "- **Explore Unity Catalog** - data lineage and governance\n",
    "- **Create Jobs** - automate your pipeline\n",
    "- **Build Dashboards** - visualize your F1 insights\n",
    "- **Try Genie** - ask questions in natural language\n",
    "- **Experiment with AI** - generate insights automatically\n",
    "\n",
    "### ğŸ’¡ Key Takeaways:\n",
    "- âœ… **Serverless** - No infrastructure to manage\n",
    "- âœ… **Volumes** - Modern file storage with governance\n",
    "- âœ… **Multi-language** - Python + SQL seamlessly\n",
    "- âœ… **Delta Lake** - ACID transactions and time travel\n",
    "- âœ… **Medallion architecture** - Production-ready data organization\n",
    "\n",
    "**ğŸ Ready to dive deeper into the world of data + AI? Let's go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a3173",
   "metadata": {},
   "source": [
    "# ğŸï¸ Databricks Notebook Tour: Build F1 Data Pipeline\n",
    "*Create a complete medallion architecture in 15 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What We'll Build\n",
    "\n",
    "**Complete Formula 1 Data Lakehouse:**\n",
    "```\n",
    "ğŸ“ Volume (Raw Files)    â†’    ğŸ¥‰ Bronze (Raw Tables)    â†’    ğŸ¥ˆ Silver (Clean Tables)    â†’    ğŸ¥‡ Gold (Analytics)\n",
    "â”œâ”€â”€ races.csv                 â”œâ”€â”€ bronze_races              â”œâ”€â”€ silver_races               â”œâ”€â”€ gold_driver_standings\n",
    "â”œâ”€â”€ drivers.csv               â”œâ”€â”€ bronze_drivers            â”œâ”€â”€ silver_drivers             â””â”€â”€ gold_season_stats  \n",
    "â””â”€â”€ results.csv               â””â”€â”€ bronze_results            â””â”€â”€ silver_results\n",
    "```\n",
    "\n",
    "**ğŸ”¥ Key Features:**\n",
    "- âš¡ **Serverless compute** (no cluster management)\n",
    "- ğŸ“ **Volumes** for file storage (no DBFS)\n",
    "- ğŸ”„ **COPY INTO** for production-ready ingestion\n",
    "- ğŸ **Python + SQL** multi-language development\n",
    "- ğŸ“Š **8 tables** across medallion layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c63b5",
   "metadata": {},
   "source": [
    "## âš¡ Step 1: Serverless Compute Setup\n",
    "\n",
    "**ğŸ“Œ IMPORTANT:** Make sure you're using **Serverless compute** for this workshop!\n",
    "\n",
    "### How to Verify Serverless Compute:\n",
    "1. Look at the top-right of this notebook\n",
    "2. You should see \"Serverless\" in the compute dropdown\n",
    "3. If not, click the dropdown and select \"Serverless\"\n",
    "\n",
    "### Why Serverless?\n",
    "- âœ… **No cluster management** - starts instantly\n",
    "- âœ… **Auto-scaling** - handles any workload size\n",
    "- âœ… **Cost efficient** - pay per second of actual usage\n",
    "- âœ… **Always up-to-date** - latest Databricks runtime\n",
    "\n",
    "*ğŸ¯ Once you see \"Serverless\" in the compute dropdown, continue to the next cell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ef29",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Step 2: Multi-Language Demo\n",
    "\n",
    "One of Databricks' superpowers is **seamless multi-language support**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python cell - let's start with some basic info\n",
    "print(\"ğŸï¸ Welcome to the F1 Data Pipeline!\")\n",
    "print(\"ğŸŒ This workspace supports multiple languages seamlessly\")\n",
    "\n",
    "# Check current compute and workspace info\n",
    "print(\"\\nâš¡ Compute Information:\")\n",
    "print(f\"ğŸ“Š Spark version: {spark.version}\")\n",
    "print(f\"ğŸ Python kernel ready for F1 analysis!\")\n",
    "\n",
    "# Quick test that everything works\n",
    "test_data = [(\"Lewis Hamilton\", \"Mercedes\", 103), (\"Max Verstappen\", \"Red Bull\", 56)]\n",
    "print(f\"\\nğŸ Quick test with sample F1 data: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551db712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cell - let's check our available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"ğŸ“‹ Available catalogs in your workspace\")\n",
    "print(\"ğŸ’¡ We'll create our F1 tables in the 'main' catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b5358",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 3: Create Volume for F1 Data\n",
    "\n",
    "**Volumes** are Databricks' modern approach to file storage (replacing DBFS). Let's create a volume for our F1 data files:\n",
    "\n",
    "### Why Volumes?\n",
    "- âœ… **Unity Catalog integration** - governance and lineage\n",
    "- âœ… **Better performance** than traditional file systems\n",
    "- âœ… **Cross-cloud compatibility** (AWS, Azure, GCP)\n",
    "- âœ… **Production ready** with security controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184973cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for F1 data storage\n",
    "volume_name = \"f1_data_volume\"\n",
    "\n",
    "try:\n",
    "    # Create the volume (this will be our file storage)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS main.default.{volume_name}\")\n",
    "    print(f\"âœ… Volume '{volume_name}' created successfully!\")\n",
    "    \n",
    "    # Show volume info\n",
    "    spark.sql(f\"DESCRIBE VOLUME main.default.{volume_name}\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Volume creation note: {e}\")\n",
    "    print(\"ğŸ’¡ The volume may already exist or you may need admin permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available volumes to confirm our setup\n",
    "spark.sql(\"SHOW VOLUMES IN main.default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44639b4b",
   "metadata": {},
   "source": [
    "## ğŸŒ Step 4: Download F1 Data from GitHub\n",
    "\n",
    "Now let's download real Formula 1 data from GitHub. We'll use production-ready techniques for data ingestion:\n",
    "\n",
    "### Data Source\n",
    "- **Repository:** `plotly/datasets`\n",
    "- **Files:** `formula1_drivers.csv`, `formula1_race_results.csv`, `formula1_races.csv`\n",
    "- **Coverage:** Historical F1 data from 1950-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52330b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define F1 data URLs from GitHub\n",
    "f1_data_urls = {\n",
    "    \"drivers\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_drivers.csv\",\n",
    "    \"results\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_race_results.csv\", \n",
    "    \"races\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_races.csv\"\n",
    "}\n",
    "\n",
    "# Volume path for storing our files\n",
    "volume_path = \"/Volumes/main/default/f1_data_volume\"\n",
    "\n",
    "print(\"ğŸï¸ Downloading F1 data from GitHub...\")\n",
    "\n",
    "for file_name, url in f1_data_urls.items():\n",
    "    try:\n",
    "        print(f\"\\nğŸ“¥ Downloading {file_name}.csv...\")\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to volume\n",
    "        file_path = f\"{volume_path}/{file_name}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Write file to volume\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"âœ… {file_name}.csv saved to {file_path}\")\n",
    "        \n",
    "        # Quick peek at file size\n",
    "        file_size = len(response.text)\n",
    "        print(f\"ğŸ“Š File size: {file_size:,} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {file_name}: {e}\")\n",
    "        print(f\"ğŸ’¡ You may need to manually upload the files to the volume\")\n",
    "\n",
    "print(\"\\nğŸ‰ F1 data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32209b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our files are in the volume\n",
    "dbutils.fs.ls(\"/Volumes/main/default/f1_data_volume/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9d98c",
   "metadata": {},
   "source": [
    "## ğŸ¥‰ Step 5: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The **Bronze layer** stores raw data exactly as received. Let's create our bronze tables using modern **COPY INTO** commands:\n",
    "\n",
    "### Bronze Layer Benefits:\n",
    "- âœ… **Preserves original data** for auditing\n",
    "- âœ… **Handles schema evolution** automatically\n",
    "- âœ… **Production-ready** error handling\n",
    "- âœ… **Efficient incremental loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Layer: Create tables from CSV files using COPY INTO\n",
    "bronze_tables = [\"drivers\", \"races\", \"results\"]\n",
    "\n",
    "for table_name in bronze_tables:\n",
    "    try:\n",
    "        print(f\"\\nğŸ¥‰ Creating bronze_{table_name} table...\")\n",
    "        \n",
    "        # Drop table if exists (for demo purposes)\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS main.default.bronze_{table_name}\")\n",
    "        \n",
    "        # Create bronze table with COPY INTO (production approach)\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE main.default.bronze_{table_name}\n",
    "        USING DELTA\n",
    "        AS SELECT * FROM \n",
    "        read_files('/Volumes/main/default/f1_data_volume/{table_name}.csv',\n",
    "                   format => 'csv',\n",
    "                   header => true,\n",
    "                   inferSchema => true)\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(create_table_sql)\n",
    "        \n",
    "        # Show table info\n",
    "        count = spark.table(f\"main.default.bronze_{table_name}\").count()\n",
    "        print(f\"âœ… bronze_{table_name} created with {count:,} rows\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"ğŸ“Š Sample data from bronze_{table_name}:\")\n",
    "        spark.table(f\"main.default.bronze_{table_name}\").limit(3).show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating bronze_{table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc76190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality check for Bronze layer\n",
    "print(\"ğŸ” Bronze Layer Data Quality Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for table_name in [\"drivers\", \"races\", \"results\"]:\n",
    "    df = spark.table(f\"main.default.bronze_{table_name}\")\n",
    "    print(f\"\\nğŸ“Š bronze_{table_name}:\")\n",
    "    print(f\"   Rows: {df.count():,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56259aa",
   "metadata": {},
   "source": [
    "## ğŸ¥ˆ Step 6: Silver Layer - Cleaned & Validated Data\n",
    "\n",
    "The **Silver layer** contains cleaned, validated, and conformed data. Let's transform our F1 data:\n",
    "\n",
    "### Silver Layer Transformations:\n",
    "- âœ… **Data type corrections** (strings â†’ dates, numbers)\n",
    "- âœ… **Column renaming** for consistency\n",
    "- âœ… **Data quality filters** (remove nulls, invalid values)\n",
    "- âœ… **Schema standardization** across related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 drivers data\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"ğŸ¥ˆ Creating silver_drivers table...\")\n",
    "\n",
    "# Transform bronze_drivers to silver_drivers\n",
    "silver_drivers_df = (\n",
    "    spark.table(\"main.default.bronze_drivers\")\n",
    "    .select(\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"forename\").alias(\"first_name\"),\n",
    "        col(\"surname\").alias(\"last_name\"),\n",
    "        concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"full_name\"),\n",
    "        col(\"nationality\"),\n",
    "        to_date(col(\"dob\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"driver_id\").isNotNull())  # Data quality: remove invalid drivers\n",
    "    .filter(col(\"nationality\").isNotNull())  # Data quality: must have nationality\n",
    ")\n",
    "\n",
    "# Write to Silver table\n",
    "silver_drivers_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_drivers\")\n",
    "\n",
    "print(f\"âœ… silver_drivers created with {silver_drivers_df.count():,} rows\")\n",
    "silver_drivers_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 races data\n",
    "print(\"ğŸ¥ˆ Creating silver_races table...\")\n",
    "\n",
    "silver_races_df = (\n",
    "    spark.table(\"main.default.bronze_races\")\n",
    "    .select(\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"year\").cast(\"integer\").alias(\"race_year\"),\n",
    "        col(\"round\").cast(\"integer\").alias(\"round_number\"),\n",
    "        col(\"name\").alias(\"race_name\"),\n",
    "        col(\"date\").alias(\"race_date\"),\n",
    "        col(\"circuitId\").cast(\"integer\").alias(\"circuit_id\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"race_year\").between(1950, 2024))  # Data quality: reasonable year range\n",
    ")\n",
    "\n",
    "silver_races_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_races\")\n",
    "\n",
    "print(f\"âœ… silver_races created with {silver_races_df.count():,} rows\")\n",
    "silver_races_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 results data  \n",
    "print(\"ğŸ¥ˆ Creating silver_results table...\")\n",
    "\n",
    "silver_results_df = (\n",
    "    spark.table(\"main.default.bronze_results\")\n",
    "    .select(\n",
    "        col(\"resultId\").cast(\"integer\").alias(\"result_id\"),\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"constructorId\").cast(\"integer\").alias(\"constructor_id\"),\n",
    "        col(\"positionOrder\").cast(\"integer\").alias(\"finish_position\"),\n",
    "        col(\"points\").cast(\"double\").alias(\"points_earned\"),\n",
    "        col(\"laps\").cast(\"integer\").alias(\"laps_completed\"),\n",
    "        when(col(\"positionOrder\") == 1, True).otherwise(False).alias(\"race_winner\"),\n",
    "        when(col(\"positionOrder\") <= 3, True).otherwise(False).alias(\"podium_finish\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"result_id\").isNotNull())\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"driver_id\").isNotNull())\n",
    ")\n",
    "\n",
    "silver_results_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_results\")\n",
    "\n",
    "print(f\"âœ… silver_results created with {silver_results_df.count():,} rows\")\n",
    "silver_results_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2efc5",
   "metadata": {},
   "source": [
    "## ğŸ¥‡ Step 7: Gold Layer - Analytics-Ready Business Data\n",
    "\n",
    "The **Gold layer** contains aggregated, business-ready data optimized for analytics and reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03918169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create driver performance analytics table\n",
    "print(\"ğŸ¥‡ Creating gold_driver_standings table...\")\n",
    "\n",
    "# Create comprehensive driver performance metrics\n",
    "gold_driver_standings = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_drivers\").alias(\"d\"), \"driver_id\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\n",
    "        col(\"d.driver_id\"),\n",
    "        col(\"d.full_name\"),\n",
    "        col(\"d.nationality\"),\n",
    "        col(\"d.first_name\"),\n",
    "        col(\"d.last_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_races\"),\n",
    "        sum(\"points_earned\").alias(\"career_points\"),\n",
    "        sum(when(col(\"race_winner\"), 1).otherwise(0)).alias(\"wins\"),\n",
    "        sum(when(col(\"podium_finish\"), 1).otherwise(0)).alias(\"podiums\"),\n",
    "        min(\"race_year\").alias(\"career_start\"),\n",
    "        max(\"race_year\").alias(\"career_end\"),\n",
    "        avg(\"finish_position\").alias(\"avg_finish_position\"),\n",
    "        avg(\"points_earned\").alias(\"points_per_race\")\n",
    "    )\n",
    "    .withColumn(\"career_length\", col(\"career_end\") - col(\"career_start\") + 1)\n",
    "    .withColumn(\"win_percentage\", round(col(\"wins\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"podium_percentage\", round(col(\"podiums\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .filter(col(\"total_races\") >= 5)  # Focus on drivers with meaningful careers\n",
    "    .orderBy(col(\"career_points\").desc())\n",
    ")\n",
    "\n",
    "gold_driver_standings.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_driver_standings\")\n",
    "\n",
    "print(f\"âœ… gold_driver_standings created with {gold_driver_standings.count():,} drivers\")\n",
    "gold_driver_standings.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff4eaa",
   "metadata": {},
   "source": [
    "## ğŸ† Step 8: Gold Layer - Season Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create season-level analytics\n",
    "print(\"ğŸ¥‡ Creating gold_season_stats table...\")\n",
    "\n",
    "gold_season_stats = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\"race_year\")\n",
    "    .agg(\n",
    "        countDistinct(\"driver_id\").alias(\"unique_drivers\"),\n",
    "        countDistinct(\"constructor_id\").alias(\"unique_constructors\"),\n",
    "        count(\"*\").alias(\"total_race_entries\"),\n",
    "        countDistinct(\"race_id\").alias(\"races_in_season\"),\n",
    "        sum(\"points_earned\").alias(\"total_points_awarded\"),\n",
    "        avg(\"laps_completed\").alias(\"avg_laps_per_race\"),\n",
    "        (count(\"*\") - count(when(col(\"finish_position\").isNull(), 1))) / count(\"*\") * 100).alias(\"completion_rate\")\n",
    "    )\n",
    "    .withColumn(\"avg_drivers_per_race\", round(col(\"total_race_entries\") / col(\"races_in_season\"), 1))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .orderBy(\"race_year\")\n",
    ")\n",
    "\n",
    "gold_season_stats.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_season_stats\")\n",
    "\n",
    "print(f\"âœ… gold_season_stats created with {gold_season_stats.count():,} seasons\")\n",
    "gold_season_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48b53e",
   "metadata": {},
   "source": [
    "## âœ… Mission Accomplished! ğŸ‰\n",
    "\n",
    "**Congratulations! You've built a complete F1 data lakehouse in 15 minutes!**\n",
    "\n",
    "### What You've Created:\n",
    "- ğŸ“ **Volume storage** for modern file management  \n",
    "- ğŸ¥‰ **Bronze layer** - 3 raw data tables (drivers, races, results)\n",
    "- ğŸ¥ˆ **Silver layer** - 3 cleaned & validated tables\n",
    "- ğŸ¥‡ **Gold layer** - 2 analytics-ready business tables\n",
    "\n",
    "### Your F1 Data Lakehouse:\n",
    "```\n",
    "ğŸ—ï¸ Architecture Complete:\n",
    "   â””â”€â”€ main.default (catalog.schema)\n",
    "       â”œâ”€â”€ ğŸ“ f1_data_volume/ (file storage)\n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_drivers (1,500+ drivers)\n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_races (5,000+ races) \n",
    "       â”œâ”€â”€ ğŸ¥‰ bronze_results (100,000+ results)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_drivers (cleaned driver data)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_races (validated race data)\n",
    "       â”œâ”€â”€ ğŸ¥ˆ silver_results (processed results)\n",
    "       â”œâ”€â”€ ğŸ¥‡ gold_driver_standings (career analytics)\n",
    "       â””â”€â”€ ğŸ¥‡ gold_season_stats (seasonal insights)\n",
    "```\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "- **Explore Unity Catalog** - data lineage and governance\n",
    "- **Create Jobs** - automate your pipeline\n",
    "- **Build Dashboards** - visualize your F1 insights\n",
    "- **Try Genie** - ask questions in natural language\n",
    "- **Experiment with AI** - generate insights automatically\n",
    "\n",
    "### ğŸ’¡ Key Takeaways:\n",
    "- âœ… **Serverless** - No infrastructure to manage\n",
    "- âœ… **Volumes** - Modern file storage with governance\n",
    "- âœ… **Multi-language** - Python + SQL seamlessly\n",
    "- âœ… **Delta Lake** - ACID transactions and time travel\n",
    "- âœ… **Medallion architecture** - Production-ready data organization\n",
    "\n",
    "**ğŸ Ready to dive deeper into the world of data + AI? Let's go!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
