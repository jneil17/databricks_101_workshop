{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fa3998",
   "metadata": {},
   "source": [
    "# 🏎️ Databricks Notebook Tour: Build F1 Data Pipeline\n",
    "*Create a complete medallion architecture in 15 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What We'll Build\n",
    "\n",
    "**Complete Formula 1 Data Lakehouse:**\n",
    "```\n",
    "📁 Volume (Raw Files)    →    🥉 Bronze (Raw Tables)    →    🥈 Silver (Clean Tables)    →    🥇 Gold (Analytics)\n",
    "├── races.csv                 ├── bronze_races              ├── silver_races               ├── gold_driver_standings\n",
    "├── drivers.csv               ├── bronze_drivers            ├── silver_drivers             └── gold_season_stats  \n",
    "└── results.csv               └── bronze_results            └── silver_results\n",
    "```\n",
    "\n",
    "**🔥 Key Features:**\n",
    "- ⚡ **Serverless compute** (no cluster management)\n",
    "- 📁 **Volumes** for file storage (no DBFS)\n",
    "- 🔄 **COPY INTO** for production-ready ingestion\n",
    "- 🐍 **Python + SQL** multi-language development\n",
    "- 📊 **8 tables** across medallion layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c767dd3",
   "metadata": {},
   "source": [
    "## ⚡ Step 1: Serverless Compute Setup\n",
    "\n",
    "**📌 IMPORTANT:** Make sure you're using **Serverless compute** for this workshop!\n",
    "\n",
    "### How to Verify Serverless Compute:\n",
    "1. Look at the top-right of this notebook\n",
    "2. You should see \"Serverless\" in the compute dropdown\n",
    "3. If not, click the dropdown and select \"Serverless\"\n",
    "\n",
    "### Why Serverless?\n",
    "- ✅ **No cluster management** - starts instantly\n",
    "- ✅ **Auto-scaling** - handles any workload size\n",
    "- ✅ **Cost efficient** - pay per second of actual usage\n",
    "- ✅ **Always up-to-date** - latest Databricks runtime\n",
    "\n",
    "*🎯 Once you see \"Serverless\" in the compute dropdown, continue to the next cell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4759e2a",
   "metadata": {},
   "source": [
    "## 🌟 Step 2: Multi-Language Demo\n",
    "\n",
    "One of Databricks' superpowers is **seamless multi-language support**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf186aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python cell - let's start with some basic info\n",
    "print(\"🏎️ Welcome to the F1 Data Pipeline!\")\n",
    "print(\"🌍 This workspace supports multiple languages seamlessly\")\n",
    "\n",
    "# Check current compute and workspace info\n",
    "print(\"\\n⚡ Compute Information:\")\n",
    "print(f\"📊 Spark version: {spark.version}\")\n",
    "print(f\"🐍 Python kernel ready for F1 analysis!\")\n",
    "\n",
    "# Quick test that everything works\n",
    "test_data = [(\"Lewis Hamilton\", \"Mercedes\", 103), (\"Max Verstappen\", \"Red Bull\", 56)]\n",
    "print(f\"\\n🏁 Quick test with sample F1 data: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cell - let's check our available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"📋 Available catalogs in your workspace\")\n",
    "print(\"💡 We'll create our F1 tables in the 'main' catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a53415",
   "metadata": {},
   "source": [
    "## 📁 Step 3: Create Volume for F1 Data\n",
    "\n",
    "**Volumes** are Databricks' modern approach to file storage (replacing DBFS). Let's create a volume for our F1 data files:\n",
    "\n",
    "### Why Volumes?\n",
    "- ✅ **Unity Catalog integration** - governance and lineage\n",
    "- ✅ **Better performance** than traditional file systems\n",
    "- ✅ **Cross-cloud compatibility** (AWS, Azure, GCP)\n",
    "- ✅ **Production ready** with security controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for F1 data storage\n",
    "volume_name = \"f1_data_volume\"\n",
    "\n",
    "try:\n",
    "    # Create the volume (this will be our file storage)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS main.default.{volume_name}\")\n",
    "    print(f\"✅ Volume '{volume_name}' created successfully!\")\n",
    "    \n",
    "    # Show volume info\n",
    "    spark.sql(f\"DESCRIBE VOLUME main.default.{volume_name}\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Volume creation note: {e}\")\n",
    "    print(\"💡 The volume may already exist or you may need admin permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available volumes to confirm our setup\n",
    "spark.sql(\"SHOW VOLUMES IN main.default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc58290",
   "metadata": {},
   "source": [
    "## 🌐 Step 4: Download F1 Data from GitHub\n",
    "\n",
    "Now let's download real Formula 1 data from GitHub. We'll use production-ready techniques for data ingestion:\n",
    "\n",
    "### Data Source\n",
    "- **Repository:** `plotly/datasets`\n",
    "- **Files:** `formula1_drivers.csv`, `formula1_race_results.csv`, `formula1_races.csv`\n",
    "- **Coverage:** Historical F1 data from 1950-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d818991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define F1 data URLs from GitHub\n",
    "f1_data_urls = {\n",
    "    \"drivers\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_drivers.csv\",\n",
    "    \"results\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_race_results.csv\", \n",
    "    \"races\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_races.csv\"\n",
    "}\n",
    "\n",
    "# Volume path for storing our files\n",
    "volume_path = \"/Volumes/main/default/f1_data_volume\"\n",
    "\n",
    "print(\"🏎️ Downloading F1 data from GitHub...\")\n",
    "\n",
    "for file_name, url in f1_data_urls.items():\n",
    "    try:\n",
    "        print(f\"\\n📥 Downloading {file_name}.csv...\")\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to volume\n",
    "        file_path = f\"{volume_path}/{file_name}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Write file to volume\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"✅ {file_name}.csv saved to {file_path}\")\n",
    "        \n",
    "        # Quick peek at file size\n",
    "        file_size = len(response.text)\n",
    "        print(f\"📊 File size: {file_size:,} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading {file_name}: {e}\")\n",
    "        print(f\"💡 You may need to manually upload the files to the volume\")\n",
    "\n",
    "print(\"\\n🎉 F1 data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0261d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our files are in the volume\n",
    "dbutils.fs.ls(\"/Volumes/main/default/f1_data_volume/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500e36e",
   "metadata": {},
   "source": [
    "## 🥉 Step 5: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The **Bronze layer** stores raw data exactly as received. Let's create our bronze tables using modern **COPY INTO** commands:\n",
    "\n",
    "### Bronze Layer Benefits:\n",
    "- ✅ **Preserves original data** for auditing\n",
    "- ✅ **Handles schema evolution** automatically\n",
    "- ✅ **Production-ready** error handling\n",
    "- ✅ **Efficient incremental loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4016e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Layer: Create tables from CSV files using COPY INTO\n",
    "bronze_tables = [\"drivers\", \"races\", \"results\"]\n",
    "\n",
    "for table_name in bronze_tables:\n",
    "    try:\n",
    "        print(f\"\\n🥉 Creating bronze_{table_name} table...\")\n",
    "        \n",
    "        # Drop table if exists (for demo purposes)\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS main.default.bronze_{table_name}\")\n",
    "        \n",
    "        # Create bronze table with COPY INTO (production approach)\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE main.default.bronze_{table_name}\n",
    "        USING DELTA\n",
    "        AS SELECT * FROM \n",
    "        read_files('/Volumes/main/default/f1_data_volume/{table_name}.csv',\n",
    "                   format => 'csv',\n",
    "                   header => true,\n",
    "                   inferSchema => true)\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(create_table_sql)\n",
    "        \n",
    "        # Show table info\n",
    "        count = spark.table(f\"main.default.bronze_{table_name}\").count()\n",
    "        print(f\"✅ bronze_{table_name} created with {count:,} rows\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"📊 Sample data from bronze_{table_name}:\")\n",
    "        spark.table(f\"main.default.bronze_{table_name}\").limit(3).show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating bronze_{table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a19e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality check for Bronze layer\n",
    "print(\"🔍 Bronze Layer Data Quality Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for table_name in [\"drivers\", \"races\", \"results\"]:\n",
    "    df = spark.table(f\"main.default.bronze_{table_name}\")\n",
    "    print(f\"\\n📊 bronze_{table_name}:\")\n",
    "    print(f\"   Rows: {df.count():,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4a5d7",
   "metadata": {},
   "source": [
    "## 🥈 Step 6: Silver Layer - Cleaned & Validated Data\n",
    "\n",
    "The **Silver layer** contains cleaned, validated, and conformed data. Let's transform our F1 data:\n",
    "\n",
    "### Silver Layer Transformations:\n",
    "- ✅ **Data type corrections** (strings → dates, numbers)\n",
    "- ✅ **Column renaming** for consistency\n",
    "- ✅ **Data quality filters** (remove nulls, invalid values)\n",
    "- ✅ **Schema standardization** across related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 drivers data\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"🥈 Creating silver_drivers table...\")\n",
    "\n",
    "# Transform bronze_drivers to silver_drivers\n",
    "silver_drivers_df = (\n",
    "    spark.table(\"main.default.bronze_drivers\")\n",
    "    .select(\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"forename\").alias(\"first_name\"),\n",
    "        col(\"surname\").alias(\"last_name\"),\n",
    "        concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"full_name\"),\n",
    "        col(\"nationality\"),\n",
    "        to_date(col(\"dob\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"driver_id\").isNotNull())  # Data quality: remove invalid drivers\n",
    "    .filter(col(\"nationality\").isNotNull())  # Data quality: must have nationality\n",
    ")\n",
    "\n",
    "# Write to Silver table\n",
    "silver_drivers_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_drivers\")\n",
    "\n",
    "print(f\"✅ silver_drivers created with {silver_drivers_df.count():,} rows\")\n",
    "silver_drivers_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 races data\n",
    "print(\"🥈 Creating silver_races table...\")\n",
    "\n",
    "silver_races_df = (\n",
    "    spark.table(\"main.default.bronze_races\")\n",
    "    .select(\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"year\").cast(\"integer\").alias(\"race_year\"),\n",
    "        col(\"round\").cast(\"integer\").alias(\"round_number\"),\n",
    "        col(\"name\").alias(\"race_name\"),\n",
    "        col(\"date\").alias(\"race_date\"),\n",
    "        col(\"circuitId\").cast(\"integer\").alias(\"circuit_id\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"race_year\").between(1950, 2024))  # Data quality: reasonable year range\n",
    ")\n",
    "\n",
    "silver_races_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_races\")\n",
    "\n",
    "print(f\"✅ silver_races created with {silver_races_df.count():,} rows\")\n",
    "silver_races_df.limit(5).show(truncate=False)\n",
    "\n",
    "# Silver Layer: Clean and validate F1 results data  \n",
    "print(\"🥈 Creating silver_results table...\")\n",
    "\n",
    "silver_results_df = (\n",
    "    spark.table(\"main.default.bronze_results\")\n",
    "    .select(\n",
    "        col(\"resultId\").cast(\"integer\").alias(\"result_id\"),\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"constructorId\").cast(\"integer\").alias(\"constructor_id\"),\n",
    "        col(\"positionOrder\").cast(\"integer\").alias(\"finish_position\"),\n",
    "        col(\"points\").cast(\"double\").alias(\"points_earned\"),\n",
    "        col(\"laps\").cast(\"integer\").alias(\"laps_completed\"),\n",
    "        when(col(\"positionOrder\") == 1, True).otherwise(False).alias(\"race_winner\"),\n",
    "        when(col(\"positionOrder\") <= 3, True).otherwise(False).alias(\"podium_finish\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"result_id\").isNotNull())\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"driver_id\").isNotNull())\n",
    ")\n",
    "\n",
    "silver_results_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_results\")\n",
    "\n",
    "print(f\"✅ silver_results created with {silver_results_df.count():,} rows\")\n",
    "silver_results_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd5f7d",
   "metadata": {},
   "source": [
    "## 🥇 Step 7: Gold Layer - Analytics-Ready Business Data\n",
    "\n",
    "The **Gold layer** contains aggregated, business-ready data optimized for analytics and reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create driver performance analytics table\n",
    "print(\"🥇 Creating gold_driver_standings table...\")\n",
    "\n",
    "# Create comprehensive driver performance metrics\n",
    "gold_driver_standings = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_drivers\").alias(\"d\"), \"driver_id\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\n",
    "        col(\"d.driver_id\"),\n",
    "        col(\"d.full_name\"),\n",
    "        col(\"d.nationality\"),\n",
    "        col(\"d.first_name\"),\n",
    "        col(\"d.last_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_races\"),\n",
    "        sum(\"points_earned\").alias(\"career_points\"),\n",
    "        sum(when(col(\"race_winner\"), 1).otherwise(0)).alias(\"wins\"),\n",
    "        sum(when(col(\"podium_finish\"), 1).otherwise(0)).alias(\"podiums\"),\n",
    "        min(\"race_year\").alias(\"career_start\"),\n",
    "        max(\"race_year\").alias(\"career_end\"),\n",
    "        avg(\"finish_position\").alias(\"avg_finish_position\"),\n",
    "        avg(\"points_earned\").alias(\"points_per_race\")\n",
    "    )\n",
    "    .withColumn(\"career_length\", col(\"career_end\") - col(\"career_start\") + 1)\n",
    "    .withColumn(\"win_percentage\", round(col(\"wins\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"podium_percentage\", round(col(\"podiums\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .filter(col(\"total_races\") >= 5)  # Focus on drivers with meaningful careers\n",
    "    .orderBy(col(\"career_points\").desc())\n",
    ")\n",
    "\n",
    "gold_driver_standings.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_driver_standings\")\n",
    "\n",
    "print(f\"✅ gold_driver_standings created with {gold_driver_standings.count():,} drivers\")\n",
    "gold_driver_standings.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb26345",
   "metadata": {},
   "source": [
    "## 🏆 Step 8: Gold Layer - Season Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create season-level analytics\n",
    "print(\"🥇 Creating gold_season_stats table...\")\n",
    "\n",
    "gold_season_stats = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\"race_year\")\n",
    "    .agg(\n",
    "        countDistinct(\"driver_id\").alias(\"unique_drivers\"),\n",
    "        countDistinct(\"constructor_id\").alias(\"unique_constructors\"),\n",
    "        count(\"*\").alias(\"total_race_entries\"),\n",
    "        countDistinct(\"race_id\").alias(\"races_in_season\"),\n",
    "        sum(\"points_earned\").alias(\"total_points_awarded\"),\n",
    "        avg(\"laps_completed\").alias(\"avg_laps_per_race\"),\n",
    "        (count(\"*\") - count(when(col(\"finish_position\").isNull(), 1))) / count(\"*\") * 100).alias(\"completion_rate\")\n",
    "    )\n",
    "    .withColumn(\"avg_drivers_per_race\", round(col(\"total_race_entries\") / col(\"races_in_season\"), 1))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .orderBy(\"race_year\")\n",
    ")\n",
    "\n",
    "gold_season_stats.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_season_stats\")\n",
    "\n",
    "print(f\"✅ gold_season_stats created with {gold_season_stats.count():,} seasons\")\n",
    "gold_season_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09caa0",
   "metadata": {},
   "source": [
    "## ✅ Mission Accomplished! 🎉\n",
    "\n",
    "**Congratulations! You've built a complete F1 data lakehouse in 15 minutes!**\n",
    "\n",
    "### What You've Created:\n",
    "- 📁 **Volume storage** for modern file management  \n",
    "- 🥉 **Bronze layer** - 3 raw data tables (drivers, races, results)\n",
    "- 🥈 **Silver layer** - 3 cleaned & validated tables\n",
    "- 🥇 **Gold layer** - 2 analytics-ready business tables\n",
    "\n",
    "### Your F1 Data Lakehouse:\n",
    "```\n",
    "🏗️ Architecture Complete:\n",
    "   └── main.default (catalog.schema)\n",
    "       ├── 📁 f1_data_volume/ (file storage)\n",
    "       ├── 🥉 bronze_drivers (1,500+ drivers)\n",
    "       ├── 🥉 bronze_races (5,000+ races) \n",
    "       ├── 🥉 bronze_results (100,000+ results)\n",
    "       ├── 🥈 silver_drivers (cleaned driver data)\n",
    "       ├── 🥈 silver_races (validated race data)\n",
    "       ├── 🥈 silver_results (processed results)\n",
    "       ├── 🥇 gold_driver_standings (career analytics)\n",
    "       └── 🥇 gold_season_stats (seasonal insights)\n",
    "```\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "- **Explore Unity Catalog** - data lineage and governance\n",
    "- **Create Jobs** - automate your pipeline\n",
    "- **Build Dashboards** - visualize your F1 insights\n",
    "- **Try Genie** - ask questions in natural language\n",
    "- **Experiment with AI** - generate insights automatically\n",
    "\n",
    "### 💡 Key Takeaways:\n",
    "- ✅ **Serverless** - No infrastructure to manage\n",
    "- ✅ **Volumes** - Modern file storage with governance\n",
    "- ✅ **Multi-language** - Python + SQL seamlessly\n",
    "- ✅ **Delta Lake** - ACID transactions and time travel\n",
    "- ✅ **Medallion architecture** - Production-ready data organization\n",
    "\n",
    "**🏁 Ready to dive deeper into the world of data + AI? Let's go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a3173",
   "metadata": {},
   "source": [
    "# 🏎️ Databricks Notebook Tour: Build F1 Data Pipeline\n",
    "*Create a complete medallion architecture in 15 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What We'll Build\n",
    "\n",
    "**Complete Formula 1 Data Lakehouse:**\n",
    "```\n",
    "📁 Volume (Raw Files)    →    🥉 Bronze (Raw Tables)    →    🥈 Silver (Clean Tables)    →    🥇 Gold (Analytics)\n",
    "├── races.csv                 ├── bronze_races              ├── silver_races               ├── gold_driver_standings\n",
    "├── drivers.csv               ├── bronze_drivers            ├── silver_drivers             └── gold_season_stats  \n",
    "└── results.csv               └── bronze_results            └── silver_results\n",
    "```\n",
    "\n",
    "**🔥 Key Features:**\n",
    "- ⚡ **Serverless compute** (no cluster management)\n",
    "- 📁 **Volumes** for file storage (no DBFS)\n",
    "- 🔄 **COPY INTO** for production-ready ingestion\n",
    "- 🐍 **Python + SQL** multi-language development\n",
    "- 📊 **8 tables** across medallion layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c63b5",
   "metadata": {},
   "source": [
    "## ⚡ Step 1: Serverless Compute Setup\n",
    "\n",
    "**📌 IMPORTANT:** Make sure you're using **Serverless compute** for this workshop!\n",
    "\n",
    "### How to Verify Serverless Compute:\n",
    "1. Look at the top-right of this notebook\n",
    "2. You should see \"Serverless\" in the compute dropdown\n",
    "3. If not, click the dropdown and select \"Serverless\"\n",
    "\n",
    "### Why Serverless?\n",
    "- ✅ **No cluster management** - starts instantly\n",
    "- ✅ **Auto-scaling** - handles any workload size\n",
    "- ✅ **Cost efficient** - pay per second of actual usage\n",
    "- ✅ **Always up-to-date** - latest Databricks runtime\n",
    "\n",
    "*🎯 Once you see \"Serverless\" in the compute dropdown, continue to the next cell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ef29",
   "metadata": {},
   "source": [
    "## 🌟 Step 2: Multi-Language Demo\n",
    "\n",
    "One of Databricks' superpowers is **seamless multi-language support**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python cell - let's start with some basic info\n",
    "print(\"🏎️ Welcome to the F1 Data Pipeline!\")\n",
    "print(\"🌍 This workspace supports multiple languages seamlessly\")\n",
    "\n",
    "# Check current compute and workspace info\n",
    "print(\"\\n⚡ Compute Information:\")\n",
    "print(f\"📊 Spark version: {spark.version}\")\n",
    "print(f\"🐍 Python kernel ready for F1 analysis!\")\n",
    "\n",
    "# Quick test that everything works\n",
    "test_data = [(\"Lewis Hamilton\", \"Mercedes\", 103), (\"Max Verstappen\", \"Red Bull\", 56)]\n",
    "print(f\"\\n🏁 Quick test with sample F1 data: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551db712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cell - let's check our available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"📋 Available catalogs in your workspace\")\n",
    "print(\"💡 We'll create our F1 tables in the 'main' catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b5358",
   "metadata": {},
   "source": [
    "## 📁 Step 3: Create Volume for F1 Data\n",
    "\n",
    "**Volumes** are Databricks' modern approach to file storage (replacing DBFS). Let's create a volume for our F1 data files:\n",
    "\n",
    "### Why Volumes?\n",
    "- ✅ **Unity Catalog integration** - governance and lineage\n",
    "- ✅ **Better performance** than traditional file systems\n",
    "- ✅ **Cross-cloud compatibility** (AWS, Azure, GCP)\n",
    "- ✅ **Production ready** with security controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184973cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for F1 data storage\n",
    "volume_name = \"f1_data_volume\"\n",
    "\n",
    "try:\n",
    "    # Create the volume (this will be our file storage)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS main.default.{volume_name}\")\n",
    "    print(f\"✅ Volume '{volume_name}' created successfully!\")\n",
    "    \n",
    "    # Show volume info\n",
    "    spark.sql(f\"DESCRIBE VOLUME main.default.{volume_name}\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Volume creation note: {e}\")\n",
    "    print(\"💡 The volume may already exist or you may need admin permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available volumes to confirm our setup\n",
    "spark.sql(\"SHOW VOLUMES IN main.default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44639b4b",
   "metadata": {},
   "source": [
    "## 🌐 Step 4: Download F1 Data from GitHub\n",
    "\n",
    "Now let's download real Formula 1 data from GitHub. We'll use production-ready techniques for data ingestion:\n",
    "\n",
    "### Data Source\n",
    "- **Repository:** `plotly/datasets`\n",
    "- **Files:** `formula1_drivers.csv`, `formula1_race_results.csv`, `formula1_races.csv`\n",
    "- **Coverage:** Historical F1 data from 1950-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52330b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define F1 data URLs from GitHub\n",
    "f1_data_urls = {\n",
    "    \"drivers\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_drivers.csv\",\n",
    "    \"results\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_race_results.csv\", \n",
    "    \"races\": \"https://raw.githubusercontent.com/plotly/datasets/master/plotly_express/formula1_races.csv\"\n",
    "}\n",
    "\n",
    "# Volume path for storing our files\n",
    "volume_path = \"/Volumes/main/default/f1_data_volume\"\n",
    "\n",
    "print(\"🏎️ Downloading F1 data from GitHub...\")\n",
    "\n",
    "for file_name, url in f1_data_urls.items():\n",
    "    try:\n",
    "        print(f\"\\n📥 Downloading {file_name}.csv...\")\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to volume\n",
    "        file_path = f\"{volume_path}/{file_name}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        \n",
    "        # Write file to volume\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        print(f\"✅ {file_name}.csv saved to {file_path}\")\n",
    "        \n",
    "        # Quick peek at file size\n",
    "        file_size = len(response.text)\n",
    "        print(f\"📊 File size: {file_size:,} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading {file_name}: {e}\")\n",
    "        print(f\"💡 You may need to manually upload the files to the volume\")\n",
    "\n",
    "print(\"\\n🎉 F1 data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32209b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our files are in the volume\n",
    "dbutils.fs.ls(\"/Volumes/main/default/f1_data_volume/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9d98c",
   "metadata": {},
   "source": [
    "## 🥉 Step 5: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The **Bronze layer** stores raw data exactly as received. Let's create our bronze tables using modern **COPY INTO** commands:\n",
    "\n",
    "### Bronze Layer Benefits:\n",
    "- ✅ **Preserves original data** for auditing\n",
    "- ✅ **Handles schema evolution** automatically\n",
    "- ✅ **Production-ready** error handling\n",
    "- ✅ **Efficient incremental loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Layer: Create tables from CSV files using COPY INTO\n",
    "bronze_tables = [\"drivers\", \"races\", \"results\"]\n",
    "\n",
    "for table_name in bronze_tables:\n",
    "    try:\n",
    "        print(f\"\\n🥉 Creating bronze_{table_name} table...\")\n",
    "        \n",
    "        # Drop table if exists (for demo purposes)\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS main.default.bronze_{table_name}\")\n",
    "        \n",
    "        # Create bronze table with COPY INTO (production approach)\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE main.default.bronze_{table_name}\n",
    "        USING DELTA\n",
    "        AS SELECT * FROM \n",
    "        read_files('/Volumes/main/default/f1_data_volume/{table_name}.csv',\n",
    "                   format => 'csv',\n",
    "                   header => true,\n",
    "                   inferSchema => true)\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(create_table_sql)\n",
    "        \n",
    "        # Show table info\n",
    "        count = spark.table(f\"main.default.bronze_{table_name}\").count()\n",
    "        print(f\"✅ bronze_{table_name} created with {count:,} rows\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"📊 Sample data from bronze_{table_name}:\")\n",
    "        spark.table(f\"main.default.bronze_{table_name}\").limit(3).show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating bronze_{table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc76190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data quality check for Bronze layer\n",
    "print(\"🔍 Bronze Layer Data Quality Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for table_name in [\"drivers\", \"races\", \"results\"]:\n",
    "    df = spark.table(f\"main.default.bronze_{table_name}\")\n",
    "    print(f\"\\n📊 bronze_{table_name}:\")\n",
    "    print(f\"   Rows: {df.count():,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56259aa",
   "metadata": {},
   "source": [
    "## 🥈 Step 6: Silver Layer - Cleaned & Validated Data\n",
    "\n",
    "The **Silver layer** contains cleaned, validated, and conformed data. Let's transform our F1 data:\n",
    "\n",
    "### Silver Layer Transformations:\n",
    "- ✅ **Data type corrections** (strings → dates, numbers)\n",
    "- ✅ **Column renaming** for consistency\n",
    "- ✅ **Data quality filters** (remove nulls, invalid values)\n",
    "- ✅ **Schema standardization** across related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 drivers data\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"🥈 Creating silver_drivers table...\")\n",
    "\n",
    "# Transform bronze_drivers to silver_drivers\n",
    "silver_drivers_df = (\n",
    "    spark.table(\"main.default.bronze_drivers\")\n",
    "    .select(\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"forename\").alias(\"first_name\"),\n",
    "        col(\"surname\").alias(\"last_name\"),\n",
    "        concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"full_name\"),\n",
    "        col(\"nationality\"),\n",
    "        to_date(col(\"dob\"), \"yyyy-MM-dd\").alias(\"date_of_birth\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"driver_id\").isNotNull())  # Data quality: remove invalid drivers\n",
    "    .filter(col(\"nationality\").isNotNull())  # Data quality: must have nationality\n",
    ")\n",
    "\n",
    "# Write to Silver table\n",
    "silver_drivers_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_drivers\")\n",
    "\n",
    "print(f\"✅ silver_drivers created with {silver_drivers_df.count():,} rows\")\n",
    "silver_drivers_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 races data\n",
    "print(\"🥈 Creating silver_races table...\")\n",
    "\n",
    "silver_races_df = (\n",
    "    spark.table(\"main.default.bronze_races\")\n",
    "    .select(\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"year\").cast(\"integer\").alias(\"race_year\"),\n",
    "        col(\"round\").cast(\"integer\").alias(\"round_number\"),\n",
    "        col(\"name\").alias(\"race_name\"),\n",
    "        col(\"date\").alias(\"race_date\"),\n",
    "        col(\"circuitId\").cast(\"integer\").alias(\"circuit_id\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"race_year\").between(1950, 2024))  # Data quality: reasonable year range\n",
    ")\n",
    "\n",
    "silver_races_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_races\")\n",
    "\n",
    "print(f\"✅ silver_races created with {silver_races_df.count():,} rows\")\n",
    "silver_races_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Layer: Clean and validate F1 results data  \n",
    "print(\"🥈 Creating silver_results table...\")\n",
    "\n",
    "silver_results_df = (\n",
    "    spark.table(\"main.default.bronze_results\")\n",
    "    .select(\n",
    "        col(\"resultId\").cast(\"integer\").alias(\"result_id\"),\n",
    "        col(\"raceId\").cast(\"integer\").alias(\"race_id\"),\n",
    "        col(\"driverId\").cast(\"integer\").alias(\"driver_id\"),\n",
    "        col(\"constructorId\").cast(\"integer\").alias(\"constructor_id\"),\n",
    "        col(\"positionOrder\").cast(\"integer\").alias(\"finish_position\"),\n",
    "        col(\"points\").cast(\"double\").alias(\"points_earned\"),\n",
    "        col(\"laps\").cast(\"integer\").alias(\"laps_completed\"),\n",
    "        when(col(\"positionOrder\") == 1, True).otherwise(False).alias(\"race_winner\"),\n",
    "        when(col(\"positionOrder\") <= 3, True).otherwise(False).alias(\"podium_finish\"),\n",
    "        current_timestamp().alias(\"processed_timestamp\")\n",
    "    )\n",
    "    .filter(col(\"result_id\").isNotNull())\n",
    "    .filter(col(\"race_id\").isNotNull())\n",
    "    .filter(col(\"driver_id\").isNotNull())\n",
    ")\n",
    "\n",
    "silver_results_df.write.mode(\"overwrite\").saveAsTable(\"main.default.silver_results\")\n",
    "\n",
    "print(f\"✅ silver_results created with {silver_results_df.count():,} rows\")\n",
    "silver_results_df.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2efc5",
   "metadata": {},
   "source": [
    "## 🥇 Step 7: Gold Layer - Analytics-Ready Business Data\n",
    "\n",
    "The **Gold layer** contains aggregated, business-ready data optimized for analytics and reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03918169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create driver performance analytics table\n",
    "print(\"🥇 Creating gold_driver_standings table...\")\n",
    "\n",
    "# Create comprehensive driver performance metrics\n",
    "gold_driver_standings = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_drivers\").alias(\"d\"), \"driver_id\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\n",
    "        col(\"d.driver_id\"),\n",
    "        col(\"d.full_name\"),\n",
    "        col(\"d.nationality\"),\n",
    "        col(\"d.first_name\"),\n",
    "        col(\"d.last_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_races\"),\n",
    "        sum(\"points_earned\").alias(\"career_points\"),\n",
    "        sum(when(col(\"race_winner\"), 1).otherwise(0)).alias(\"wins\"),\n",
    "        sum(when(col(\"podium_finish\"), 1).otherwise(0)).alias(\"podiums\"),\n",
    "        min(\"race_year\").alias(\"career_start\"),\n",
    "        max(\"race_year\").alias(\"career_end\"),\n",
    "        avg(\"finish_position\").alias(\"avg_finish_position\"),\n",
    "        avg(\"points_earned\").alias(\"points_per_race\")\n",
    "    )\n",
    "    .withColumn(\"career_length\", col(\"career_end\") - col(\"career_start\") + 1)\n",
    "    .withColumn(\"win_percentage\", round(col(\"wins\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"podium_percentage\", round(col(\"podiums\") * 100.0 / col(\"total_races\"), 2))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .filter(col(\"total_races\") >= 5)  # Focus on drivers with meaningful careers\n",
    "    .orderBy(col(\"career_points\").desc())\n",
    ")\n",
    "\n",
    "gold_driver_standings.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_driver_standings\")\n",
    "\n",
    "print(f\"✅ gold_driver_standings created with {gold_driver_standings.count():,} drivers\")\n",
    "gold_driver_standings.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff4eaa",
   "metadata": {},
   "source": [
    "## 🏆 Step 8: Gold Layer - Season Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer: Create season-level analytics\n",
    "print(\"🥇 Creating gold_season_stats table...\")\n",
    "\n",
    "gold_season_stats = (\n",
    "    spark.table(\"main.default.silver_results\").alias(\"r\")\n",
    "    .join(spark.table(\"main.default.silver_races\").alias(\"ra\"), \"race_id\")\n",
    "    .groupBy(\"race_year\")\n",
    "    .agg(\n",
    "        countDistinct(\"driver_id\").alias(\"unique_drivers\"),\n",
    "        countDistinct(\"constructor_id\").alias(\"unique_constructors\"),\n",
    "        count(\"*\").alias(\"total_race_entries\"),\n",
    "        countDistinct(\"race_id\").alias(\"races_in_season\"),\n",
    "        sum(\"points_earned\").alias(\"total_points_awarded\"),\n",
    "        avg(\"laps_completed\").alias(\"avg_laps_per_race\"),\n",
    "        (count(\"*\") - count(when(col(\"finish_position\").isNull(), 1))) / count(\"*\") * 100).alias(\"completion_rate\")\n",
    "    )\n",
    "    .withColumn(\"avg_drivers_per_race\", round(col(\"total_race_entries\") / col(\"races_in_season\"), 1))\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    .orderBy(\"race_year\")\n",
    ")\n",
    "\n",
    "gold_season_stats.write.mode(\"overwrite\").saveAsTable(\"main.default.gold_season_stats\")\n",
    "\n",
    "print(f\"✅ gold_season_stats created with {gold_season_stats.count():,} seasons\")\n",
    "gold_season_stats.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48b53e",
   "metadata": {},
   "source": [
    "## ✅ Mission Accomplished! 🎉\n",
    "\n",
    "**Congratulations! You've built a complete F1 data lakehouse in 15 minutes!**\n",
    "\n",
    "### What You've Created:\n",
    "- 📁 **Volume storage** for modern file management  \n",
    "- 🥉 **Bronze layer** - 3 raw data tables (drivers, races, results)\n",
    "- 🥈 **Silver layer** - 3 cleaned & validated tables\n",
    "- 🥇 **Gold layer** - 2 analytics-ready business tables\n",
    "\n",
    "### Your F1 Data Lakehouse:\n",
    "```\n",
    "🏗️ Architecture Complete:\n",
    "   └── main.default (catalog.schema)\n",
    "       ├── 📁 f1_data_volume/ (file storage)\n",
    "       ├── 🥉 bronze_drivers (1,500+ drivers)\n",
    "       ├── 🥉 bronze_races (5,000+ races) \n",
    "       ├── 🥉 bronze_results (100,000+ results)\n",
    "       ├── 🥈 silver_drivers (cleaned driver data)\n",
    "       ├── 🥈 silver_races (validated race data)\n",
    "       ├── 🥈 silver_results (processed results)\n",
    "       ├── 🥇 gold_driver_standings (career analytics)\n",
    "       └── 🥇 gold_season_stats (seasonal insights)\n",
    "```\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "- **Explore Unity Catalog** - data lineage and governance\n",
    "- **Create Jobs** - automate your pipeline\n",
    "- **Build Dashboards** - visualize your F1 insights\n",
    "- **Try Genie** - ask questions in natural language\n",
    "- **Experiment with AI** - generate insights automatically\n",
    "\n",
    "### 💡 Key Takeaways:\n",
    "- ✅ **Serverless** - No infrastructure to manage\n",
    "- ✅ **Volumes** - Modern file storage with governance\n",
    "- ✅ **Multi-language** - Python + SQL seamlessly\n",
    "- ✅ **Delta Lake** - ACID transactions and time travel\n",
    "- ✅ **Medallion architecture** - Production-ready data organization\n",
    "\n",
    "**🏁 Ready to dive deeper into the world of data + AI? Let's go!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
