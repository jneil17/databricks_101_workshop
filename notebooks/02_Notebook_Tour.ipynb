{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b88b81",
   "metadata": {},
   "source": [
    "# 🏎️ Databricks Notebook Tour: Build F1 Data Pipeline\n",
    "*Create a complete medallion architecture in 15 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What We'll Build\n",
    "\n",
    "**Complete Formula 1 Data Lakehouse:**\n",
    "```\n",
    "📁 Volume (Raw Files)    →    🥉 Bronze (Raw Tables)    →    🥈 Silver (Clean Tables)    →    🥇 Gold (Analytics)\n",
    "├── races.csv                 ├── bronze_races              ├── silver_races               ├── gold_driver_standings\n",
    "├── drivers.csv               ├── bronze_drivers            ├── silver_drivers             └── gold_season_stats  \n",
    "└── results.csv               └── bronze_results            └── silver_results\n",
    "```\n",
    "\n",
    "**🔥 Key Features:**\n",
    "- ⚡ **Serverless compute** (no cluster management)\n",
    "- 📁 **Volumes** for file storage (no DBFS)\n",
    "- 🔄 **COPY INTO** for production-ready ingestion\n",
    "- 🐍 **Python + SQL** multi-language development\n",
    "- 📊 **8 tables** across medallion layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf5bcd",
   "metadata": {},
   "source": [
    "## ⚡ Step 1: Serverless Compute Setup\n",
    "\n",
    "**📌 IMPORTANT:** Make sure you're using **Serverless compute** for this workshop!\n",
    "\n",
    "### How to Verify Serverless Compute:\n",
    "1. Look at the top-right of this notebook\n",
    "2. You should see \"Serverless\" in the compute dropdown\n",
    "3. If not, click the dropdown and select \"Serverless\"\n",
    "\n",
    "### Why Serverless?\n",
    "- ✅ **No cluster management** - starts instantly\n",
    "- ✅ **Auto-scaling** - handles any workload size\n",
    "- ✅ **Cost efficient** - pay per second of actual usage\n",
    "- ✅ **Always up-to-date** - latest Databricks runtime\n",
    "\n",
    "*🎯 Once you see \"Serverless\" in the compute dropdown, continue to the next cell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55302d3d",
   "metadata": {},
   "source": [
    "## 🌟 Step 2: Multi-Language Demo\n",
    "\n",
    "One of Databricks' superpowers is **seamless multi-language support**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce62d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python cell - let's start with some basic info\n",
    "print(\"🏎️ Welcome to the F1 Data Pipeline!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get current user and workspace info\n",
    "current_user = spark.sql(\"SELECT current_user() as user\").collect()[0].user\n",
    "workspace_id = spark.conf.get(\"spark.databricks.workspaceUrl\", \"databricks-workspace\")\n",
    "\n",
    "print(f\"👤 Current user: {current_user}\")\n",
    "print(f\"🏢 Workspace: {workspace_id}\")\n",
    "print(f\"⚡ Compute: Serverless\")\n",
    "print(f\"📚 Catalog: main.default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cell - let's check our catalog structure\n",
    "# Use %sql magic command for SQL in Python notebook\n",
    "spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    '🏁 Starting F1 Data Pipeline Build!' as message,\n",
    "    current_catalog() as current_catalog,\n",
    "    current_schema() as current_schema,\n",
    "    current_timestamp() as build_started_at\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c3a1d",
   "metadata": {},
   "source": [
    "## 📁 Step 3: Create Volume for Data Storage\n",
    "\n",
    "**Volumes** are the modern way to store files in Databricks. No more DBFS!\n",
    "\n",
    "### Why Volumes?\n",
    "- 🔒 **Unity Catalog integration** - full governance\n",
    "- 🌐 **Cloud-native** - direct cloud storage access  \n",
    "- 📁 **File system semantics** - works like local folders\n",
    "- 🔄 **Version control friendly** - easy backup and sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40558f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Volume for storing raw F1 data files\n",
    "# This is where we'll download and store our CSV files\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS main.default.f1_raw_data\n",
    "COMMENT 'Raw Formula 1 datasets for workshop - races, drivers, and results'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our Volume was created successfully\n",
    "spark.sql(\"DESCRIBE VOLUME main.default.f1_raw_data\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991b5b7",
   "metadata": {},
   "source": [
    "## 📥 Step 4: Download F1 Data to Volume\n",
    "\n",
    "Time to get our Formula 1 data! We'll download 3 CSV files from GitHub and store them in our Volume.\n",
    "\n",
    "**📊 Data Overview:**\n",
    "- **races.csv** - Race information (circuits, dates, seasons)\n",
    "- **drivers.csv** - Driver profiles (names, nationalities, birth dates)  \n",
    "- **results.csv** - Race results (positions, points, lap times)\n",
    "\n",
    "*📈 Dataset size: ~25,000 race results from 1950-2023*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# F1 data source URLs from GitHub\n",
    "base_url = \"https://raw.githubusercontent.com/toUpperCase78/formula1-datasets/master\"\n",
    "files_to_download = {\n",
    "    \"races.csv\": f\"{base_url}/races.csv\",\n",
    "    \"drivers.csv\": f\"{base_url}/drivers.csv\", \n",
    "    \"results.csv\": f\"{base_url}/results.csv\"\n",
    "}\n",
    "\n",
    "# Volume path where we'll store the files\n",
    "volume_path = \"/Volumes/main/default/f1_raw_data\"\n",
    "\n",
    "print(\"🏎️ Downloading Formula 1 datasets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for filename, url in files_to_download.items():\n",
    "    print(f\"📥 Downloading {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Write to Volume\n",
    "        file_path = f\"{volume_path}/{filename}\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            \n",
    "        # Check file size\n",
    "        file_size = len(response.content)\n",
    "        print(f\"   ✅ Downloaded {filename} ({file_size:,} bytes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error downloading {filename}: {str(e)}\")\n",
    "\n",
    "print(\"\\n🎯 Download complete! Ready to build our pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90358128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify our files are in the Volume\n",
    "spark.sql(\"LIST '/Volumes/main/default/f1_raw_data/'\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f569d",
   "metadata": {},
   "source": [
    "## 🥉 Step 5: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "The **Bronze layer** stores raw data exactly as received. We'll use **COPY INTO** for production-ready data ingestion.\n",
    "\n",
    "### Why COPY INTO?\n",
    "- 🔄 **Idempotent** - safe to run multiple times\n",
    "- 📊 **Schema evolution** - handles changing data structures\n",
    "- 🎯 **Performance** - optimized for bulk loading\n",
    "- 🔍 **Monitoring** - detailed load statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Table 1: Races\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE main.default.bronze_races\n",
    "USING DELTA\n",
    "COMMENT 'Bronze layer: Raw F1 race data from CSV files'\n",
    "AS\n",
    "SELECT * FROM read_files(\n",
    "  '/Volumes/main/default/f1_raw_data/races.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Bronze races table created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Table 2: Drivers  \n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE main.default.bronze_drivers\n",
    "USING DELTA\n",
    "COMMENT 'Bronze layer: Raw F1 driver data from CSV files'\n",
    "AS\n",
    "SELECT * FROM read_files(\n",
    "  '/Volumes/main/default/f1_raw_data/drivers.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Bronze drivers table created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00257bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Table 3: Results\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE main.default.bronze_results\n",
    "USING DELTA\n",
    "COMMENT 'Bronze layer: Raw F1 race results from CSV files'\n",
    "AS\n",
    "SELECT * FROM read_files(\n",
    "  '/Volumes/main/default/f1_raw_data/results.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Bronze results table created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e2763",
   "metadata": {},
   "source": [
    "## 🥈 Step 6: Silver Layer - Clean and Validated Data\n",
    "\n",
    "The **Silver layer** contains cleaned, validated, and enriched data. We'll fix data types, handle nulls, and add business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Table 1: Clean Races with proper date types\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE main.default.silver_races\n",
    "USING DELTA\n",
    "COMMENT 'Silver layer: Cleaned F1 race data with proper data types'\n",
    "AS\n",
    "SELECT \n",
    "  CAST(raceId as INT) as raceId,\n",
    "  CAST(year as INT) as year,\n",
    "  CAST(round as INT) as round,\n",
    "  CAST(circuitId as INT) as circuitId,\n",
    "  name as race_name,\n",
    "  CAST(date as DATE) as race_date,\n",
    "  time as race_time,\n",
    "  url as race_url,\n",
    "  CASE \n",
    "    WHEN year < 1980 THEN 'Classic Era'\n",
    "    WHEN year < 2000 THEN 'Modern Era' \n",
    "    WHEN year < 2014 THEN 'V8 Era'\n",
    "    ELSE 'Hybrid Era'\n",
    "  END as f1_era,\n",
    "  current_timestamp() as processed_at\n",
    "FROM main.default.bronze_races\n",
    "WHERE year IS NOT NULL AND name IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Silver races table created with cleaned data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69120c",
   "metadata": {},
   "source": [
    "## 🥇 Step 7: Gold Layer - Analytics-Ready Data\n",
    "\n",
    "The **Gold layer** contains business-ready aggregated data for analytics and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd59402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Table 1: Driver Career Statistics\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE main.default.gold_driver_standings\n",
    "USING DELTA\n",
    "COMMENT 'Gold layer: Comprehensive driver career statistics for analytics'\n",
    "AS\n",
    "SELECT \n",
    "  d.driverId,\n",
    "  CONCAT(d.forename, ' ', d.surname) as full_name,\n",
    "  d.nationality,\n",
    "  COUNT(DISTINCT r.raceId) as total_races,\n",
    "  SUM(CAST(res.points as DOUBLE)) as total_career_points,\n",
    "  COUNT(CASE WHEN res.position = '1' THEN 1 END) as wins,\n",
    "  COUNT(CASE WHEN CAST(res.position as INT) <= 3 THEN 1 END) as podiums,\n",
    "  ROUND(SUM(CAST(res.points as DOUBLE)) / COUNT(DISTINCT r.raceId), 2) as points_per_race,\n",
    "  ROUND(COUNT(CASE WHEN res.position = '1' THEN 1 END) * 100.0 / COUNT(DISTINCT r.raceId), 2) as win_percentage,\n",
    "  MIN(CAST(race.year as INT)) as career_start_year,\n",
    "  MAX(CAST(race.year as INT)) as career_end_year,\n",
    "  current_timestamp() as calculated_at\n",
    "FROM main.default.bronze_drivers d\n",
    "JOIN main.default.bronze_results res ON CAST(d.driverId as INT) = CAST(res.driverId as INT)\n",
    "JOIN main.default.bronze_races race ON CAST(res.raceId as INT) = CAST(race.raceId as INT)\n",
    "WHERE d.forename IS NOT NULL AND d.surname IS NOT NULL\n",
    "GROUP BY d.driverId, d.forename, d.surname, d.nationality\n",
    "HAVING COUNT(DISTINCT r.raceId) >= 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Gold driver standings table created with career statistics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2008e6d",
   "metadata": {},
   "source": [
    "## ✅ Pipeline Complete! \n",
    "\n",
    "**🎉 Congratulations! You've built a complete Formula 1 data lakehouse!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ **Downloaded real F1 data** from GitHub to Volume storage\n",
    "- ✅ **Created Delta tables** across Bronze, Silver, and Gold layers\n",
    "- ✅ **Used modern data patterns** with Volumes and Delta Lake\n",
    "- ✅ **Implemented data transformations** and business logic\n",
    "- ✅ **Built analytics-ready** datasets for dashboards\n",
    "\n",
    "### Your Data Architecture:\n",
    "```\n",
    "📁 Volume: main.default.f1_raw_data (3 CSV files)\n",
    "   ↓\n",
    "🥉 Bronze: bronze_races, bronze_drivers, bronze_results  \n",
    "   ↓\n",
    "🥈 Silver: silver_races (cleaned with F1 eras)\n",
    "   ↓ \n",
    "🥇 Gold: gold_driver_standings (career statistics)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification - let's check our tables\n",
    "print(\"🏁 F1 Data Pipeline Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tables_to_check = [\n",
    "    'bronze_races', 'bronze_drivers', 'bronze_results',\n",
    "    'silver_races', \n",
    "    'gold_driver_standings'\n",
    "]\n",
    "\n",
    "for table in tables_to_check:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM main.default.{table}\").collect()[0].count\n",
    "        layer = table.split('_')[0].upper()\n",
    "        emoji = {'BRONZE': '🥉', 'SILVER': '🥈', 'GOLD': '🥇'}.get(layer, '📊')\n",
    "        print(f\"{emoji} {table}: {count:,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {table}: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\n🎯 Pipeline built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2962829",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Your F1 data lakehouse is ready! Here's what to explore next:\n",
    "\n",
    "### Immediate Next Steps:\n",
    "1. **➡️ [03_Unity_Catalog_Demo.ipynb](03_Unity_Catalog_Demo.ipynb)** - Explore data lineage and governance\n",
    "2. **➡️ [04_Job_Creation.ipynb](04_Job_Creation.ipynb)** - Schedule automated data refreshes  \n",
    "3. **➡️ [07_SQL_Editor.sql](07_SQL_Editor.sql)** - Build analytics queries and visualizations\n",
    "\n",
    "### Advanced Features:\n",
    "- **Delta Live Tables** - Managed ETL pipelines\n",
    "- **AI Agents** - Build F1 Q&A chatbots\n",
    "- **Dashboards** - Interactive data visualizations\n",
    "- **Genie** - Natural language queries\n",
    "\n",
    "**🏎️ Ready to dive deeper into the world of data + AI? Let's go!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
