{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9765335-5a3f-477b-91e6-2aefc5cbe612",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Bronze Table: Raw Qualifying Results"
    }
   },
   "source": [
    "## 🏎️ F1 Declarative Pipeline\n",
    "\n",
    "This notebook demonstrates Declarative Pipeline concepts, but the **actual runnable pipeline** is located in:\n",
    "\n",
    "### 📂 **`06_Formula1_Declarative_Pipeline`**\n",
    "\n",
    "**👉 Navigate there now to run the pre-built F1 qualifying data pipeline!**\n",
    "\n",
    "---\n",
    "\n",
    "![](./Images/06_Formula1_Declarative_Pipeline folder.png \"06_Formula1_Declarative_Pipeline folder.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b267df2-2a54-4a9d-994b-e02ee4c9c943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 Building Your Declarative Pipeline\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "\n",
    "### 1️⃣ Get the Pipeline Code\n",
    "Navigate to the **`06_Formula1_Declarative_Pipeline/transformations/`** folder and open **`formula1_sdp.py`**\n",
    "\n",
    "This file contains the complete Declarative Pipeline code that you'll copy into your new pipeline.\n",
    "\n",
    "![](./Images/06_Formula1_Declarative_Pipeline.png \"06_Formula1_Declarative_Pipeline.png\")\n",
    "\n",
    "### 2️⃣ Create New ETL Pipeline\n",
    "1. **Navigate up one level** to the main notebooks folder\n",
    "2. **Click \"Create\"** → **\"ETL Pipeline\"**\n",
    "\n",
    "![](./Images/Create ETL Pipeline Navigation.png \"Create ETL Pipeline Navigation.png\")\n",
    "\n",
    "3. **Name your pipeline:** `Formula1 Declarative Pipeline` (or your preferred name)\n",
    "4. **Select \"Start with empty file\"**\n",
    "5. **Choose \"Python\"** as the language\n",
    "\n",
    "![](./Images/Declarative Pipeline Plus Empty File.png \"Declarative Pipeline Plus Empty File.png\")\n",
    "\n",
    "### 3️⃣ Add Pipeline Code\n",
    "1. **Copy the code** from `formula1_sdp.py` \n",
    "2. **Paste it** into your new empty pipeline file\n",
    "3. **Click \"Run Pipeline\"** to execute\n",
    "\n",
    "![](./Images/Declarative Pipeline DAG.png \"Declarative Pipeline DAG.png\")\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What This Pipeline Does\n",
    "\n",
    "**🥉 Bronze Layer:**\n",
    "- Incrementally ingests F1 sprint qualifying CSV files using Auto Loader\n",
    "- Applies data quality checks (no rescued data, valid track/driver)\n",
    "\n",
    "**🔄 Change Data Capture:**\n",
    "- Creates streaming table for sprint qualifying results\n",
    "- Uses Track + Driver as composite keys\n",
    "- Sequences updates by Q1 qualifying times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78ae96d-c562-450a-b1cf-d7650d10f7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Ready to explore AI-powered features and advanced analytics?\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **🔄 Create Your Declarative Pipeline:**\n",
    "   - Go to Workflows → Declarative Pipelines → Create Pipeline\n",
    "   - Use this notebook as the source\n",
    "   - Configure with Serverless compute\n",
    "\n",
    "2. **📊 Monitor Pipeline Execution:**\n",
    "   - Watch automatic dependency resolution\n",
    "   - Check data quality expectation results\n",
    "   - Explore generated lineage graphs\n",
    "\n",
    "3. **➡️ Next Notebook:** [07_SQL_Editor.sql](07_SQL_Editor.sql)\n",
    "   - Explore SQL analytics and visualization capabilities\n",
    "   - Build interactive F1 dashboards and reports\n",
    "\n",
    "### 🎯 Best Practices Checklist:\n",
    "- ✅ **Start simple** with basic Bronze → Silver → Gold\n",
    "- ✅ **Add expectations gradually** as you understand your data\n",
    "- ✅ **Use descriptive table names** for clarity\n",
    "- ✅ **Document transformations** with comments\n",
    "- ✅ **Monitor data quality** trends over time\n",
    "- ✅ **Test expectations** before production deployment\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- **🔧 Start with `@dlt.expect()`** to understand data patterns\n",
    "- **📊 Use pipeline dashboards** for quality monitoring\n",
    "- **⚡ Leverage Serverless** for cost-effective execution\n",
    "- **🔄 Design for incremental processing** from day one\n",
    "\n",
    "**🔄 Your data pipelines are now production-ready! 🚀**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Declarative_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
