{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9765335-5a3f-477b-91e6-2aefc5cbe612",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Bronze Table: Raw Qualifying Results"
    }
   },
   "source": [
    "## ğŸï¸ F1 Declarative Pipeline\n",
    "\n",
    "This notebook demonstrates Declarative Pipeline concepts, but the **actual runnable pipeline** is located in:\n",
    "\n",
    "### ğŸ“‚ **`06_Formula1_Declarative_Pipeline`**\n",
    "\n",
    "**ğŸ‘‰ Navigate there now to run the pre-built F1 qualifying data pipeline!**\n",
    "\n",
    "---\n",
    "\n",
    "![](./Images/06_Formula1_Declarative_Pipeline folder.png \"06_Formula1_Declarative_Pipeline folder.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b267df2-2a54-4a9d-994b-e02ee4c9c943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸš€ Building Your Declarative Pipeline\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "\n",
    "### 1ï¸âƒ£ Get the Pipeline Code\n",
    "Navigate to the **`06_Formula1_Declarative_Pipeline/transformations/`** folder and open **`formula1_sdp.py`**\n",
    "\n",
    "This file contains the complete Declarative Pipeline code that you'll copy into your new pipeline.\n",
    "\n",
    "![](./Images/06_Formula1_Declarative_Pipeline.png \"06_Formula1_Declarative_Pipeline.png\")\n",
    "\n",
    "### 2ï¸âƒ£ Create New ETL Pipeline\n",
    "1. **Navigate up one level** to the main notebooks folder\n",
    "2. **Click \"Create\"** â†’ **\"ETL Pipeline\"**\n",
    "\n",
    "![](./Images/Create ETL Pipeline Navigation.png \"Create ETL Pipeline Navigation.png\")\n",
    "\n",
    "3. **Name your pipeline:** `Formula1 Declarative Pipeline` (or your preferred name)\n",
    "4. **Select \"Start with empty file\"**\n",
    "5. **Choose \"Python\"** as the language\n",
    "\n",
    "![](./Images/Declarative Pipeline Plus Empty File.png \"Declarative Pipeline Plus Empty File.png\")\n",
    "\n",
    "### 3ï¸âƒ£ Add Pipeline Code\n",
    "1. **Copy the code** from `formula1_sdp.py` \n",
    "2. **Paste it** into your new empty pipeline file\n",
    "3. **Click \"Run Pipeline\"** to execute\n",
    "\n",
    "![](./Images/Declarative Pipeline DAG.png \"Declarative Pipeline DAG.png\")\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What This Pipeline Does\n",
    "\n",
    "**ğŸ¥‰ Bronze Layer:**\n",
    "- Incrementally ingests F1 sprint qualifying CSV files using Auto Loader\n",
    "- Applies data quality checks (no rescued data, valid track/driver)\n",
    "\n",
    "**ğŸ”„ Change Data Capture:**\n",
    "- Creates streaming table for sprint qualifying results\n",
    "- Uses Track + Driver as composite keys\n",
    "- Sequences updates by Q1 qualifying times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78ae96d-c562-450a-b1cf-d7650d10f7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸš€ Next Steps\n",
    "\n",
    "**ğŸ‰ Congratulations! You've created your first Declarative Pipeline!**\n",
    "\n",
    "### What You Just Accomplished:\n",
    "- âœ… **Created an ETL pipeline** using the new Declarative Pipeline interface\n",
    "- âœ… **Implemented data quality checks** with automatic bad record handling\n",
    "- âœ… **Set up Change Data Capture** for real-time data processing\n",
    "- âœ… **Used Auto Loader** for scalable file ingestion\n",
    "\n",
    "### Pipeline Features You Built:\n",
    "- **ğŸ”„ Streaming ingestion** from cloud storage with Auto Loader\n",
    "- **âœ… Data quality expectations** that drop invalid records\n",
    "- **ğŸ“Š Change Data Capture** with composite keys and sequencing\n",
    "- **âš¡ Serverless execution** for cost-effective processing\n",
    "\n",
    "### ğŸ” Monitor Your Pipeline:\n",
    "1. **Check pipeline status** in the Workflows â†’ ETL Pipelines section\n",
    "2. **View data quality metrics** and expectation results\n",
    "3. **Explore lineage graphs** showing data flow\n",
    "4. **Monitor execution logs** for troubleshooting\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "- **Declarative Pipelines** simplify complex ETL workflows\n",
    "- **Data quality expectations** catch issues early\n",
    "- **Auto Loader** handles file ingestion automatically\n",
    "- **Change Data Capture** keeps data current and consistent\n",
    "\n",
    "### â¡ï¸ **Next:** SQL Editor for Interactive Analytics\n",
    "- Build interactive F1 analytics and visualizations\n",
    "- Create dashboards from your processed data\n",
    "- Explore advanced SQL features and charts\n",
    "\n",
    "**ğŸï¸ Ready to analyze your F1 data with SQL! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Declarative_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
