{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd58af5f",
   "metadata": {},
   "source": [
    "# 🔄 Delta Live Tables: Production ETL Pipeline\n",
    "*Build reliable, maintainable data pipelines with declarative ETL*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- ✅ **Delta Live Tables (DLT)** concepts and benefits\n",
    "- ✅ **Declarative pipeline** development approach\n",
    "- ✅ **Data quality expectations** and monitoring\n",
    "- ✅ **Production deployment** best practices\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ What We'll Build\n",
    "\n",
    "**Production F1 ETL Pipeline:**\n",
    "```\n",
    "🔄 DLT Pipeline:\n",
    "├── 📊 Live Tables (batch processing)\n",
    "├── 🌊 Streaming Tables (real-time ingestion)\n",
    "├── ✅ Data Quality Expectations\n",
    "├── 📈 Automatic Lineage Tracking\n",
    "└── 🔍 Pipeline Monitoring Dashboard\n",
    "```\n",
    "\n",
    "### 💡 DLT Pipeline Creation:\n",
    "1. **Navigate** to Workflows → Delta Live Tables\n",
    "2. **Create** new pipeline with F1 transformation logic\n",
    "3. **Define** data quality expectations\n",
    "4. **Configure** compute and scheduling\n",
    "5. **Monitor** pipeline execution and lineage\n",
    "\n",
    "### 🎯 Key Benefits:\n",
    "- **Automatic dependency resolution** (no manual ordering)\n",
    "- **Built-in data quality** monitoring and quarantine\n",
    "- **Schema evolution** handling\n",
    "- **Live monitoring** and alerting\n",
    "- **Cost optimization** with auto-scaling\n",
    "\n",
    "**Continue to the next notebook:** `06_AI_Agent_Bricks.ipynb`\n",
    "\n",
    "**🏁 Ready to explore AI-powered applications? Let's dive into intelligent F1 analytics! 🤖**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec3b15",
   "metadata": {},
   "source": [
    "# 🔄 Delta Live Tables: Managed ETL Pipeline\n",
    "*Build declarative, production-ready data pipelines in 5 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- ✅ **Delta Live Tables (DLT) fundamentals** and decorators\n",
    "- ✅ **Data quality expectations** with built-in monitoring\n",
    "- ✅ **Managed pipeline execution** and dependency management\n",
    "- ✅ **DLT vs Jobs comparison** and when to use each\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c0bdc",
   "metadata": {},
   "source": [
    "## 🏗️ What We'll Build\n",
    "\n",
    "**Managed F1 Data Pipeline with DLT:**\n",
    "```\n",
    "📁 Volume Files           🥉 DLT Bronze              🥈 DLT Silver              🥇 DLT Gold\n",
    "┌─────────────────┐      ┌─────────────────┐       ┌─────────────────┐       ┌─────────────────┐\n",
    "│ drivers.csv     │  →   │ dlt_bronze_     │   →   │ dlt_silver_     │   →   │ dlt_gold_driver_│\n",
    "│ results.csv     │      │ drivers         │       │ drivers_clean   │       │ stats           │\n",
    "└─────────────────┘      │ dlt_bronze_     │       │ dlt_silver_     │       │ dlt_gold_top_   │\n",
    "                         │ results         │       │ results_clean   │       │ performers      │\n",
    "                         └─────────────────┘       └─────────────────┘       └─────────────────┘\n",
    "```\n",
    "\n",
    "**🔥 Key Features:**\n",
    "- ⚡ **Declarative syntax** - focus on what, not how\n",
    "- 🎯 **Data quality expectations** - automatic monitoring\n",
    "- 🔄 **Automatic dependency management** - smart execution order\n",
    "- 📊 **Built-in monitoring** - pipeline health and lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925f3b6",
   "metadata": {},
   "source": [
    "## 📋 Delta Live Tables Overview\n",
    "\n",
    "**Delta Live Tables (DLT)** is Databricks' framework for building reliable, maintainable, and testable data processing pipelines.\n",
    "\n",
    "### 🌟 Key Benefits:\n",
    "\n",
    "#### 🎯 **Declarative Development**\n",
    "- Write **what** you want, not **how** to compute it\n",
    "- Automatic dependency resolution and execution order\n",
    "- Focus on business logic, not infrastructure\n",
    "\n",
    "#### 🔍 **Built-in Data Quality**\n",
    "- **Expectations** define data quality rules\n",
    "- **Quarantine** bad data instead of failing pipelines  \n",
    "- **Monitoring** with automatic quality metrics\n",
    "\n",
    "#### ⚡ **Managed Operations**\n",
    "- **Auto-scaling** compute based on workload\n",
    "- **Error recovery** and automatic retries\n",
    "- **Incremental processing** for efficiency\n",
    "\n",
    "#### 📊 **Observability**\n",
    "- **Lineage tracking** shows data flow\n",
    "- **Performance metrics** and optimization suggestions\n",
    "- **Data freshness** monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c034e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314086b",
   "metadata": {},
   "source": [
    "## 🥉 Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "Bronze tables in DLT ingest raw data with minimal transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaaef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_bronze_drivers\",\n",
    "    comment=\"DLT Bronze: Raw driver data from Volume CSV files\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def bronze_drivers():\n",
    "    \"\"\"\n",
    "    Ingest raw driver data from Volume CSV files.\n",
    "    DLT automatically handles schema inference and evolution.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/main/default/f1_raw_data/dlt_schema/drivers/\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/main/default/f1_raw_data/drivers.csv\")\n",
    "        .select(\n",
    "            col(\"driverId\").cast(\"int\").alias(\"driver_id\"),\n",
    "            col(\"driverRef\").alias(\"driver_ref\"),\n",
    "            col(\"number\").alias(\"car_number\"),\n",
    "            col(\"code\").alias(\"driver_code\"),\n",
    "            col(\"forename\").alias(\"first_name\"),\n",
    "            col(\"surname\").alias(\"last_name\"),\n",
    "            col(\"dob\").alias(\"date_of_birth\"),\n",
    "            col(\"nationality\"),\n",
    "            col(\"url\").alias(\"wiki_url\"),\n",
    "            current_timestamp().alias(\"ingested_at\"),\n",
    "            lit(\"dlt_bronze_pipeline\").alias(\"ingestion_method\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_bronze_results\",\n",
    "    comment=\"DLT Bronze: Raw race results data from Volume CSV files\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def bronze_results():\n",
    "    \"\"\"\n",
    "    Ingest raw race results data from Volume CSV files.\n",
    "    Includes all original columns for complete data preservation.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/main/default/f1_raw_data/dlt_schema/results/\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/main/default/f1_raw_data/results.csv\")\n",
    "        .select(\n",
    "            col(\"resultId\").cast(\"int\").alias(\"result_id\"),\n",
    "            col(\"raceId\").cast(\"int\").alias(\"race_id\"),\n",
    "            col(\"driverId\").cast(\"int\").alias(\"driver_id\"),\n",
    "            col(\"constructorId\").cast(\"int\").alias(\"constructor_id\"),\n",
    "            col(\"number\").alias(\"car_number\"),\n",
    "            col(\"grid\").cast(\"int\").alias(\"grid_position\"),\n",
    "            col(\"position\").alias(\"finish_position_text\"),\n",
    "            col(\"positionOrder\").cast(\"int\").alias(\"position_order\"),\n",
    "            col(\"points\").cast(\"double\").alias(\"points_scored\"),\n",
    "            col(\"laps\").cast(\"int\").alias(\"laps_completed\"),\n",
    "            col(\"time\").alias(\"race_time\"),\n",
    "            col(\"milliseconds\").alias(\"race_time_ms\"),\n",
    "            col(\"statusId\").cast(\"int\").alias(\"status_id\"),\n",
    "            current_timestamp().alias(\"ingested_at\"),\n",
    "            lit(\"dlt_bronze_pipeline\").alias(\"ingestion_method\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad0439",
   "metadata": {},
   "source": [
    "## 🥈 Silver Layer: Clean and Validated Data\n",
    "\n",
    "Silver tables implement data quality expectations and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_silver_drivers_clean\",\n",
    "    comment=\"DLT Silver: Cleaned and validated driver data with quality expectations\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_driver_id\", \"driver_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_names\", \"first_name IS NOT NULL AND last_name IS NOT NULL\")\n",
    "@dlt.expect(\"reasonable_birth_date\", \"date_of_birth >= '1900-01-01' AND date_of_birth <= current_date()\")\n",
    "def silver_drivers_clean():\n",
    "    \"\"\"\n",
    "    Clean and validate driver data with data quality expectations.\n",
    "    - Drop records with missing critical fields\n",
    "    - Warn on suspicious birth dates\n",
    "    - Enrich with calculated fields\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"dlt_bronze_drivers\")\n",
    "        .filter(col(\"driver_id\").isNotNull())\n",
    "        .select(\n",
    "            col(\"driver_id\"),\n",
    "            col(\"driver_ref\"),\n",
    "            col(\"car_number\"),\n",
    "            col(\"driver_code\"),\n",
    "            col(\"first_name\"),\n",
    "            col(\"last_name\"),\n",
    "            concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias(\"full_name\"),\n",
    "            # Clean and convert date of birth\n",
    "            when(col(\"date_of_birth\") != \"\\\\N\", \n",
    "                 to_date(col(\"date_of_birth\"), \"yyyy-MM-dd\")\n",
    "            ).alias(\"birth_date\"),\n",
    "            col(\"nationality\"),\n",
    "            # Calculate current age\n",
    "            when(col(\"date_of_birth\") != \"\\\\N\",\n",
    "                 floor(datediff(current_date(), to_date(col(\"date_of_birth\"), \"yyyy-MM-dd\")) / 365)\n",
    "            ).alias(\"current_age\"),\n",
    "            col(\"wiki_url\"),\n",
    "            col(\"ingested_at\"),\n",
    "            current_timestamp().alias(\"processed_at\"),\n",
    "            lit(\"silver_quality_processed\").alias(\"processing_stage\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ca4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_silver_results_clean\",\n",
    "    comment=\"DLT Silver: Cleaned race results with data quality validations\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_result_id\", \"result_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_race_and_driver\", \"race_id IS NOT NULL AND driver_id IS NOT NULL\")\n",
    "@dlt.expect(\"valid_points\", \"points_scored >= 0\")\n",
    "@dlt.expect(\"reasonable_laps\", \"laps_completed >= 0 AND laps_completed <= 200\")\n",
    "def silver_results_clean():\n",
    "    \"\"\"\n",
    "    Clean and validate race results with comprehensive quality checks.\n",
    "    - Ensure critical IDs are present\n",
    "    - Validate points and lap counts\n",
    "    - Convert position data to proper types\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"dlt_bronze_results\")\n",
    "        .filter(col(\"result_id\").isNotNull())\n",
    "        .select(\n",
    "            col(\"result_id\"),\n",
    "            col(\"race_id\"),\n",
    "            col(\"driver_id\"),\n",
    "            col(\"constructor_id\"),\n",
    "            col(\"car_number\"),\n",
    "            col(\"grid_position\"),\n",
    "            # Clean finish position - handle DNF, DNS, etc.\n",
    "            when(col(\"finish_position_text\").rlike(\"^[0-9]+$\"), \n",
    "                 col(\"finish_position_text\").cast(\"int\")\n",
    "            ).alias(\"finish_position\"),\n",
    "            col(\"finish_position_text\"),\n",
    "            col(\"position_order\"),\n",
    "            col(\"points_scored\"),\n",
    "            col(\"laps_completed\"),\n",
    "            col(\"race_time\"),\n",
    "            # Convert milliseconds if numeric\n",
    "            when(col(\"race_time_ms\").rlike(\"^[0-9]+$\"),\n",
    "                 col(\"race_time_ms\").cast(\"bigint\")\n",
    "            ).alias(\"race_duration_ms\"),\n",
    "            col(\"status_id\"),\n",
    "            # Add derived fields\n",
    "            when(col(\"points_scored\") > 0, true).otherwise(false).alias(\"scored_points\"),\n",
    "            when(col(\"finish_position_text\") == \"1\", true).otherwise(false).alias(\"race_winner\"),\n",
    "            when(col(\"finish_position_text\").isin(\"1\", \"2\", \"3\"), true).otherwise(false).alias(\"podium_finish\"),\n",
    "            col(\"ingested_at\"),\n",
    "            current_timestamp().alias(\"processed_at\"),\n",
    "            lit(\"silver_quality_processed\").alias(\"processing_stage\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa851d06",
   "metadata": {},
   "source": [
    "## 🥇 Gold Layer: Analytics-Ready Aggregations\n",
    "\n",
    "Gold tables provide business-ready analytics with complex aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_gold_driver_stats\",\n",
    "    comment=\"DLT Gold: Comprehensive driver career statistics and performance metrics\"\n",
    ")\n",
    "@dlt.expect(\"drivers_with_races\", \"total_races > 0\")\n",
    "def gold_driver_stats():\n",
    "    \"\"\"\n",
    "    Calculate comprehensive driver career statistics.\n",
    "    Aggregates from clean silver tables to create analytics-ready data.\n",
    "    \"\"\"\n",
    "    drivers = dlt.read(\"dlt_silver_drivers_clean\")\n",
    "    results = dlt.read(\"dlt_silver_results_clean\")\n",
    "    \n",
    "    return (\n",
    "        drivers.alias(\"d\")\n",
    "        .join(results.alias(\"r\"), col(\"d.driver_id\") == col(\"r.driver_id\"), \"inner\")\n",
    "        .groupBy(\n",
    "            col(\"d.driver_id\"),\n",
    "            col(\"d.full_name\"),\n",
    "            col(\"d.nationality\"),\n",
    "            col(\"d.current_age\"),\n",
    "            col(\"d.birth_date\")\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"r.result_id\").alias(\"total_races\"),\n",
    "            sum(\"r.points_scored\").alias(\"career_points\"),\n",
    "            sum(when(col(\"r.race_winner\"), 1).otherwise(0)).alias(\"wins\"),\n",
    "            sum(when(col(\"r.podium_finish\"), 1).otherwise(0)).alias(\"podiums\"),\n",
    "            sum(when(col(\"r.scored_points\"), 1).otherwise(0)).alias(\"points_finishes\"),\n",
    "            avg(\"r.finish_position\").alias(\"avg_finish_position\"),\n",
    "            min(\"r.finish_position\").alias(\"best_finish\"),\n",
    "            max(\"r.finish_position\").alias(\"worst_finish\"),\n",
    "            sum(\"r.laps_completed\").alias(\"total_laps\"),\n",
    "            # Performance ratios\n",
    "            round(sum(\"r.points_scored\") / count(\"r.result_id\"), 2).alias(\"points_per_race\"),\n",
    "            round(sum(when(col(\"r.race_winner\"), 1).otherwise(0)) * 100.0 / count(\"r.result_id\"), 2).alias(\"win_percentage\"),\n",
    "            round(sum(when(col(\"r.podium_finish\"), 1).otherwise(0)) * 100.0 / count(\"r.result_id\"), 2).alias(\"podium_percentage\"),\n",
    "            # Data lineage\n",
    "            current_timestamp().alias(\"calculated_at\"),\n",
    "            lit(\"dlt_gold_aggregation\").alias(\"calculation_method\")\n",
    "        )\n",
    "        .filter(col(\"total_races\") >= 1)  # Only drivers with actual race participation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eeaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dlt_gold_top_performers\",\n",
    "    comment=\"DLT Gold: Top performing drivers across different performance categories\"\n",
    ")\n",
    "@dlt.expect(\"performance_categories\", \"performance_tier IS NOT NULL\")\n",
    "def gold_top_performers():\n",
    "    \"\"\"\n",
    "    Create performance tiers and identify top performers.\n",
    "    Builds on driver stats to create business-friendly categorizations.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"dlt_gold_driver_stats\")\n",
    "        .select(\n",
    "            col(\"driver_id\"),\n",
    "            col(\"full_name\"),\n",
    "            col(\"nationality\"),\n",
    "            col(\"total_races\"),\n",
    "            col(\"career_points\"),\n",
    "            col(\"wins\"),\n",
    "            col(\"podiums\"),\n",
    "            col(\"points_per_race\"),\n",
    "            col(\"win_percentage\"),\n",
    "            col(\"podium_percentage\"),\n",
    "            # Create performance tiers\n",
    "            when(col(\"wins\") >= 20, \"F1 Legend\")\n",
    "            .when(col(\"wins\") >= 5, \"Race Winner\")\n",
    "            .when(col(\"podiums\") >= 10, \"Podium Regular\")\n",
    "            .when(col(\"career_points\") >= 100, \"Points Scorer\")\n",
    "            .when(col(\"total_races\") >= 20, \"Veteran\")\n",
    "            .otherwise(\"Rookie\").alias(\"performance_tier\"),\n",
    "            # Excellence indicators\n",
    "            when(col(\"win_percentage\") >= 25, \"Elite Winner\")\n",
    "            .when(col(\"podium_percentage\") >= 50, \"Consistent Podium\")\n",
    "            .when(col(\"points_per_race\") >= 5, \"Strong Performer\")\n",
    "            .otherwise(\"Developing\").alias(\"consistency_rating\"),\n",
    "            # Experience categorization\n",
    "            when(col(\"total_races\") >= 200, \"Ultra Veteran\")\n",
    "            .when(col(\"total_races\") >= 100, \"Veteran\")\n",
    "            .when(col(\"total_races\") >= 50, \"Experienced\")\n",
    "            .otherwise(\"Newcomer\").alias(\"experience_level\"),\n",
    "            col(\"calculated_at\"),\n",
    "            current_timestamp().alias(\"categorized_at\")\n",
    "        )\n",
    "        .filter(col(\"total_races\") >= 5)  # Focus on drivers with meaningful careers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58dcfd",
   "metadata": {},
   "source": [
    "## 📊 Data Quality Expectations Explained\n",
    "\n",
    "DLT provides powerful data quality features through **expectations**:\n",
    "\n",
    "### 🎯 Expectation Types:\n",
    "\n",
    "#### 1. `@dlt.expect()`\n",
    "```python\n",
    "@dlt.expect(\"reasonable_birth_date\", \"date_of_birth >= '1900-01-01'\")\n",
    "```\n",
    "- **Behavior:** Records violation but continues processing\n",
    "- **Use case:** Data quality monitoring and alerts\n",
    "- **Result:** Violating records included in output with quality metrics tracked\n",
    "\n",
    "#### 2. `@dlt.expect_or_drop()`\n",
    "```python\n",
    "@dlt.expect_or_drop(\"valid_driver_id\", \"driver_id IS NOT NULL\")\n",
    "```\n",
    "- **Behavior:** Drops records that fail the expectation\n",
    "- **Use case:** Critical data quality requirements\n",
    "- **Result:** Only valid records in output table\n",
    "\n",
    "#### 3. `@dlt.expect_or_fail()`\n",
    "```python\n",
    "@dlt.expect_or_fail(\"critical_data\", \"COUNT(*) > 0\")\n",
    "```\n",
    "- **Behavior:** Stops pipeline execution if expectation fails\n",
    "- **Use case:** Critical business rules that cannot be violated\n",
    "- **Result:** Pipeline failure with clear error message\n",
    "\n",
    "### 📈 Quality Monitoring:\n",
    "- **Automatic dashboards** show data quality trends\n",
    "- **Alerting** when quality degrades\n",
    "- **Historical tracking** of data quality over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795661d",
   "metadata": {},
   "source": [
    "## 🚀 DLT Pipeline Creation Guide\n",
    "\n",
    "### 📋 How to Create a DLT Pipeline:\n",
    "\n",
    "#### 1. Navigate to Delta Live Tables 🔄\n",
    "- Click **\"Workflows\"** in the left sidebar\n",
    "- Click **\"Delta Live Tables\"** tab\n",
    "- Click **\"Create Pipeline\"** button\n",
    "\n",
    "#### 2. Configure Pipeline Settings ⚙️\n",
    "```\n",
    "Pipeline Name: \"F1 Data Pipeline with DLT\"\n",
    "Description: \"Managed ETL pipeline for Formula 1 analytics with data quality\"\n",
    "```\n",
    "\n",
    "#### 3. Source Configuration 📝\n",
    "- **Source Type:** `Notebook`\n",
    "- **Notebook Path:** Select this notebook (`05_Delta_Live_Pipeline.ipynb`)\n",
    "- **Source:** Your workspace location\n",
    "\n",
    "#### 4. Target Configuration 🎯\n",
    "```\n",
    "Target Catalog: main\n",
    "Target Schema: default\n",
    "Storage Location: Managed (Unity Catalog)\n",
    "```\n",
    "\n",
    "#### 5. Compute Configuration ⚡\n",
    "```\n",
    "Cluster Mode: Serverless (recommended)\n",
    "Min Workers: 1\n",
    "Max Workers: 5 (auto-scaling)\n",
    "```\n",
    "\n",
    "#### 6. Advanced Settings 🎛️\n",
    "- **Pipeline Mode:** `Triggered` (manual) or `Continuous` (streaming)\n",
    "- **Channel:** `Current` (latest features)\n",
    "- **Edition:** `Advanced` (for expectations and monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bc0b9",
   "metadata": {},
   "source": [
    "## 🔄 DLT vs Jobs: When to Use Each\n",
    "\n",
    "### 🏗️ **Use Delta Live Tables When:**\n",
    "\n",
    "✅ **Complex ETL with dependencies**\n",
    "- Multiple transformation layers (Bronze → Silver → Gold)\n",
    "- Automatic dependency resolution needed\n",
    "- Schema evolution and data quality critical\n",
    "\n",
    "✅ **Data quality is paramount**\n",
    "- Need built-in expectations and monitoring\n",
    "- Automatic quarantine of bad data\n",
    "- Quality metrics and alerting required\n",
    "\n",
    "✅ **Streaming and incremental processing**\n",
    "- Near real-time data processing\n",
    "- Change data capture (CDC) patterns\n",
    "- Efficient incremental updates\n",
    "\n",
    "✅ **Team collaboration on pipelines**\n",
    "- Declarative code is easier to understand\n",
    "- Built-in lineage and documentation\n",
    "- Standardized patterns across teams\n",
    "\n",
    "### ⚙️ **Use Jobs When:**\n",
    "\n",
    "✅ **Simple, scheduled tasks**\n",
    "- Single notebook execution\n",
    "- Basic data refresh operations\n",
    "- Notification and reporting workflows\n",
    "\n",
    "✅ **Custom orchestration logic**\n",
    "- Complex conditional workflows\n",
    "- Integration with external systems\n",
    "- Custom retry and error handling\n",
    "\n",
    "✅ **Ad-hoc or exploratory processing**\n",
    "- One-time data migration\n",
    "- Experimental data processing\n",
    "- Quick fixes and patches\n",
    "\n",
    "### 📊 **Feature Comparison:**\n",
    "\n",
    "| **Feature** | **Delta Live Tables** | **Jobs** |\n",
    "|-------------|----------------------|----------|\n",
    "| **Dependency Management** | ✅ Automatic | ⚙️ Manual |\n",
    "| **Data Quality** | ✅ Built-in expectations | ⚙️ Custom code |\n",
    "| **Streaming** | ✅ Native support | ⚙️ Structured streaming |\n",
    "| **Monitoring** | ✅ Automatic dashboards | ⚙️ Custom monitoring |\n",
    "| **Cost** | 💰 DLT premium | 💰 Standard compute |\n",
    "| **Flexibility** | 🎯 Declarative patterns | 🔧 Full control |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72231828",
   "metadata": {},
   "source": [
    "## 📈 Advanced DLT Features\n",
    "\n",
    "### 🔄 **Change Data Capture (CDC)**\n",
    "```python\n",
    "@dlt.table\n",
    "def customers_cdc():\n",
    "    return dlt.read_stream(\"customers_raw\").apply_changes(\n",
    "        keys=[\"customer_id\"],\n",
    "        sequence_by=\"update_timestamp\",\n",
    "        apply_as_deletes=expr(\"operation = 'DELETE'\"),\n",
    "        except_column_list=[\"operation\", \"update_timestamp\"]\n",
    "    )\n",
    "```\n",
    "\n",
    "### 📊 **Pipeline Dependencies**\n",
    "```python\n",
    "# Automatic dependency resolution\n",
    "@dlt.table\n",
    "def downstream_table():\n",
    "    return dlt.read(\"upstream_table_1\").join(dlt.read(\"upstream_table_2\"))\n",
    "```\n",
    "\n",
    "### 🎯 **Custom Expectations**\n",
    "```python\n",
    "@dlt.expect_or_fail(\"freshness_check\", \"max(update_time) > current_timestamp() - interval 1 day\")\n",
    "def time_sensitive_data():\n",
    "    return dlt.read(\"source_data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674a710",
   "metadata": {},
   "source": [
    "## ✅ Delta Live Tables Complete!\n",
    "\n",
    "**🎉 Outstanding! You've mastered Delta Live Tables fundamentals!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ **Built DLT pipeline** with Bronze, Silver, and Gold layers\n",
    "- ✅ **Implemented data quality expectations** for automatic monitoring\n",
    "- ✅ **Used declarative transformations** with automatic dependencies\n",
    "- ✅ **Learned DLT vs Jobs** comparison and use cases\n",
    "- ✅ **Explored advanced features** (CDC, streaming, quality monitoring)\n",
    "\n",
    "### 🏗️ Your DLT Pipeline Architecture:\n",
    "```\n",
    "📁 Volume CSV Files\n",
    "    ↓ (Auto Loader)\n",
    "🥉 DLT Bronze Tables (Raw ingestion)\n",
    "    ↓ (Quality expectations)\n",
    "🥈 DLT Silver Tables (Clean & validated)  \n",
    "    ↓ (Business aggregations)\n",
    "🥇 DLT Gold Tables (Analytics ready)\n",
    "```\n",
    "\n",
    "### 📊 Tables Created:\n",
    "- **Bronze:** `dlt_bronze_drivers`, `dlt_bronze_results`\n",
    "- **Silver:** `dlt_silver_drivers_clean`, `dlt_silver_results_clean`  \n",
    "- **Gold:** `dlt_gold_driver_stats`, `dlt_gold_top_performers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show what DLT tables would be created (this is descriptive since DLT runs in pipeline mode)\n",
    "print(\"🔄 Delta Live Tables Pipeline Summary\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "dlt_tables = [\n",
    "    \"🥉 dlt_bronze_drivers - Raw driver data ingestion\",\n",
    "    \"🥉 dlt_bronze_results - Raw results data ingestion\", \n",
    "    \"🥈 dlt_silver_drivers_clean - Validated driver data\",\n",
    "    \"🥈 dlt_silver_results_clean - Validated results data\",\n",
    "    \"🥇 dlt_gold_driver_stats - Driver career statistics\",\n",
    "    \"🥇 dlt_gold_top_performers - Performance categorization\"\n",
    "]\n",
    "\n",
    "for table in dlt_tables:\n",
    "    print(f\"{table}\")\n",
    "\n",
    "print(f\"\\n📊 Data Quality: Built-in expectations and monitoring\")\n",
    "print(f\"🔄 Dependencies: Automatic resolution and execution order\")\n",
    "print(f\"⚡ Compute: Serverless managed infrastructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178526f",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Ready to explore AI-powered features and advanced analytics?\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **🔄 Create Your DLT Pipeline:**\n",
    "   - Go to Workflows → Delta Live Tables → Create Pipeline\n",
    "   - Use this notebook as the source\n",
    "   - Configure with Serverless compute\n",
    "\n",
    "2. **📊 Monitor Pipeline Execution:**\n",
    "   - Watch automatic dependency resolution\n",
    "   - Check data quality expectation results\n",
    "   - Explore generated lineage graphs\n",
    "\n",
    "3. **➡️ Next Notebook:** [06_AI_Agent_Bricks.ipynb](06_AI_Agent_Bricks.ipynb)\n",
    "   - Explore AI Agents and intelligent applications\n",
    "   - Build F1 Q&A chatbots with your data\n",
    "\n",
    "### 🎯 Best Practices Checklist:\n",
    "- ✅ **Start simple** with basic Bronze → Silver → Gold\n",
    "- ✅ **Add expectations gradually** as you understand your data\n",
    "- ✅ **Use descriptive table names** for clarity\n",
    "- ✅ **Document transformations** with comments\n",
    "- ✅ **Monitor data quality** trends over time\n",
    "- ✅ **Test expectations** before production deployment\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- **🔧 Start with `@dlt.expect()`** to understand data patterns\n",
    "- **📊 Use DLT dashboards** for quality monitoring\n",
    "- **⚡ Leverage Serverless** for cost-effective execution\n",
    "- **🔄 Design for incremental processing** from day one\n",
    "\n",
    "**🔄 Your data pipelines are now production-ready! 🚀**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
