{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56baca84",
   "metadata": {},
   "source": [
    "# 🤖 Databricks One: Your AI-Powered Assistant\n",
    "*Accelerate your F1 analytics workflow with intelligent platform assistance*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this guide, you'll understand:\n",
    "- ✅ **Databricks One fundamentals** and AI assistant capabilities\n",
    "- ✅ **Intelligent code generation** for F1 data analysis\n",
    "- ✅ **Productivity acceleration** through AI-powered workflows\n",
    "- ✅ **Platform navigation assistance** and feature discovery\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60a68e",
   "metadata": {},
   "source": [
    "## 🧠 What is Databricks One?\n",
    "\n",
    "**Databricks One** is your AI-powered platform assistant that helps you work more efficiently across the entire Databricks platform. Think of it as your intelligent copilot for data and AI workflows.\n",
    "\n",
    "### 🌟 Key Capabilities:\n",
    "\n",
    "#### 💻 **Intelligent Code Generation**\n",
    "- Generate Python, SQL, and Scala code from natural language\n",
    "- Context-aware suggestions based on your data\n",
    "- Best practices and optimization recommendations\n",
    "\n",
    "#### 🧭 **Platform Navigation**\n",
    "- Find features and functionality quickly\n",
    "- Step-by-step guidance for complex tasks\n",
    "- Troubleshooting and problem resolution\n",
    "\n",
    "#### 📊 **Data Analysis Assistance**\n",
    "- Automated data profiling and insights\n",
    "- Visualization recommendations\n",
    "- Query optimization suggestions\n",
    "\n",
    "#### 🎓 **Learning and Discovery**\n",
    "- Feature explanations and tutorials\n",
    "- Best practice recommendations\n",
    "- Documentation and resource finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5d9e5",
   "metadata": {},
   "source": [
    "## 🏎️ Databricks One for F1 Analytics\n",
    "\n",
    "Let's explore how Databricks One can accelerate your F1 data analysis workflow!\n",
    "\n",
    "### 🚀 Getting Started with Databricks One\n",
    "\n",
    "**[Screenshot: Databricks One interface showing the AI assistant chat and code generation]**\n",
    "*📁 Image location: `images/10_databricks_one_interface.png`*\n",
    "*Screenshot guidance: Show the Databricks One chat interface with an F1-related query and generated code response*\n",
    "\n",
    "#### Accessing Databricks One:\n",
    "1. **Look for the AI assistant icon** (🤖) in your workspace\n",
    "2. **Click to open chat interface** in the right sidebar\n",
    "3. **Start typing your question** or request\n",
    "4. **Get intelligent responses** with actionable code and guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66dfcfc",
   "metadata": {},
   "source": [
    "### 💻 Code Generation Examples\n",
    "\n",
    "#### Example 1: F1 Data Analysis\n",
    "```\n",
    "You: \"Help me analyze which F1 drivers have the most consistent performance\"\n",
    "\n",
    "Databricks One: I'll help you analyze driver consistency! Here's a comprehensive approach:\n",
    "\n",
    "```python\n",
    "# Calculate driver consistency metrics\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Read your F1 driver standings data\n",
    "drivers_df = spark.table(\"main.default.gold_driver_standings\")\n",
    "\n",
    "# Calculate consistency metrics\n",
    "consistency_analysis = drivers_df.select(\n",
    "    \"full_name\",\n",
    "    \"total_races\",\n",
    "    \"wins\",\n",
    "    \"podiums\",\n",
    "    \"points_per_race\",\n",
    "    \"win_percentage\",\n",
    "    # Consistency score (podiums per race)\n",
    "    round(col(\"podiums\") / col(\"total_races\") * 100, 2).alias(\"podium_consistency_pct\"),\n",
    "    # Points consistency (points per race)\n",
    "    round(col(\"points_per_race\"), 2).alias(\"points_consistency\"),\n",
    "    # Overall consistency ranking\n",
    "    when(col(\"total_races\") >= 50, \"Veteran\")\n",
    "    .when(col(\"total_races\") >= 20, \"Experienced\")\n",
    "    .otherwise(\"Newcomer\").alias(\"experience_level\")\n",
    ").filter(col(\"total_races\") >= 20)  # Focus on experienced drivers\n",
    "\n",
    "# Show most consistent performers\n",
    "consistency_analysis.orderBy(\n",
    "    col(\"podium_consistency_pct\").desc()\n",
    ").show(15)\n",
    "```\n",
    "\n",
    "💡 **Next steps:**\n",
    "- Run this analysis on your F1 data\n",
    "- Create visualizations to show consistency trends\n",
    "- Filter by different eras or nationalities\n",
    "- Compare consistency vs raw performance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac8c5a",
   "metadata": {},
   "source": [
    "#### Example 2: Advanced F1 Queries\n",
    "```\n",
    "You: \"Create a SQL query to find F1 drivers who peaked late in their careers\"\n",
    "\n",
    "Databricks One: Great question! Let me create a query to identify late-career peak performers:\n",
    "\n",
    "```sql\n",
    "-- Find drivers who achieved their best results in the latter half of their careers\n",
    "WITH driver_career_analysis AS (\n",
    "  SELECT \n",
    "    d.full_name,\n",
    "    d.nationality,\n",
    "    d.total_races,\n",
    "    d.wins,\n",
    "    d.win_percentage,\n",
    "    -- Calculate career phases\n",
    "    CASE \n",
    "      WHEN r.race_year <= d.career_start + (d.career_length / 2) THEN 'Early Career'\n",
    "      ELSE 'Late Career'\n",
    "    END as career_phase,\n",
    "    COUNT(r.race_winner) as wins_in_phase,\n",
    "    COUNT(*) as races_in_phase\n",
    "  FROM main.default.gold_driver_standings d\n",
    "  JOIN main.default.silver_results r ON d.driverId = r.driverId  \n",
    "  JOIN main.default.silver_races ra ON r.raceId = ra.raceId\n",
    "  WHERE d.total_races >= 50  -- Focus on drivers with substantial careers\n",
    "  GROUP BY d.full_name, d.nationality, d.total_races, d.wins, d.win_percentage, career_phase\n",
    "),\n",
    "late_bloomers AS (\n",
    "  SELECT \n",
    "    full_name,\n",
    "    nationality,\n",
    "    total_races,\n",
    "    wins,\n",
    "    MAX(CASE WHEN career_phase = 'Early Career' THEN wins_in_phase ELSE 0 END) as early_wins,\n",
    "    MAX(CASE WHEN career_phase = 'Late Career' THEN wins_in_phase ELSE 0 END) as late_wins,\n",
    "    MAX(CASE WHEN career_phase = 'Late Career' THEN wins_in_phase ELSE 0 END) - \n",
    "    MAX(CASE WHEN career_phase = 'Early Career' THEN wins_in_phase ELSE 0 END) as late_career_advantage\n",
    "  FROM driver_career_analysis\n",
    "  GROUP BY full_name, nationality, total_races, wins\n",
    ")\n",
    "SELECT \n",
    "  full_name,\n",
    "  nationality,\n",
    "  total_races,\n",
    "  wins,\n",
    "  early_wins,\n",
    "  late_wins,\n",
    "  late_career_advantage,\n",
    "  ROUND(late_wins * 100.0 / wins, 1) as late_career_win_percentage\n",
    "FROM late_bloomers\n",
    "WHERE late_career_advantage > 0  -- More wins in late career\n",
    "  AND wins >= 5  -- At least 5 total wins\n",
    "ORDER BY late_career_advantage DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "🎯 **This query identifies:**\n",
    "- Drivers who won more races in their career's second half\n",
    "- Late-career improvement patterns\n",
    "- Experience vs youth performance dynamics\n",
    "\n",
    "💡 **Pro tip:** You might find drivers like Nigel Mansell or Damon Hill who had their championship success later in their careers!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08c040",
   "metadata": {},
   "source": [
    "#### Example 3: Data Visualization Help\n",
    "```\n",
    "You: \"How do I create a compelling visualization showing F1 era evolution?\"\n",
    "\n",
    "Databricks One: Excellent visualization idea! Here's how to create an engaging F1 era evolution chart:\n",
    "\n",
    "```python\n",
    "# F1 Era Evolution Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1. Prepare era-based data\n",
    "era_data = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    CASE \n",
    "      WHEN year BETWEEN 1950 AND 1959 THEN '1950s - Birth of F1'\n",
    "      WHEN year BETWEEN 1960 AND 1969 THEN '1960s - Speed Demons'\n",
    "      WHEN year BETWEEN 1970 AND 1979 THEN '1970s - Safety Revolution'\n",
    "      WHEN year BETWEEN 1980 AND 1989 THEN '1980s - Turbo Era'\n",
    "      WHEN year BETWEEN 1990 AND 1999 THEN '1990s - Tech Innovation'\n",
    "      WHEN year BETWEEN 2000 AND 2009 THEN '2000s - Schumacher Era'\n",
    "      WHEN year BETWEEN 2010 AND 2019 THEN '2010s - Hybrid Power'\n",
    "      ELSE '2020s - New Generation'\n",
    "    END as era,\n",
    "    AVG(completion_rate) as avg_completion_rate,\n",
    "    COUNT(DISTINCT unique_drivers) as total_drivers,\n",
    "    AVG(unique_drivers) as avg_drivers_per_season,\n",
    "    COUNT(*) as total_seasons\n",
    "  FROM main.default.gold_season_stats\n",
    "  GROUP BY \n",
    "    CASE \n",
    "      WHEN year BETWEEN 1950 AND 1959 THEN '1950s - Birth of F1'\n",
    "      WHEN year BETWEEN 1960 AND 1969 THEN '1960s - Speed Demons'\n",
    "      WHEN year BETWEEN 1970 AND 1979 THEN '1970s - Safety Revolution'\n",
    "      WHEN year BETWEEN 1980 AND 1989 THEN '1980s - Turbo Era'\n",
    "      WHEN year BETWEEN 1990 AND 1999 THEN '1990s - Tech Innovation'\n",
    "      WHEN year BETWEEN 2000 AND 2009 THEN '2000s - Schumacher Era'\n",
    "      WHEN year BETWEEN 2010 AND 2019 THEN '2010s - Hybrid Power'\n",
    "      ELSE '2020s - New Generation'\n",
    "    END\n",
    "  ORDER BY MIN(year)\n",
    "\"\"\")\n",
    "\n",
    "# 2. Convert to Pandas for visualization\n",
    "era_df = era_data.toPandas()\n",
    "\n",
    "# 3. Create multi-panel visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Formula 1: Evolution Across Eras', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Completion rate evolution\n",
    "ax1.plot(era_df['era'], era_df['avg_completion_rate'], marker='o', linewidth=3, markersize=8)\n",
    "ax1.set_title('Race Completion Rate Evolution')\n",
    "ax1.set_ylabel('Completion Rate (%)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Driver participation trends\n",
    "ax2.bar(era_df['era'], era_df['avg_drivers_per_season'], color='skyblue')\n",
    "ax2.set_title('Average Drivers per Season')\n",
    "ax2.set_ylabel('Number of Drivers')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Total unique drivers per era\n",
    "ax3.barh(era_df['era'], era_df['total_drivers'], color='lightcoral')\n",
    "ax3.set_title('Total Unique Drivers per Era')\n",
    "ax3.set_xlabel('Number of Drivers')\n",
    "\n",
    "# Era timeline\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(era_df)))\n",
    "ax4.scatter(era_df['total_seasons'], era_df['avg_completion_rate'], \n",
    "           s=era_df['total_drivers']*3, c=colors, alpha=0.7)\n",
    "ax4.set_xlabel('Seasons in Era')\n",
    "ax4.set_ylabel('Completion Rate')\n",
    "ax4.set_title('Era Characteristics (bubble size = drivers)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "🎨 **Visualization Tips:**\n",
    "- Use F1-themed colors (red, black, white)\n",
    "- Add era annotations with key innovations\n",
    "- Include interactive elements for exploration\n",
    "- Export high-resolution versions for presentations\n",
    "\n",
    "📊 **Alternative tools:**\n",
    "- Databricks visualizations for quick exploration\n",
    "- Plotly for interactive charts\n",
    "- Tableau/Power BI for executive dashboards\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96abb1",
   "metadata": {},
   "source": [
    "### 🧭 Platform Navigation Assistance\n",
    "\n",
    "#### Example 1: Feature Discovery\n",
    "```\n",
    "You: \"I want to automate my F1 data pipeline. What's the best approach in Databricks?\"\n",
    "\n",
    "Databricks One: Great question! For automating F1 data pipelines, you have several excellent options:\n",
    "\n",
    "🔄 **Recommended Approach: Delta Live Tables**\n",
    "✅ **Why DLT is perfect for F1 data:**\n",
    "- Declarative pipeline definition (focus on what, not how)\n",
    "- Built-in data quality expectations for race data validation\n",
    "- Automatic dependency management between Bronze→Silver→Gold layers\n",
    "- Real-time monitoring and lineage tracking\n",
    "\n",
    "📋 **Step-by-step setup:**\n",
    "1. Navigate to **Workflows** → **Delta Live Tables**\n",
    "2. Click **\"Create Pipeline\"**\n",
    "3. Configure with your F1 transformation notebook\n",
    "4. Set up Serverless compute for cost efficiency\n",
    "5. Define data quality expectations for critical metrics\n",
    "\n",
    "⚙️ **Alternative: Databricks Jobs**\n",
    "✅ **Use Jobs when you need:**\n",
    "- Simple scheduled notebook execution\n",
    "- Custom orchestration logic\n",
    "- Integration with external F1 data APIs\n",
    "- Email notifications for race weekend updates\n",
    "\n",
    "🎯 **Pro tip:** Start with DLT for your main F1 pipeline, then use Jobs for supplementary tasks like sending race weekend reports!\n",
    "\n",
    "Would you like me to help you set up either approach?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ffb39",
   "metadata": {},
   "source": [
    "#### Example 2: Troubleshooting Help\n",
    "```\n",
    "You: \"My F1 data query is running slowly. How can I optimize it?\"\n",
    "\n",
    "Databricks One: I'll help you optimize your F1 query performance! Here's my diagnostic approach:\n",
    "\n",
    "🔍 **Quick Performance Diagnostics:**\n",
    "\n",
    "1. **Check your query execution plan:**\n",
    "```sql\n",
    "EXPLAIN EXTENDED \n",
    "SELECT * FROM main.default.gold_driver_standings \n",
    "WHERE nationality = 'British' AND wins > 10;\n",
    "```\n",
    "\n",
    "2. **Common F1 data optimization strategies:**\n",
    "\n",
    "⚡ **Table Optimizations:**\n",
    "```sql\n",
    "-- Optimize your F1 tables\n",
    "OPTIMIZE main.default.gold_driver_standings ZORDER BY (nationality, wins);\n",
    "OPTIMIZE main.default.silver_results ZORDER BY (driverId, raceId);\n",
    "```\n",
    "\n",
    "📊 **Query Improvements:**\n",
    "```python\n",
    "# Instead of this (slow):\n",
    "slow_query = spark.sql(\"\"\"\n",
    "    SELECT d.full_name, COUNT(r.race_winner) as wins\n",
    "    FROM drivers d \n",
    "    LEFT JOIN results r ON d.driverId = r.driverId \n",
    "    WHERE r.race_winner = true\n",
    "    GROUP BY d.full_name\n",
    "\"\"\")\n",
    "\n",
    "# Use this (fast - pre-aggregated):\n",
    "fast_query = spark.sql(\"\"\"\n",
    "    SELECT full_name, wins \n",
    "    FROM main.default.gold_driver_standings\n",
    "    WHERE wins > 0\n",
    "    ORDER BY wins DESC\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "🎯 **Specific to F1 data patterns:**\n",
    "- **Partition by season/year** for time-based queries\n",
    "- **Cache frequently accessed driver standings**\n",
    "- **Use broadcast joins** for small lookup tables (circuits, constructors)\n",
    "- **Pre-filter on nationality/era** before complex joins\n",
    "\n",
    "📈 **Performance monitoring:**\n",
    "```python\n",
    "# Monitor query performance\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = your_f1_query.collect()\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Query executed in {execution_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "Would you like me to analyze a specific query you're having trouble with?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3369a2",
   "metadata": {},
   "source": [
    "### 🎓 Learning and Discovery\n",
    "\n",
    "#### Example 1: Feature Learning\n",
    "```\n",
    "You: \"I keep hearing about Unity Catalog. How does it help with my F1 data project?\"\n",
    "\n",
    "Databricks One: Unity Catalog is perfect for F1 data projects! Let me explain why:\n",
    "\n",
    "🗄️ **What Unity Catalog Does for F1 Data:**\n",
    "\n",
    "📊 **Organized Data Structure:**\n",
    "```\n",
    "f1_catalog\n",
    "├── bronze_schema (raw race data)\n",
    "│   ├── drivers_raw\n",
    "│   ├── results_raw\n",
    "│   └── races_raw\n",
    "├── silver_schema (cleaned data)\n",
    "│   ├── drivers_clean\n",
    "│   ├── results_validated\n",
    "│   └── races_enriched\n",
    "└── gold_schema (analytics-ready)\n",
    "    ├── driver_standings\n",
    "    ├── season_stats\n",
    "    └── career_analysis\n",
    "```\n",
    "\n",
    "🔍 **Data Discovery Benefits:**\n",
    "- **Search F1 tables** by driver name, season, or race type\n",
    "- **Automatic lineage** shows how Hamilton's stats flow from raw data\n",
    "- **Rich metadata** with descriptions like \"Career wins including sprint races\"\n",
    "- **Column-level lineage** traces individual metrics through transformations\n",
    "\n",
    "🔒 **Governance for F1 Data:**\n",
    "- **Row-level security:** Show only specific seasons to certain users\n",
    "- **Column masking:** Hide sensitive timing data from non-technical users\n",
    "- **Audit trails:** Track who accessed championship-sensitive data\n",
    "\n",
    "🚀 **Getting Started with Your F1 Data:**\n",
    "1. **Create catalogs** for different F1 data domains (historical, current, predictions)\n",
    "2. **Set up schemas** for your bronze/silver/gold architecture\n",
    "3. **Add rich descriptions** to tables (\"Driver career statistics from 1950-2023\")\n",
    "4. **Tag tables** by era, category, or data quality level\n",
    "\n",
    "💡 **Pro tip:** Use Unity Catalog's lineage feature to show stakeholders exactly how you calculate championship probabilities from raw race results!\n",
    "\n",
    "Want me to help you set up your F1 catalog structure?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0ba25",
   "metadata": {},
   "source": [
    "#### Example 2: Best Practices Guidance\n",
    "```\n",
    "You: \"What are the best practices for organizing my F1 analytics project?\"\n",
    "\n",
    "Databricks One: Excellent question! Here's a comprehensive F1 analytics project structure:\n",
    "\n",
    "📁 **Recommended F1 Project Organization:**\n",
    "\n",
    "```\n",
    "F1_Analytics_Project/\n",
    "├── 📂 data_ingestion/\n",
    "│   ├── 01_raw_data_ingestion.py\n",
    "│   ├── 02_api_connectors.py\n",
    "│   └── 03_data_validation.py\n",
    "├── 📂 data_processing/\n",
    "│   ├── bronze_to_silver_drivers.py\n",
    "│   ├── bronze_to_silver_results.py\n",
    "│   └── silver_to_gold_analytics.py\n",
    "├── 📂 analysis/\n",
    "│   ├── driver_performance_analysis.py\n",
    "│   ├── historical_trend_analysis.py\n",
    "│   └── championship_predictions.py\n",
    "├── 📂 dashboards/\n",
    "│   ├── executive_f1_dashboard.sql\n",
    "│   ├── driver_comparison_dashboard.sql\n",
    "│   └── race_weekend_dashboard.sql\n",
    "├── 📂 utilities/\n",
    "│   ├── f1_data_quality_checks.py\n",
    "│   ├── performance_optimization.py\n",
    "│   └── common_transformations.py\n",
    "└── 📂 documentation/\n",
    "    ├── data_dictionary.md\n",
    "    ├── business_logic.md\n",
    "    └── deployment_guide.md\n",
    "```\n",
    "\n",
    "🎯 **Best Practices for F1 Data:**\n",
    "\n",
    "**1. Data Layer Standards:**\n",
    "```python\n",
    "# Bronze Layer - Raw data preservation\n",
    "bronze_drivers = (\n",
    "    spark.read.csv(\"/path/to/raw/drivers.csv\")\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Silver Layer - Cleaned and validated\n",
    "silver_drivers = (\n",
    "    bronze_drivers\n",
    "    .filter(col(\"driverId\").isNotNull())  # Data quality\n",
    "    .withColumn(\"full_name\", concat(col(\"forename\"), lit(\" \"), col(\"surname\")))\n",
    "    .withColumn(\"current_age\", \n",
    "        floor(datediff(current_date(), col(\"dob\")) / 365))\n",
    ")\n",
    "\n",
    "# Gold Layer - Business-ready analytics\n",
    "gold_driver_standings = (\n",
    "    silver_drivers.join(aggregated_results, \"driverId\")\n",
    "    .select(\"full_name\", \"nationality\", \"career_wins\", \"championship_titles\")\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Naming Conventions:**\n",
    "- **Tables:** `{layer}_{entity}_{purpose}` (e.g., `gold_driver_championship_analysis`)\n",
    "- **Columns:** Clear, descriptive names (`race_finish_position` not `pos`)\n",
    "- **Files:** `{sequence}_{purpose}_{entity}` (e.g., `01_ingest_race_results.py`)\n",
    "\n",
    "**3. Documentation Standards:**\n",
    "```python\n",
    "# Document your F1 business logic\n",
    "def calculate_championship_points(finish_position):\n",
    "    \"\"\"\n",
    "    Calculate F1 championship points based on finishing position.\n",
    "    \n",
    "    Uses current F1 points system (2010+):\n",
    "    1st: 25 points, 2nd: 18 points, 3rd: 15 points, etc.\n",
    "    \n",
    "    Args:\n",
    "        finish_position (int): Race finishing position (1-20)\n",
    "        \n",
    "    Returns:\n",
    "        int: Championship points earned\n",
    "    \"\"\"\n",
    "    points_map = {1: 25, 2: 18, 3: 15, 4: 12, 5: 10, \n",
    "                  6: 8, 7: 6, 8: 4, 9: 2, 10: 1}\n",
    "    return points_map.get(finish_position, 0)\n",
    "```\n",
    "\n",
    "**4. Testing and Quality:**\n",
    "```python\n",
    "# Add data quality tests\n",
    "def test_driver_data_quality(df):\n",
    "    \"\"\"\n",
    "    Validate F1 driver data quality.\n",
    "    \"\"\"\n",
    "    # Check for duplicates\n",
    "    assert df.count() == df.select(\"driverId\").distinct().count()\n",
    "    \n",
    "    # Validate data ranges\n",
    "    assert df.filter(col(\"wins\") < 0).count() == 0\n",
    "    assert df.filter(col(\"current_age\") > 100).count() == 0\n",
    "    \n",
    "    print(\"✅ Driver data quality checks passed!\")\n",
    "```\n",
    "\n",
    "💡 **Pro Tips:**\n",
    "- **Version control** all notebooks in Git\n",
    "- **Use parameters** for easy environment switching\n",
    "- **Implement logging** for production debugging\n",
    "- **Create reusable functions** for common F1 calculations\n",
    "\n",
    "Would you like help implementing any of these practices in your F1 project?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9736845",
   "metadata": {},
   "source": [
    "## 🚀 Productivity Acceleration Tips\n",
    "\n",
    "### ⚡ Quick Wins with Databricks One\n",
    "\n",
    "#### 1. **Rapid Code Generation**\n",
    "- **Describe what you want** in natural language\n",
    "- **Get instant code** for common F1 analysis patterns\n",
    "- **Iterate quickly** with modifications and improvements\n",
    "- **Learn new techniques** through generated examples\n",
    "\n",
    "#### 2. **Smart Error Resolution**\n",
    "```\n",
    "Error: \"AnalysisException: Column 'driver_name' doesn't exist\"\n",
    "\n",
    "You: \"Help me fix this column error in my F1 query\"\n",
    "\n",
    "Databricks One: I see the issue! Your table likely uses 'full_name' instead of 'driver_name'. Here's the fix:\n",
    "\n",
    "```sql\n",
    "-- Instead of:\n",
    "SELECT driver_name, wins FROM main.default.gold_driver_standings\n",
    "\n",
    "-- Use:\n",
    "SELECT full_name, wins FROM main.default.gold_driver_standings\n",
    "\n",
    "-- Or check available columns:\n",
    "DESCRIBE main.default.gold_driver_standings;\n",
    "```\n",
    "\n",
    "💡 Always check your table schema first with DESCRIBE or show the first few rows with LIMIT 5!\n",
    "```\n",
    "\n",
    "#### 3. **Feature Discovery**\n",
    "- **Ask about capabilities** you haven't explored yet\n",
    "- **Get guided tours** of complex features\n",
    "- **Learn shortcuts** and productivity hacks\n",
    "- **Stay updated** on new platform features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8b319",
   "metadata": {},
   "source": [
    "### 🎯 Advanced Databricks One Techniques\n",
    "\n",
    "#### Context-Aware Assistance\n",
    "```\n",
    "You: \"I'm working on F1 driver clustering analysis. What's the best approach?\"\n",
    "\n",
    "Databricks One: [Sees you're in a notebook with F1 data loaded]\n",
    "\n",
    "Perfect! I can see you have F1 driver data loaded. For clustering analysis, here's a tailored approach:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Use your existing F1 data for clustering\n",
    "f1_features = spark.table(\"main.default.gold_driver_standings\").select(\n",
    "    \"full_name\",\n",
    "    \"wins\",\n",
    "    \"podiums\", \n",
    "    \"points_per_race\",\n",
    "    \"win_percentage\",\n",
    "    \"total_races\"\n",
    ").filter(col(\"total_races\") >= 20)  # Focus on experienced drivers\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"wins\", \"podiums\", \"points_per_race\", \"win_percentage\", \"total_races\"],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Scale features for fair comparison\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# K-means clustering (try k=4 for F1 tiers)\n",
    "kmeans = KMeans(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster\",\n",
    "    k=4,  # Legends, Stars, Regulars, Rookies\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "model = pipeline.fit(f1_features)\n",
    "clustered_drivers = model.transform(f1_features)\n",
    "\n",
    "# Analyze clusters\n",
    "clustered_drivers.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"driver_count\"),\n",
    "    avg(\"wins\").alias(\"avg_wins\"),\n",
    "    avg(\"win_percentage\").alias(\"avg_win_pct\")\n",
    ").orderBy(\"avg_wins\").show()\n",
    "```\n",
    "\n",
    "💡 Based on your F1 data, I expect to see clusters like:\n",
    "- **Cluster 0:** Legends (Hamilton, Schumacher level)\n",
    "- **Cluster 1:** Race winners (Multiple wins, strong performers)\n",
    "- **Cluster 2:** Point scorers (Consistent but fewer wins)\n",
    "- **Cluster 3:** Participation-focused (Many races, fewer points)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f22dae",
   "metadata": {},
   "source": [
    "## 💡 Best Practices for Databricks One\n",
    "\n",
    "### 🎯 Effective Prompting\n",
    "- ✅ **Be specific** about your F1 data context\n",
    "- ✅ **Mention your goals** (analysis, visualization, optimization)\n",
    "- ✅ **Include relevant details** (table names, column names, constraints)\n",
    "- ✅ **Ask for explanations** when learning new concepts\n",
    "\n",
    "### 🔄 Iterative Improvement\n",
    "```\n",
    "Good progression:\n",
    "1. \"Help me analyze F1 driver performance\"\n",
    "2. \"Can you modify that to focus on drivers from the 2000s?\"\n",
    "3. \"Add win percentage and podium rates to the analysis\"\n",
    "4. \"Create a visualization showing these trends over time\"\n",
    "```\n",
    "\n",
    "### 📚 Learning Integration\n",
    "- **Ask \"why\"** to understand the reasoning behind suggestions\n",
    "- **Request alternatives** to learn different approaches\n",
    "- **Explore edge cases** specific to F1 data peculiarities\n",
    "- **Save useful patterns** for future F1 projects\n",
    "\n",
    "### 🔧 Troubleshooting Tips\n",
    "- **Share error messages** for specific debugging help\n",
    "- **Describe expected vs actual results** for logic issues\n",
    "- **Mention performance problems** for optimization guidance\n",
    "- **Ask about best practices** for production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1141ad",
   "metadata": {},
   "source": [
    "## ✅ Databricks One Complete!\n",
    "\n",
    "**🎉 Outstanding! You've mastered AI-powered productivity with Databricks One!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ **Understood Databricks One** capabilities and AI assistance features\n",
    "- ✅ **Explored code generation** for F1 data analysis workflows\n",
    "- ✅ **Learned platform navigation** with intelligent guidance\n",
    "- ✅ **Discovered productivity acceleration** through AI-powered workflows\n",
    "- ✅ **Applied best practices** for effective AI assistant interaction\n",
    "\n",
    "### 🤖 Your AI-Powered Workflow:\n",
    "```\n",
    "💭 Natural Language Request\n",
    "    ↓\n",
    "🧠 AI Understanding & Context Analysis\n",
    "    ↓\n",
    "💻 Intelligent Code Generation & Guidance\n",
    "    ↓\n",
    "🚀 Accelerated F1 Analytics Development\n",
    "```\n",
    "\n",
    "### 🎯 Key Capabilities Unlocked:\n",
    "- **Smart code generation** for complex F1 analysis\n",
    "- **Instant troubleshooting** for common issues\n",
    "- **Feature discovery** and platform navigation\n",
    "- **Best practices guidance** for professional development\n",
    "- **Learning acceleration** through interactive assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57b9cb",
   "metadata": {},
   "source": [
    "## 🏁 Workshop Complete!\n",
    "\n",
    "**🎉 Congratulations! You've completed the comprehensive Databricks 101 Workshop!**\n",
    "\n",
    "### 🏆 Your F1 Analytics Journey:\n",
    "```\n",
    "01. Platform Tour         → Navigation mastery\n",
    "02. Notebook Tour        → Complete F1 pipeline built  \n",
    "03. Unity Catalog        → Data governance & lineage\n",
    "04. Job Creation         → Automation & scheduling\n",
    "05. Delta Live Tables    → Managed ETL pipelines\n",
    "06. AI Agent Bricks      → Intelligent applications\n",
    "07. SQL Editor          → Analytics & visualization\n",
    "08. Dashboard Template   → Executive reporting\n",
    "09. Genie Rooms         → Natural language queries\n",
    "10. Databricks One      → AI-powered assistance\n",
    "```\n",
    "\n",
    "### 🚀 What's Next?\n",
    "\n",
    "#### Immediate Actions:\n",
    "1. **🏗️ Build your production pipeline** using the patterns learned\n",
    "2. **📊 Create executive dashboards** for stakeholder reporting  \n",
    "3. **🤖 Deploy AI agents** for intelligent data exploration\n",
    "4. **🧞 Set up Genie Rooms** for business user self-service\n",
    "\n",
    "#### Advanced Learning:\n",
    "- **📚 Explore Databricks Academy** for deep-dive courses\n",
    "- **🎓 Pursue certifications** (Data Engineer, Data Analyst, ML Practitioner)\n",
    "- **👥 Join community forums** for ongoing learning and support\n",
    "- **🔬 Experiment with real datasets** in your domain\n",
    "\n",
    "#### Production Readiness:\n",
    "- **🔒 Implement security** and governance controls\n",
    "- **📈 Set up monitoring** and alerting systems\n",
    "- **💰 Optimize costs** with right-sizing and scheduling\n",
    "- **🔄 Establish CI/CD** for production deployments\n",
    "\n",
    "### 💡 Remember:\n",
    "- **🎯 Start simple** and build complexity gradually\n",
    "- **👥 Collaborate** with your team and stakeholders\n",
    "- **🔄 Iterate** based on user feedback and requirements\n",
    "- **🤖 Leverage AI** to accelerate your development\n",
    "\n",
    "**🏁 Welcome to the world of modern data and AI with Databricks! 🚀**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
