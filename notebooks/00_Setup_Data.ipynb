{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96dcf2d",
   "metadata": {},
   "source": [
    "# üèÅ Formula 1 Workshop: Data Setup\n",
    "\n",
    "*This notebook prepares all data needed for the Databricks 101 Workshop by downloading F1 datasets and loading them into a volume.*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Do\n",
    "\n",
    "This setup notebook will:\n",
    "1. **Create a Volume** to store raw data files\n",
    "2. **Download F1 datasets** from GitHub \n",
    "3. **Save CSV files** to the Volume\n",
    "4. **Verify data** is properly loaded\n",
    "\n",
    "**Note:** Run this notebook first before proceeding to the workshop materials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4b98d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Create a Volume for Raw Data Storage\n",
    "\n",
    "First, let's create a volume in the Unity Catalog to store our raw F1 data files. \n",
    "Volumes are ideal for storing unstructured data like CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a volume to store our raw data\n",
    "# We'll use the main catalog and default schema\n",
    "\n",
    "try:\n",
    "    # Check if volume already exists\n",
    "    spark.sql(\"DESCRIBE VOLUME main.default.f1_raw_data\")\n",
    "    print(\"‚úÖ Volume 'main.default.f1_raw_data' already exists.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        # Volume doesn't exist, create it\n",
    "        spark.sql(\"CREATE VOLUME IF NOT EXISTS main.default.f1_raw_data\")\n",
    "        print(\"‚úÖ Volume 'main.default.f1_raw_data' created successfully.\")\n",
    "    else:\n",
    "        # Some other error occurred\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "# Display the volume information\n",
    "display(spark.sql(\"DESCRIBE VOLUME main.default.f1_raw_data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1177eca",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download F1 Datasets from GitHub\n",
    "\n",
    "Next, we'll download the Formula 1 datasets from the GitHub repository.\n",
    "We'll focus on these key files:\n",
    "- **races.csv** - Information about F1 races\n",
    "- **drivers.csv** - Information about F1 drivers\n",
    "- **results.csv** - Race results and performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4786c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Set the GitHub repository URL\n",
    "github_repo = \"https://github.com/toUpperCase78/formula1-datasets\"\n",
    "raw_content_base = \"https://raw.githubusercontent.com/toUpperCase78/formula1-datasets/master\"\n",
    "\n",
    "# Define the files we want to download\n",
    "files_to_download = [\n",
    "    \"races.csv\",\n",
    "    \"drivers.csv\", \n",
    "    \"results.csv\"\n",
    "]\n",
    "\n",
    "# Set volume path\n",
    "volume_path = \"/Volumes/main/default/f1_raw_data/\"\n",
    "\n",
    "# Ensure the volume directory exists\n",
    "os.makedirs(volume_path, exist_ok=True)\n",
    "\n",
    "# Function to download and save files\n",
    "def download_and_save(filename):\n",
    "    # Construct the URL\n",
    "    url = f\"{raw_content_base}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Save to volume\n",
    "        file_path = os.path.join(volume_path, filename)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "            \n",
    "        # Get file size for verification\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"status\": \"Success\",\n",
    "            \"size_bytes\": file_size,\n",
    "            \"path\": file_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"status\": \"Failed\",\n",
    "            \"error\": str(e),\n",
    "            \"path\": None\n",
    "        }\n",
    "\n",
    "# Download each file and collect results\n",
    "download_results = []\n",
    "for file in files_to_download:\n",
    "    print(f\"Downloading {file}...\")\n",
    "    result = download_and_save(file)\n",
    "    download_results.append(result)\n",
    "    if result[\"status\"] == \"Success\":\n",
    "        print(f\"‚úÖ Successfully downloaded {file} ({result['size_bytes']:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {file}: {result['error']}\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "download_df = spark.createDataFrame(download_results)\n",
    "display(download_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170131fc",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify File Contents\n",
    "\n",
    "Let's take a quick look at the files we've downloaded to ensure they contain the expected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36433a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the volume\n",
    "import os\n",
    "volume_files = os.listdir(volume_path)\n",
    "\n",
    "print(f\"Files in {volume_path}:\")\n",
    "for file in volume_files:\n",
    "    file_path = os.path.join(volume_path, file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"- {file} ({file_size:,} bytes)\")\n",
    "\n",
    "# Function to peek at CSV files\n",
    "def peek_csv(file_path, num_lines=5):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [f.readline().strip() for _ in range(num_lines)]\n",
    "    return lines\n",
    "\n",
    "# Peek at each CSV file\n",
    "for file in files_to_download:\n",
    "    if file in volume_files:\n",
    "        file_path = os.path.join(volume_path, file)\n",
    "        print(f\"\\nüìÑ {file} (first 5 lines):\")\n",
    "        lines = peek_csv(file_path)\n",
    "        for i, line in enumerate(lines):\n",
    "            print(f\"  {i+1}: {line[:100]}...\" if len(line) > 100 else f\"  {i+1}: {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2411f",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Create Schema Directory for DLT\n",
    "\n",
    "Delta Live Tables requires a schema location directory. Let's create that structure now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for DLT schema\n",
    "dlt_schema_dir = os.path.join(volume_path, \"dlt_schema\")\n",
    "os.makedirs(dlt_schema_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for each table\n",
    "for file in files_to_download:\n",
    "    table_name = file.split('.')[0]  # Remove file extension\n",
    "    table_schema_dir = os.path.join(dlt_schema_dir, table_name)\n",
    "    os.makedirs(table_schema_dir, exist_ok=True)\n",
    "    \n",
    "print(f\"‚úÖ Created DLT schema directories in {dlt_schema_dir}\")\n",
    "\n",
    "# List the directory structure\n",
    "def list_dir_structure(path, indent=0):\n",
    "    print(' ' * indent + 'üìÅ ' + os.path.basename(path))\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            list_dir_structure(item_path, indent + 2)\n",
    "        else:\n",
    "            print(' ' * (indent + 2) + 'üìÑ ' + item)\n",
    "\n",
    "print(\"\\nVolume Directory Structure:\")\n",
    "list_dir_structure(volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b46c7",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create a Schema Discovery Function for Future Use\n",
    "\n",
    "Let's create a function that can be used in other notebooks to preview the schema of our raw CSV files.\n",
    "This will help when creating tables and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preview file schema\n",
    "def preview_csv_schema(file_name):\n",
    "    file_path = f\"/Volumes/main/default/f1_raw_data/{file_name}\"\n",
    "    \n",
    "    # Read the CSV file with header and infer schema\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(file_path)\n",
    "    \n",
    "    # Print schema information\n",
    "    print(f\"üìä Schema for {file_name}:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nüìã Sample data from {file_name}:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preview schemas for our files\n",
    "for file in files_to_download:\n",
    "    if file in volume_files:\n",
    "        preview_csv_schema(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75ab33",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "- ‚úÖ Created a Volume for raw data storage\n",
    "- ‚úÖ Downloaded F1 datasets from GitHub\n",
    "- ‚úÖ Saved CSV files to the Volume\n",
    "- ‚úÖ Verified the data is properly loaded\n",
    "- ‚úÖ Created necessary directory structure for DLT\n",
    "- ‚úÖ Previewed data schemas for future use\n",
    "\n",
    "**Next Steps:**\n",
    "- Continue to [01_Platform_Tour.ipynb](01_Platform_Tour.ipynb) to start exploring the Databricks platform\n",
    "- Follow through each notebook in the workshop sequence\n",
    "- Use the data you've just prepared for your F1 analytics journey!\n",
    "\n",
    "The following data files are now available in the Volume:\n",
    "```\n",
    "/Volumes/main/default/f1_raw_data/\n",
    "‚îú‚îÄ‚îÄ races.csv\n",
    "‚îú‚îÄ‚îÄ drivers.csv\n",
    "‚îú‚îÄ‚îÄ results.csv\n",
    "‚îî‚îÄ‚îÄ dlt_schema/\n",
    "    ‚îú‚îÄ‚îÄ races/\n",
    "    ‚îú‚îÄ‚îÄ drivers/\n",
    "    ‚îî‚îÄ‚îÄ results/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
