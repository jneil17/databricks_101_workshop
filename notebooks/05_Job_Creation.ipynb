{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c664270c-3434-4673-bbdd-4c801406cbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📅 Job Creation: Automate F1 Data Pipeline\n",
    "*Schedule and automate your F1 data workflows*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- ✅ **Creating Databricks Jobs** for automation\n",
    "- ✅ **Scheduling workflows** for regular data updates\n",
    "- ✅ **Monitoring job execution** and handling failures\n",
    "- ✅ **Best practices** for production job management\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What We'll Build\n",
    "\n",
    "**Automated F1 Data Pipeline Job:**\n",
    "```\n",
    "📅 Scheduled Job:\n",
    "├── 🔄 Daily F1 data refresh (6 AM UTC)\n",
    "├── 📧 Email notifications on success/failure\n",
    "├── 🔁 Retry policies for resilience\n",
    "└── 📊 Performance monitoring and alerts\n",
    "```\n",
    "\n",
    "### 🎯 Use Cases:\n",
    "- **Race weekend data updates** (automated Sunday evening)\n",
    "- **Season statistics refresh** (weekly aggregations)\n",
    "- **Performance monitoring** (daily driver rankings)\n",
    "- **Data quality checks** (validation and alerts)\n",
    "\n",
    "**🏁 Ready to build production ETL pipelines? Let's explore Delta Live Tables! 🏎️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a911ddb2-d796-4bd0-a681-967b02d43718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⏰ Job Creation: Automate Your Data Pipelines\n",
    "*Learn to schedule and monitor data workflows in 3 minutes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521218f6-7c92-452c-99b0-03b5225488a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 What We'll Build\n",
    "\n",
    "**Automated Data Refresh Job:**\n",
    "```\n",
    "📅 Daily Schedule (6 AM)\n",
    "    ↓\n",
    "🔄 Refresh Driver Standings\n",
    "    ↓  \n",
    "📊 Update job_driver_standings_daily\n",
    "    ↓\n",
    "📝 Log Execution Status\n",
    "    ↓\n",
    "📧 Send Alerts (if needed)\n",
    "```\n",
    "\n",
    "**🎯 Goal:** Create a production-ready job that can run automatically to keep our F1 data fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd2dd19-4e35-42ff-9de4-95380305a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 Step 1: Create Job-Ready Data Table\n",
    "\n",
    "First, let's create a table that our job will refresh daily with the latest F1 driver standings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575d4178-d083-4431-a6ed-13b2788c0999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table for daily driver standings refresh using race results\n",
    "CREATE OR REPLACE TABLE main.default.f1_job_driver_standings_daily\n",
    "USING DELTA\n",
    "COMMENT 'Daily refreshed driver standings - maintained by automated job'\n",
    "AS\n",
    "WITH driver_points AS (\n",
    "  SELECT\n",
    "    driver,\n",
    "    team,\n",
    "    SUM(points) AS total_points,\n",
    "    COUNT(*) AS total_races,\n",
    "    SUM(CASE WHEN position = '1' THEN 1 ELSE 0 END) AS wins,\n",
    "    SUM(CASE WHEN position IN ('1','2','3') THEN 1 ELSE 0 END) AS podiums\n",
    "  FROM main.default.f1_bronze_race_results\n",
    "  GROUP BY driver, team\n",
    "),\n",
    "standings AS (\n",
    "  SELECT\n",
    "    driver,\n",
    "    team,\n",
    "    total_points,\n",
    "    total_races,\n",
    "    wins,\n",
    "    podiums,\n",
    "    ROUND(total_points / total_races, 2) AS points_per_race,\n",
    "    ROUND(wins * 100.0 / total_races, 2) AS win_percentage\n",
    "  FROM driver_points\n",
    ")\n",
    "SELECT\n",
    "  driver AS full_name,\n",
    "  team,\n",
    "  total_points,\n",
    "  wins,\n",
    "  podiums,\n",
    "  total_races,\n",
    "  points_per_race,\n",
    "  win_percentage,\n",
    "  -- Add job execution metadata\n",
    "  'manual_creation' as refresh_method,\n",
    "  current_timestamp() as last_updated,\n",
    "  current_user() as updated_by\n",
    "FROM standings\n",
    "ORDER BY total_points DESC, wins DESC, podiums DESC, full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc80b20a-96fe-43c3-9c07-5d5740183861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify our job table was created and check all required columns\n",
    "SELECT \n",
    "  'job_driver_standings_daily' as table_name,\n",
    "  COUNT(*) as driver_count,\n",
    "  MAX(last_updated) as last_refresh,\n",
    "  MAX(updated_by) as last_updated_by,\n",
    "  COUNT(DISTINCT full_name) as full_name_count,\n",
    "  COUNT(DISTINCT team) as team_count,\n",
    "  COUNT(DISTINCT total_points) as total_points_count,\n",
    "  COUNT(wins) as wins_count,\n",
    "  COUNT(podiums) as podiums_count,\n",
    "  COUNT(total_races) as total_races_count,\n",
    "  COUNT(points_per_race) as points_per_race_count,\n",
    "  COUNT(win_percentage) as win_percentage_count,\n",
    "  COUNT(DISTINCT refresh_method) as refresh_method_count\n",
    "FROM main.default.f1_job_driver_standings_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b901b517-4ee5-43e9-9d45-3e44ecaf29d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🏗️ Step 2: Complete Job Creation Guide\n",
    "\n",
    "Now let's learn how to create an automated job in the Databricks workspace.\n",
    "\n",
    "### 📋 Job Creation Steps:\n",
    "\n",
    "#### 1. Navigate to Workflows 🔄\n",
    "- Click **\"Jobs & Pipelines\"** in the left sidebar\n",
    "- Click **\"Create\"** and then **\"Job\"**\n",
    "- You'll see the job configuration interface\n",
    "\n",
    "#### 2. Configure Basic Job Settings ⚙️\n",
    "```\n",
    "Job Name: \"F1 Driver Standings Daily Refresh\"\n",
    "Description: \"Automated daily refresh of F1 driver standings data\"\n",
    "```\n",
    "\n",
    "#### 3. Add Job Task 📝\n",
    "- **Task Name:** `refresh_driver_standings`\n",
    "- **Type:** `Notebook`\n",
    "- **Source:** Select this notebook (`04_Job_Creation.ipynb`)\n",
    "- **Cluster:** Choose `Serverless` compute\n",
    "\n",
    "#### 4. Set Schedule ⏰\n",
    "- **Trigger Type:** `Scheduled`\n",
    "- **Schedule:** `0 6 * * *` (Daily at 6 AM)\n",
    "- **Timezone:** Your local timezone\n",
    "\n",
    "#### 5. Configure Notifications 📧\n",
    "- **On Success:** Email notification (optional)\n",
    "- **On Failure:** Email + Slack alert (recommended)\n",
    "- **Recipients:** Your email or team distribution list\n",
    "\n",
    "#### 6. Advanced Options 🎛️\n",
    "- **Max Concurrent Runs:** `1` (prevent overlapping executions)\n",
    "- **Timeout:** `30 minutes` (reasonable for this job)\n",
    "- **Retry Policy:** `Retry 2 times with 5 minute intervals`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc3ed1-c7ea-4144-b757-332954e5a54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 Job Monitoring and Troubleshooting\n",
    "\n",
    "### 🔍 Monitoring Your Jobs:\n",
    "\n",
    "#### Job Run History 📈\n",
    "- **View runs:** Workflows → Your Job → \"Runs\" tab\n",
    "- **Check status:** SUCCESS, FAILED, RUNNING, CANCELED\n",
    "- **View logs:** Click on any run to see detailed logs\n",
    "- **Performance:** Check duration trends over time\n",
    "\n",
    "#### Common Job Issues & Solutions 🔧\n",
    "\n",
    "| **Issue** | **Symptoms** | **Solution** |\n",
    "|-----------|-------------|-------------|\n",
    "| **Timeout** | Job runs too long | Optimize queries, increase timeout |\n",
    "| **Cluster startup** | Slow job start | Use Serverless compute |\n",
    "| **Data skew** | Uneven task performance | Repartition data, optimize joins |\n",
    "| **Memory errors** | OOM exceptions | Increase cluster size, optimize code |\n",
    "| **Dependencies** | Missing tables/files | Check data availability, add retries |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200ac691-6795-4593-b56a-85aa403b4e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 Advanced Job Patterns\n",
    "\n",
    "### Multi-Step Workflows 🔗\n",
    "\n",
    "For complex data pipelines, you can create jobs with multiple tasks:\n",
    "\n",
    "```\n",
    "📥 Task 1: Data Ingestion\n",
    "    ↓\n",
    "🔄 Task 2: Data Transformation  \n",
    "    ↓\n",
    "📊 Task 3: Generate Reports\n",
    "    ↓\n",
    "📧 Task 4: Send Notifications\n",
    "```\n",
    "\n",
    "### Job Dependencies 🔗\n",
    "- **Sequential:** Tasks run one after another\n",
    "- **Parallel:** Multiple tasks run simultaneously  \n",
    "- **Conditional:** Tasks run based on previous results\n",
    "\n",
    "### Resource Management 💰\n",
    "- **Serverless:** Recommended for most jobs (auto-scaling)\n",
    "- **Shared clusters:** Cost-effective for multiple small jobs\n",
    "- **Dedicated clusters:** High-performance critical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d92e4624-19cf-45ec-a3d3-45e687782f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Job Creation Complete!\n",
    "\n",
    "**🎉 Excellent! You've learned how to create production-ready automated jobs!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ **Created job-ready data table** for daily driver standings\n",
    "- ✅ **Built execution logging** for monitoring and debugging\n",
    "- ✅ **Developed job function** with comprehensive error handling\n",
    "- ✅ **Learned job configuration** (scheduling, notifications, monitoring)\n",
    "- ✅ **Explored advanced patterns** (multi-step workflows, dependencies)\n",
    "\n",
    "### 🔄 Your Job Architecture:\n",
    "```\n",
    "⏰ Schedule (Daily 6 AM)\n",
    "    ↓\n",
    "🔄 refresh_driver_standings_job()\n",
    "    ↓\n",
    "📊 job_driver_standings_daily (Updated)\n",
    "    ↓\n",
    "📝 job_run_log (Execution tracked)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b02edc-fce5-455a-8bbd-100b6c983bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Ready to explore more advanced data engineering features?\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **🔄 Create Your Job:** \n",
    "   - Go to Workflows → Create Job\n",
    "   - Follow the configuration guide above\n",
    "   - Schedule your first automated refresh!\n",
    "\n",
    "2. **➡️ Next Notebook:** [05_Delta_Live_Pipeline.ipynb](05_Delta_Live_Pipeline.ipynb)\n",
    "   - Learn about managed ETL pipelines\n",
    "   - Declarative data transformations\n",
    "   - Built-in data quality expectations\n",
    "\n",
    "3. **📊 Monitor Your Jobs:**\n",
    "   - Check the job_run_log table regularly\n",
    "   - Set up email notifications for failures\n",
    "   - Monitor job performance trends\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- **🧪 Test thoroughly** before scheduling in production\n",
    "- **📧 Set up alerts** for job failures (early detection is key)\n",
    "- **📊 Monitor performance** to optimize job runtime\n",
    "- **🔄 Use retries** for transient failures\n",
    "- **📝 Log everything** for easier debugging\n",
    "\n",
    "**⏰ Time to automate your data pipelines! 🚀**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5703089302997148,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Job_Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
