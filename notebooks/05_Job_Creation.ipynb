{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c664270c-3434-4673-bbdd-4c801406cbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸ“… Job Creation: Automate F1 Data Pipeline\n",
    "*Schedule and automate your F1 data workflows*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- âœ… **Creating Databricks Jobs** for automation\n",
    "- âœ… **Scheduling workflows** for regular data updates\n",
    "- âœ… **Monitoring job execution** and handling failures\n",
    "- âœ… **Best practices** for production job management\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ What We'll Build\n",
    "\n",
    "**Automated F1 Data Pipeline Job:**\n",
    "```\n",
    "ğŸ“… Scheduled Job:\n",
    "â”œâ”€â”€ ğŸ”„ Daily F1 data refresh (6 AM UTC)\n",
    "â”œâ”€â”€ ğŸ“§ Email notifications on success/failure\n",
    "â”œâ”€â”€ ğŸ” Retry policies for resilience\n",
    "â””â”€â”€ ğŸ“Š Performance monitoring and alerts\n",
    "```\n",
    "\n",
    "### ğŸ¯ Use Cases:\n",
    "- **Race weekend data updates** (automated Sunday evening)\n",
    "- **Season statistics refresh** (weekly aggregations)\n",
    "- **Performance monitoring** (daily driver rankings)\n",
    "- **Data quality checks** (validation and alerts)\n",
    "\n",
    "**ğŸ Ready to build production ETL pipelines? Let's explore Delta Live Tables! ğŸï¸**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a911ddb2-d796-4bd0-a681-967b02d43718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# â° Job Creation: Automate Your Data Pipelines\n",
    "*Learn to schedule and monitor data workflows in 3 minutes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521218f6-7c92-452c-99b0-03b5225488a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸ”„ What We'll Build\n",
    "\n",
    "**Automated Data Refresh Job:**\n",
    "```\n",
    "ğŸ“… Daily Schedule (6 AM)\n",
    "    â†“\n",
    "ğŸ”„ Refresh Driver Standings\n",
    "    â†“  \n",
    "ğŸ“Š Update job_driver_standings_daily\n",
    "    â†“\n",
    "ğŸ“ Log Execution Status\n",
    "    â†“\n",
    "ğŸ“§ Send Alerts (if needed)\n",
    "```\n",
    "\n",
    "**ğŸ¯ Goal:** Create a production-ready job that can run automatically to keep our F1 data fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd2dd19-4e35-42ff-9de4-95380305a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸ“Š Step 1: Create Job-Ready Data Table\n",
    "\n",
    "First, let's create a table that our job will refresh daily with the latest F1 driver standings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575d4178-d083-4431-a6ed-13b2788c0999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table for daily driver standings refresh using race results\n",
    "CREATE OR REPLACE TABLE main.default.f1_job_driver_standings_daily\n",
    "USING DELTA\n",
    "COMMENT 'Daily refreshed driver standings - maintained by automated job'\n",
    "AS\n",
    "WITH driver_points AS (\n",
    "  SELECT\n",
    "    driver,\n",
    "    team,\n",
    "    SUM(points) AS total_points,\n",
    "    COUNT(*) AS total_races,\n",
    "    SUM(CASE WHEN position = '1' THEN 1 ELSE 0 END) AS wins,\n",
    "    SUM(CASE WHEN position IN ('1','2','3') THEN 1 ELSE 0 END) AS podiums\n",
    "  FROM main.default.f1_bronze_race_results\n",
    "  GROUP BY driver, team\n",
    "),\n",
    "standings AS (\n",
    "  SELECT\n",
    "    driver,\n",
    "    team,\n",
    "    total_points,\n",
    "    total_races,\n",
    "    wins,\n",
    "    podiums,\n",
    "    ROUND(total_points / total_races, 2) AS points_per_race,\n",
    "    ROUND(wins * 100.0 / total_races, 2) AS win_percentage\n",
    "  FROM driver_points\n",
    ")\n",
    "SELECT\n",
    "  driver AS full_name,\n",
    "  team,\n",
    "  total_points,\n",
    "  wins,\n",
    "  podiums,\n",
    "  total_races,\n",
    "  points_per_race,\n",
    "  win_percentage,\n",
    "  -- Add job execution metadata\n",
    "  'manual_creation' as refresh_method,\n",
    "  current_timestamp() as last_updated,\n",
    "  current_user() as updated_by\n",
    "FROM standings\n",
    "ORDER BY total_points DESC, wins DESC, podiums DESC, full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc80b20a-96fe-43c3-9c07-5d5740183861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify our job table was created and check all required columns\n",
    "SELECT \n",
    "  'job_driver_standings_daily' as table_name,\n",
    "  COUNT(*) as driver_count,\n",
    "  MAX(last_updated) as last_refresh,\n",
    "  MAX(updated_by) as last_updated_by,\n",
    "  COUNT(DISTINCT full_name) as full_name_count,\n",
    "  COUNT(DISTINCT team) as team_count,\n",
    "  COUNT(DISTINCT total_points) as total_points_count,\n",
    "  COUNT(wins) as wins_count,\n",
    "  COUNT(podiums) as podiums_count,\n",
    "  COUNT(total_races) as total_races_count,\n",
    "  COUNT(points_per_race) as points_per_race_count,\n",
    "  COUNT(win_percentage) as win_percentage_count,\n",
    "  COUNT(DISTINCT refresh_method) as refresh_method_count\n",
    "FROM main.default.f1_job_driver_standings_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b901b517-4ee5-43e9-9d45-3e44ecaf29d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸ—ï¸ Step 2: Complete Job Creation Guide\n",
    "\n",
    "Now let's learn how to create an automated job in the Databricks workspace.\n",
    "\n",
    "### ğŸ“‹ Job Creation Steps:\n",
    "\n",
    "#### 1. Navigate to Workflows ğŸ”„\n",
    "- Click **\"Jobs & Pipelines\"** in the left sidebar\n",
    "- Click **\"Create\"** and then **\"Job\"**\n",
    "- You'll see the job configuration interface\n",
    "\n",
    "#### 2. Configure Basic Job Settings âš™ï¸\n",
    "```\n",
    "Job Name: \"F1 Driver Standings Daily Refresh\"\n",
    "Description: \"Automated daily refresh of F1 driver standings data\"\n",
    "```\n",
    "\n",
    "#### 3. Add Job Task ğŸ“\n",
    "- **Task Name:** `refresh_driver_standings`\n",
    "- **Type:** `Notebook`\n",
    "- **Source:** Select this notebook (`04_Job_Creation.ipynb`)\n",
    "- **Cluster:** Choose `Serverless` compute\n",
    "\n",
    "#### 4. Set Schedule â°\n",
    "- **Trigger Type:** `Scheduled`\n",
    "- **Schedule:** `0 6 * * *` (Daily at 6 AM)\n",
    "- **Timezone:** Your local timezone\n",
    "\n",
    "#### 5. Configure Notifications ğŸ“§\n",
    "- **On Success:** Email notification (optional)\n",
    "- **On Failure:** Email + Slack alert (recommended)\n",
    "- **Recipients:** Your email or team distribution list\n",
    "\n",
    "#### 6. Advanced Options ğŸ›ï¸\n",
    "- **Max Concurrent Runs:** `1` (prevent overlapping executions)\n",
    "- **Timeout:** `30 minutes` (reasonable for this job)\n",
    "- **Retry Policy:** `Retry 2 times with 5 minute intervals`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc3ed1-c7ea-4144-b757-332954e5a54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸ“Š Job Monitoring and Troubleshooting\n",
    "\n",
    "### ğŸ” Monitoring Your Jobs:\n",
    "\n",
    "#### Job Run History ğŸ“ˆ\n",
    "- **View runs:** Workflows â†’ Your Job â†’ \"Runs\" tab\n",
    "- **Check status:** SUCCESS, FAILED, RUNNING, CANCELED\n",
    "- **View logs:** Click on any run to see detailed logs\n",
    "- **Performance:** Check duration trends over time\n",
    "\n",
    "#### Common Job Issues & Solutions ğŸ”§\n",
    "\n",
    "| **Issue** | **Symptoms** | **Solution** |\n",
    "|-----------|-------------|-------------|\n",
    "| **Timeout** | Job runs too long | Optimize queries, increase timeout |\n",
    "| **Cluster startup** | Slow job start | Use Serverless compute |\n",
    "| **Data skew** | Uneven task performance | Repartition data, optimize joins |\n",
    "| **Memory errors** | OOM exceptions | Increase cluster size, optimize code |\n",
    "| **Dependencies** | Missing tables/files | Check data availability, add retries |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200ac691-6795-4593-b56a-85aa403b4e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸ”„ Advanced Job Patterns\n",
    "\n",
    "### Multi-Step Workflows ğŸ”—\n",
    "\n",
    "For complex data pipelines, you can create jobs with multiple tasks:\n",
    "\n",
    "```\n",
    "ğŸ“¥ Task 1: Data Ingestion\n",
    "    â†“\n",
    "ğŸ”„ Task 2: Data Transformation  \n",
    "    â†“\n",
    "ğŸ“Š Task 3: Generate Reports\n",
    "    â†“\n",
    "ğŸ“§ Task 4: Send Notifications\n",
    "```\n",
    "\n",
    "### Job Dependencies ğŸ”—\n",
    "- **Sequential:** Tasks run one after another\n",
    "- **Parallel:** Multiple tasks run simultaneously  \n",
    "- **Conditional:** Tasks run based on previous results\n",
    "\n",
    "### Resource Management ğŸ’°\n",
    "- **Serverless:** Recommended for most jobs (auto-scaling)\n",
    "- **Shared clusters:** Cost-effective for multiple small jobs\n",
    "- **Dedicated clusters:** High-performance critical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d92e4624-19cf-45ec-a3d3-45e687782f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Job Creation Complete!\n",
    "\n",
    "**ğŸ‰ Excellent! You've learned how to create production-ready automated jobs!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- âœ… **Created job-ready data table** for daily driver standings\n",
    "- âœ… **Built execution logging** for monitoring and debugging\n",
    "- âœ… **Developed job function** with comprehensive error handling\n",
    "- âœ… **Learned job configuration** (scheduling, notifications, monitoring)\n",
    "- âœ… **Explored advanced patterns** (multi-step workflows, dependencies)\n",
    "\n",
    "### ğŸ”„ Your Job Architecture:\n",
    "```\n",
    "â° Schedule (Daily 6 AM)\n",
    "    â†“\n",
    "ğŸ”„ refresh_driver_standings_job()\n",
    "    â†“\n",
    "ğŸ“Š job_driver_standings_daily (Updated)\n",
    "    â†“\n",
    "ğŸ“ job_run_log (Execution tracked)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b02edc-fce5-455a-8bbd-100b6c983bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ğŸš€ Next Steps\n",
    "\n",
    "Ready to explore more advanced data engineering features?\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **ğŸ”„ Create Your Job:** \n",
    "   - Go to Workflows â†’ Create Job\n",
    "   - Follow the configuration guide above\n",
    "   - Schedule your first automated refresh!\n",
    "\n",
    "2. **â¡ï¸ Next Notebook:** [05_Delta_Live_Pipeline.ipynb](05_Delta_Live_Pipeline.ipynb)\n",
    "   - Learn about managed ETL pipelines\n",
    "   - Declarative data transformations\n",
    "   - Built-in data quality expectations\n",
    "\n",
    "3. **ğŸ“Š Monitor Your Jobs:**\n",
    "   - Check the job_run_log table regularly\n",
    "   - Set up email notifications for failures\n",
    "   - Monitor job performance trends\n",
    "\n",
    "### ğŸ’¡ Pro Tips:\n",
    "- **ğŸ§ª Test thoroughly** before scheduling in production\n",
    "- **ğŸ“§ Set up alerts** for job failures (early detection is key)\n",
    "- **ğŸ“Š Monitor performance** to optimize job runtime\n",
    "- **ğŸ”„ Use retries** for transient failures\n",
    "- **ğŸ“ Log everything** for easier debugging\n",
    "\n",
    "**â° Time to automate your data pipelines! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5703089302997148,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Job_Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
