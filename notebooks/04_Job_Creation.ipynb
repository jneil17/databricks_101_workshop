{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c664270c-3434-4673-bbdd-4c801406cbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📅 Job Creation: Automate F1 Data Pipeline\n",
    "*Schedule and automate your F1 data workflows*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll understand:\n",
    "- ✅ **Creating Databricks Jobs** for automation\n",
    "- ✅ **Scheduling workflows** for regular data updates\n",
    "- ✅ **Monitoring job execution** and handling failures\n",
    "- ✅ **Best practices** for production job management\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What We'll Build\n",
    "\n",
    "**Automated F1 Data Pipeline Job:**\n",
    "```\n",
    "📅 Scheduled Job:\n",
    "├── 🔄 Daily F1 data refresh (6 AM UTC)\n",
    "├── 📧 Email notifications on success/failure\n",
    "├── 🔁 Retry policies for resilience\n",
    "└── 📊 Performance monitoring and alerts\n",
    "```\n",
    "\n",
    "### 💡 Job Creation Steps:\n",
    "1. **Navigate** to Workflows → Jobs in the left sidebar\n",
    "2. **Click** \"Create Job\" button\n",
    "3. **Select** the F1 Notebook Tour as the main task\n",
    "4. **Configure** scheduling and notifications\n",
    "5. **Test** the job execution\n",
    "\n",
    "### 🎯 Use Cases:\n",
    "- **Race weekend data updates** (automated Sunday evening)\n",
    "- **Season statistics refresh** (weekly aggregations)\n",
    "- **Performance monitoring** (daily driver rankings)\n",
    "- **Data quality checks** (validation and alerts)\n",
    "\n",
    "**Continue to the next notebook:** `05_Delta_Live_Pipeline.ipynb`\n",
    "\n",
    "**🏁 Ready to build production ETL pipelines? Let's explore Delta Live Tables! 🏎️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a911ddb2-d796-4bd0-a681-967b02d43718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⏰ Job Creation: Automate Your Data Pipelines\n",
    "*Learn to schedule and monitor data workflows in 3 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this demo, you'll know how to:\n",
    "- ✅ **Create automated jobs** to refresh data regularly\n",
    "- ✅ **Configure scheduling** for different business needs\n",
    "- ✅ **Set up monitoring and alerts** for job failures\n",
    "- ✅ **Track job execution** with logging and history\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521218f6-7c92-452c-99b0-03b5225488a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 What We'll Build\n",
    "\n",
    "**Automated Data Refresh Job:**\n",
    "```\n",
    "📅 Daily Schedule (6 AM)\n",
    "    ↓\n",
    "🔄 Refresh Driver Standings\n",
    "    ↓  \n",
    "📊 Update job_driver_standings_daily\n",
    "    ↓\n",
    "📝 Log Execution Status\n",
    "    ↓\n",
    "📧 Send Alerts (if needed)\n",
    "```\n",
    "\n",
    "**🎯 Goal:** Create a production-ready job that can run automatically to keep our F1 data fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd2dd19-4e35-42ff-9de4-95380305a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 Step 1: Create Job-Ready Data Table\n",
    "\n",
    "First, let's create a table that our job will refresh daily with the latest F1 driver standings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "575d4178-d083-4431-a6ed-13b2788c0999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table for daily driver standings refresh\n",
    "CREATE OR REPLACE TABLE main.default.job_driver_standings_daily\n",
    "USING DELTA\n",
    "COMMENT 'Daily refreshed driver standings - maintained by automated job'\n",
    "AS\n",
    "SELECT \n",
    "  driverId,\n",
    "  full_name,\n",
    "  nationality,\n",
    "  total_career_points,\n",
    "  wins,\n",
    "  podiums,\n",
    "  total_races,\n",
    "  points_per_race,\n",
    "  win_percentage,\n",
    "  -- Add job execution metadata\n",
    "  'manual_creation' as refresh_method,\n",
    "  current_timestamp() as last_updated,\n",
    "  current_user() as updated_by\n",
    "FROM main.default.gold_driver_standings\n",
    "ORDER BY total_career_points DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc80b20a-96fe-43c3-9c07-5d5740183861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify our job table was created\n",
    "SELECT \n",
    "  'job_driver_standings_daily' as table_name,\n",
    "  COUNT(*) as driver_count,\n",
    "  MAX(last_updated) as last_refresh,\n",
    "  MAX(updated_by) as last_updated_by\n",
    "FROM main.default.job_driver_standings_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec36421c-c421-4a35-8e43-b2ceed4aed2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📝 Step 2: Create Job Execution Log Table\n",
    "\n",
    "Good production jobs always log their execution for monitoring and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa93fd47-9f6c-4560-9cd3-652d9f5bda8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create job execution log table\n",
    "CREATE OR REPLACE TABLE main.default.job_run_log\n",
    "(\n",
    "  job_run_id STRING,\n",
    "  job_name STRING,\n",
    "  start_time TIMESTAMP,\n",
    "  end_time TIMESTAMP,\n",
    "  status STRING,\n",
    "  records_processed BIGINT,\n",
    "  error_message STRING,\n",
    "  execution_user STRING,\n",
    "  execution_details MAP<STRING, STRING>\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Job execution tracking and monitoring log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "909cc592-d81b-47c1-894f-55e195998163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 Step 3: Build Job Logic with Logging\n",
    "\n",
    "This is the core logic that our scheduled job will execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbe52b1c-2a16-42d9-8ca2-39862eec77b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Job execution function with comprehensive logging\n",
    "def refresh_driver_standings_job():\n",
    "    \"\"\"\n",
    "    Refreshes the driver standings table and logs execution details.\n",
    "    This function will be called by our scheduled job.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate unique job run ID\n",
    "    job_run_id = str(uuid.uuid4())\n",
    "    job_name = \"daily_driver_standings_refresh\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"🚀 Starting job: {job_name}\")\n",
    "    print(f\"📝 Job Run ID: {job_run_id}\")\n",
    "    print(f\"⏰ Start Time: {start_time}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Refresh the driver standings data\n",
    "        print(\"📊 Refreshing driver standings data...\")\n",
    "        \n",
    "        # Get current record count before refresh\n",
    "        old_count = spark.sql(\"SELECT COUNT(*) as count FROM main.default.job_driver_standings_daily\").collect()[0].count\n",
    "        \n",
    "        # Refresh with latest data from gold layer\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE main.default.job_driver_standings_daily\n",
    "            USING DELTA\n",
    "            AS\n",
    "            SELECT \n",
    "              driverId,\n",
    "              full_name,\n",
    "              nationality,\n",
    "              total_career_points,\n",
    "              wins,\n",
    "              podiums,\n",
    "              total_races,\n",
    "              points_per_race,\n",
    "              win_percentage,\n",
    "              'automated_job_refresh' as refresh_method,\n",
    "              current_timestamp() as last_updated,\n",
    "              current_user() as updated_by\n",
    "            FROM main.default.gold_driver_standings\n",
    "            ORDER BY total_career_points DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        # Get new record count\n",
    "        new_count = spark.sql(\"SELECT COUNT(*) as count FROM main.default.job_driver_standings_daily\").collect()[0].count\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"✅ Job completed successfully!\")\n",
    "        print(f\"📊 Records processed: {new_count}\")\n",
    "        print(f\"⏱️ Duration: {duration:.2f} seconds\")\n",
    "        \n",
    "        # Log successful execution\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO main.default.job_run_log VALUES (\n",
    "                '{job_run_id}',\n",
    "                '{job_name}',\n",
    "                timestamp('{start_time}'),\n",
    "                timestamp('{end_time}'),\n",
    "                'SUCCESS',\n",
    "                {new_count},\n",
    "                NULL,\n",
    "                current_user(),\n",
    "                map('duration_seconds', '{duration:.2f}', 'old_count', '{old_count}', 'new_count', '{new_count}')\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        return {\"status\": \"SUCCESS\", \"records_processed\": new_count, \"duration\": duration}\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = datetime.now()\n",
    "        error_message = str(e)\n",
    "        \n",
    "        print(f\"❌ Job failed: {error_message}\")\n",
    "        \n",
    "        # Log failed execution\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO main.default.job_run_log VALUES (\n",
    "                '{job_run_id}',\n",
    "                '{job_name}',\n",
    "                timestamp('{start_time}'),\n",
    "                timestamp('{end_time}'),\n",
    "                'FAILED',\n",
    "                0,\n",
    "                '{error_message}',\n",
    "                current_user(),\n",
    "                map('error_type', 'execution_error')\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        raise e\n",
    "\n",
    "# Test our job function\n",
    "print(\"🧪 Testing job execution...\")\n",
    "result = refresh_driver_standings_job()\n",
    "print(f\"🎯 Job test result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d17a951-8c02-4b76-bdb2-26d1fed8496c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check our job execution log\n",
    "SELECT \n",
    "  job_name,\n",
    "  start_time,\n",
    "  end_time,\n",
    "  status,\n",
    "  records_processed,\n",
    "  execution_details\n",
    "FROM main.default.job_run_log\n",
    "ORDER BY start_time DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b901b517-4ee5-43e9-9d45-3e44ecaf29d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🏗️ Step 4: Complete Job Creation Guide\n",
    "\n",
    "Now let's learn how to create an automated job in the Databricks workspace.\n",
    "\n",
    "### 📋 Job Creation Steps:\n",
    "\n",
    "#### 1. Navigate to Workflows 🔄\n",
    "- Click **\"Workflows\"** in the left sidebar\n",
    "- Click **\"Create Job\"** button\n",
    "- You'll see the job configuration interface\n",
    "\n",
    "#### 2. Configure Basic Job Settings ⚙️\n",
    "```\n",
    "Job Name: \"F1 Driver Standings Daily Refresh\"\n",
    "Description: \"Automated daily refresh of F1 driver standings data\"\n",
    "```\n",
    "\n",
    "#### 3. Add Job Task 📝\n",
    "- **Task Name:** `refresh_driver_standings`\n",
    "- **Type:** `Notebook`\n",
    "- **Source:** Select this notebook (`04_Job_Creation.ipynb`)\n",
    "- **Cluster:** Choose `Serverless` compute\n",
    "\n",
    "#### 4. Set Schedule ⏰\n",
    "- **Trigger Type:** `Scheduled`\n",
    "- **Schedule:** `0 6 * * *` (Daily at 6 AM)\n",
    "- **Timezone:** Your local timezone\n",
    "\n",
    "#### 5. Configure Notifications 📧\n",
    "- **On Success:** Email notification (optional)\n",
    "- **On Failure:** Email + Slack alert (recommended)\n",
    "- **Recipients:** Your email or team distribution list\n",
    "\n",
    "#### 6. Advanced Options 🎛️\n",
    "- **Max Concurrent Runs:** `1` (prevent overlapping executions)\n",
    "- **Timeout:** `30 minutes` (reasonable for this job)\n",
    "- **Retry Policy:** `Retry 2 times with 5 minute intervals`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09a7c6d6-64ff-4ea7-a5d4-d36f66ca247c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Common Job Configuration Examples\n",
    "\n",
    "Here are some typical scheduling patterns for different business needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2261cf37-59c2-497f-ae50-6b87b05e7de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"⏰ Common Job Scheduling Patterns\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "schedules = {\n",
    "    \"Every Hour\": \"0 * * * *\",\n",
    "    \"Daily at 6 AM\": \"0 6 * * *\", \n",
    "    \"Daily at Midnight\": \"0 0 * * *\",\n",
    "    \"Weekly on Monday\": \"0 6 * * 1\",\n",
    "    \"Monthly on 1st\": \"0 6 1 * *\",\n",
    "    \"Business Days Only\": \"0 6 * * 1-5\",\n",
    "    \"Every 15 minutes\": \"*/15 * * * *\",\n",
    "    \"Twice Daily\": \"0 6,18 * * *\"\n",
    "}\n",
    "\n",
    "for description, cron in schedules.items():\n",
    "    print(f\"📅 {description:<20} {cron}\")\n",
    "\n",
    "print(\"\\n💡 Cron format: minute hour day month day-of-week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc3ed1-c7ea-4144-b757-332954e5a54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 Job Monitoring and Troubleshooting\n",
    "\n",
    "### 🔍 Monitoring Your Jobs:\n",
    "\n",
    "#### Job Run History 📈\n",
    "- **View runs:** Workflows → Your Job → \"Runs\" tab\n",
    "- **Check status:** SUCCESS, FAILED, RUNNING, CANCELED\n",
    "- **View logs:** Click on any run to see detailed logs\n",
    "- **Performance:** Check duration trends over time\n",
    "\n",
    "#### Common Job Issues & Solutions 🔧\n",
    "\n",
    "| **Issue** | **Symptoms** | **Solution** |\n",
    "|-----------|-------------|-------------|\n",
    "| **Timeout** | Job runs too long | Optimize queries, increase timeout |\n",
    "| **Cluster startup** | Slow job start | Use Serverless compute |\n",
    "| **Data skew** | Uneven task performance | Repartition data, optimize joins |\n",
    "| **Memory errors** | OOM exceptions | Increase cluster size, optimize code |\n",
    "| **Dependencies** | Missing tables/files | Check data availability, add retries |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec842855-ccd0-4b29-9ea5-6df1e7a29653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a job monitoring query\n",
    "print(\"📊 Job Performance Monitoring\")\n",
    "print(\"=\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c174d7-92f4-4417-b248-4cf1c5946401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Job performance monitoring query\n",
    "SELECT \n",
    "  job_name,\n",
    "  status,\n",
    "  COUNT(*) as run_count,\n",
    "  AVG(CAST(execution_details['duration_seconds'] AS DOUBLE)) as avg_duration_seconds,\n",
    "  MAX(end_time) as last_run,\n",
    "  SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_runs,\n",
    "  SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed_runs,\n",
    "  ROUND(SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as success_rate_pct\n",
    "FROM main.default.job_run_log\n",
    "GROUP BY job_name, status\n",
    "ORDER BY last_run DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200ac691-6795-4593-b56a-85aa403b4e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 Advanced Job Patterns\n",
    "\n",
    "### Multi-Step Workflows 🔗\n",
    "\n",
    "For complex data pipelines, you can create jobs with multiple tasks:\n",
    "\n",
    "```\n",
    "📥 Task 1: Data Ingestion\n",
    "    ↓\n",
    "🔄 Task 2: Data Transformation  \n",
    "    ↓\n",
    "📊 Task 3: Generate Reports\n",
    "    ↓\n",
    "📧 Task 4: Send Notifications\n",
    "```\n",
    "\n",
    "### Job Dependencies 🔗\n",
    "- **Sequential:** Tasks run one after another\n",
    "- **Parallel:** Multiple tasks run simultaneously  \n",
    "- **Conditional:** Tasks run based on previous results\n",
    "\n",
    "### Resource Management 💰\n",
    "- **Serverless:** Recommended for most jobs (auto-scaling)\n",
    "- **Shared clusters:** Cost-effective for multiple small jobs\n",
    "- **Dedicated clusters:** High-performance critical workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b77ce3-c48e-4931-9f58-e46b27f4515f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example of a more complex job function with multiple steps\n",
    "def multi_step_etl_job():\n",
    "    \"\"\"\n",
    "    Example of a complex ETL job with multiple steps and error handling.\n",
    "    \"\"\"\n",
    "    job_run_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Data validation\n",
    "        print(\"🔍 Step 1: Validating source data...\")\n",
    "        validation_result = spark.sql(\"\"\"\n",
    "            SELECT COUNT(*) as count \n",
    "            FROM main.default.silver_drivers \n",
    "            WHERE full_name IS NOT NULL\n",
    "        \"\"\").collect()[0].count\n",
    "        \n",
    "        if validation_result == 0:\n",
    "            raise Exception(\"No valid driver data found\")\n",
    "            \n",
    "        # Step 2: Data processing\n",
    "        print(\"⚙️ Step 2: Processing data transformations...\")\n",
    "        # (Your transformation logic here)\n",
    "        \n",
    "        # Step 3: Data quality checks\n",
    "        print(\"✅ Step 3: Running data quality checks...\")\n",
    "        # (Your quality check logic here)\n",
    "        \n",
    "        # Step 4: Update production tables\n",
    "        print(\"📊 Step 4: Updating production tables...\")\n",
    "        # (Your table update logic here)\n",
    "        \n",
    "        print(\"🎉 Multi-step ETL job completed successfully!\")\n",
    "        return {\"status\": \"SUCCESS\", \"steps_completed\": 4}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Multi-step ETL job failed: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "print(\"🧪 Example multi-step job structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d92e4624-19cf-45ec-a3d3-45e687782f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Job Creation Complete!\n",
    "\n",
    "**🎉 Excellent! You've learned how to create production-ready automated jobs!**\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ **Created job-ready data table** for daily driver standings\n",
    "- ✅ **Built execution logging** for monitoring and debugging\n",
    "- ✅ **Developed job function** with comprehensive error handling\n",
    "- ✅ **Learned job configuration** (scheduling, notifications, monitoring)\n",
    "- ✅ **Explored advanced patterns** (multi-step workflows, dependencies)\n",
    "\n",
    "### 🔄 Your Job Architecture:\n",
    "```\n",
    "⏰ Schedule (Daily 6 AM)\n",
    "    ↓\n",
    "🔄 refresh_driver_standings_job()\n",
    "    ↓\n",
    "📊 job_driver_standings_daily (Updated)\n",
    "    ↓\n",
    "📝 job_run_log (Execution tracked)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d14951c-b309-4368-bfe2-ed9f027d6636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final verification of our job-ready components\n",
    "print(\"⏰ Job Creation Summary\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check our job table\n",
    "job_table_count = spark.sql(\"SELECT COUNT(*) as count FROM main.default.job_driver_standings_daily\").collect()[0].count\n",
    "print(f\"📊 Driver standings table: {job_table_count:,} records\")\n",
    "\n",
    "# Check our log table  \n",
    "log_count = spark.sql(\"SELECT COUNT(*) as count FROM main.default.job_run_log\").collect()[0].count\n",
    "print(f\"📝 Job execution logs: {log_count} entries\")\n",
    "\n",
    "print(f\"\\n✅ Job components ready for scheduling!\")\n",
    "print(f\"🎯 Next: Create your job in Workflows → Create Job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b02edc-fce5-455a-8bbd-100b6c983bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Ready to explore more advanced data engineering features?\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **🔄 Create Your Job:** \n",
    "   - Go to Workflows → Create Job\n",
    "   - Follow the configuration guide above\n",
    "   - Schedule your first automated refresh!\n",
    "\n",
    "2. **➡️ Next Notebook:** [05_Delta_Live_Pipeline.ipynb](05_Delta_Live_Pipeline.ipynb)\n",
    "   - Learn about managed ETL pipelines\n",
    "   - Declarative data transformations\n",
    "   - Built-in data quality expectations\n",
    "\n",
    "3. **📊 Monitor Your Jobs:**\n",
    "   - Check the job_run_log table regularly\n",
    "   - Set up email notifications for failures\n",
    "   - Monitor job performance trends\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- **🧪 Test thoroughly** before scheduling in production\n",
    "- **📧 Set up alerts** for job failures (early detection is key)\n",
    "- **📊 Monitor performance** to optimize job runtime\n",
    "- **🔄 Use retries** for transient failures\n",
    "- **📝 Log everything** for easier debugging\n",
    "\n",
    "**⏰ Time to automate your data pipelines! 🚀**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Job_Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
