# Databricks 1.5-Hour Intro Workshop - Project Requirements

## 🎯 Project Overview

**Goal:** Create 10 focused Databricks notebooks/files for a hands-on workshop using Formula 1 data.

**Duration:** 1.5 hours (30 min presentation + 45 min demo + 15 min Q&A)

**Environment:** Databricks trial workspace, `main.default` catalog

**Data Source:** https://github.com/toUpperCase78/formula1-datasets

## Github layout
- Images will be stored in the images folder
- There will be placeholders in the notebooks for now
- The images file name will be clear for the LLM to know what is being referenced and where it should go

## 📊 Cell Optimization Summary

Each notebook is designed with a target cell count under 20, prioritizing:
- **Information density:** Rich markdown cells with visual formatting
- **Conceptual grouping:** Related operations in single cells
- **Visual hierarchy:** Using headers and emojis for structure
- **Strategic breaks:** New cells only at meaningful boundaries
- **Practical balance:** Readability vs. concise presentation

## Core Principles

- **Languages:** Python and SQL only (no Scala or R)
- **Structure:** One topic per notebook
- **Approach:** SQL-first where appropriate
- **Patterns:** Use Volumes and COPY INTO (real-world)
- **Simplicity:** Anyone can follow
- **20 Cell Limit:** Keep all notebooks under 20 cells to maintain clarity and manageability

---

## 📚 Deliverables: 11 Files

```
notebooks/
├── 00_Setup.ipynb                      (5 min - prerequisite data setup)
├── 01_Platform_Tour.ipynb              (5 min - navigation guide)
├── 02_Databricks_Notebook_Tour.ipynb   (5 min - notebook fundamentals)
├── 03_Medallion Architecture.ipynb     (15 min - complete Bronze→Silver→Gold pipeline)
├── 04_Unity_Catalog.ipynb              (5 min - governance & lineage)
├── 05_Job_Creation.ipynb               (3 min - scheduling workflows)
├── 06_Declarative_Pipeline.ipynb       (5 min - managed ETL)
├── 07_SQL_Editor.sql                   (10 min - analytics queries)
├── 08_Formula_1_Dashboard.lvdash.json  (reference - dashboard template)
├── 09_Genie_Room.ipynb                 (3 min - natural language queries)
├── 10_Agent_Bricks.ipynb               (3 min - AI overview)
├── 11_Databricks_One.ipynb             (3 min - business user interface)
```

---

## 📓 Notebook Specifications

### 01_Platform_Tour.ipynb
**Type:** Markdown guide  
**Content:**
- Left sidebar navigation (Workspace, Catalog, Compute, Workflows)
- Data ingestion options (Lakehouse Federation, Fivetran, Partner Connect, Custom Scripts)
- ML features overview (Experiments, Models, Feature Store, AutoML)
- SQL Editor & Analytics intro
- Screenshot placeholders throughout

**Cell Optimization (Target: 6 cells):**
1. Title and introduction with learning objectives
2. Left sidebar navigation features
3. Data ingestion options overview
4. ML features explanation
5. SQL Editor & Analytics introduction
6. Conclusion and next steps

### 02_Notebook_Tour.ipynb
**Type:** Jupyter notebook (CRITICAL - Main Setup)  
**Content Structure:**
1. Serverless compute setup instructions
2. Multi-language demo (Python + SQL cells)
3. Create Volume: `main.default.f1_raw_data`
4. Download 3 CSVs (races, drivers, results) to Volume
5. **Bronze Layer:** Load CSVs using COPY INTO
   - `bronze_races`
   - `bronze_drivers`
   - `bronze_results`
6. **Silver Layer:** Clean and validate
   - `silver_races` (date types, filtering)
   - `silver_drivers` (full names, date types)
   - `silver_results` (position cleanup)
7. **Gold Layer:** Analytics tables
   - `gold_driver_standings` (aggregated stats)
   - `gold_season_stats` (yearly metrics)
8. Table operations demo (TABLE, VIEW, MATERIALIZED VIEW)
9. Auto Loader example (streaming ingestion)
10. Databricks Assistant usage guide

**Cell Optimization (Target: 14-16 cells):**
1. Introduction and learning objectives
2-3. Basic Python and SQL demo cells
4-5. Volume creation and data download
6-8. Bronze layer table creation and population (3 tables)
9-11. Silver layer transformations (3 tables)
12-13. Gold layer analytics (2 tables)
14. Table operations demonstration
15. Conclusion and next steps

**Key Requirements:**
- Proper Jupyter notebook format with markdown and code cells
- Uses `requests` library for CSV downloads
- COPY INTO pattern for all Bronze loads
- SQL cells for data transformations
- Complete medallion architecture (8 tables total)
- All code must be runnable

### 00_Setup_Data.ipynb
**Type:** Jupyter notebook
**Content:**
- Volume creation
- Download F1 dataset files
- Save to volume
- Verify files exist

**Cell Optimization (Target: 3 cells):**
1. Introduction markdown
2. Python code for downloading and verifying data
3. Next steps guidance

### 03_Unity_Catalog_Demo.ipynb
**Type:** Jupyter notebook  
**Content:**
- Unity Catalog overview (3-level namespace, governance)
- Create 5 lineage demo tables with dependencies:
  1. `lineage_drivers_source`
  2. `lineage_results_source`
  3. `lineage_driver_performance` (joins sources)
  4. `lineage_career_stats` (aggregates)
  5. `lineage_championship_tiers` (classifies)
- Instructions for viewing lineage in Catalog UI
- Unity Catalog features overview
- Best practices for organization and security

**Cell Optimization (Target: 14 cells):**
1. Introduction with learning objectives
2-3. Table exploration and discovery
4. Lineage demo tables explanation
5-7. Source tables creation (2 tables)
8-10. Transformation and aggregation tables (3 tables)
11. Lineage visualization with diagram
12. Sample query for classification results
13. Governance features explanation
14. Conclusion and next steps

### 04_Job_Creation.ipynb
**Type:** Jupyter notebook  
**Content:**
- Simple job task: refresh `job_driver_standings_daily`
- Create `job_run_log` table
- Log execution with timestamp and status
- Complete job creation guide:
  - Navigate to Workflows
  - Configure job settings
  - Set schedule (examples)
  - Notifications setup
  - Advanced options
- Common job patterns (ETL, multi-source, ML)
- Troubleshooting guide

**Cell Optimization (Target: 10-12 cells):**
1. Combined markdown introduction with learning objectives
2. Single SQL cell to create both job tables
3. One comprehensive Python cell for job execution and logging
4. Consolidated markdown for job creation steps with screenshots
5. Single visual guide for different scheduling options
6. Merged patterns and best practices into one rich markdown cell

### 05_Delta_Live_Pipeline.ipynb
**Type:** DLT Jupyter notebook  
**Content:**
- DLT decorators and patterns
- **Bronze:** Ingest drivers and results from Volume
- **Silver:** Clean with data quality expectations
  - `@dlt.expect_or_drop` for critical fields
  - `@dlt.expect` for warnings
- **Gold:** Aggregated analytics
  - `dlt_gold_driver_stats`
  - `dlt_gold_top_performers`
- DLT pipeline creation guide
- Features: expectations, dependencies, CDC, streaming
- DLT vs Jobs comparison
- Best practices

**Cell Optimization (Target: 14-16 cells):**
1. Combined introduction and DLT overview into one rich markdown cell
2. One Python cell showing all DLT import statements and utilities
3. Bronze layer ingestion functions in a single cell with clear comments
4. Consolidated all data quality expectations in one cell
5. Silver layer transformations in a single comprehensive cell
6. Gold layer aggregations in one cell with multiple tables
7. Multi-part pipeline setup guide with ASCII flow diagram
8. Single markdown cell for features and comparison with Jobs

### 06_AI_Agent_Bricks.ipynb
**Type:** Markdown guide  
**Content:**
- AI Agents overview (what they are, why use them)
- Key components (Vector Search, Foundation Models, Agent Framework, Playground)
- Simple F1 Q&A bot example
- Building steps (prepare data, create index, configure agent, test, deploy)
- Agent types (SQL, RAG, Function Calling, Multi-Agent)
- Use cases and examples
- Monitoring and best practices
- Screenshot placeholders

**Cell Optimization (Target: 8-10 cells):**
1. Comprehensive introduction with objectives and AI Agent concept
2. Single rich markdown cell for all key components with visual hierarchy
3. F1 Q&A bot example with code snippets in one markdown cell
4. Building steps as a single visual workflow markdown cell
5. Agent types and capabilities in a consolidated formatted cell
6. Combined use cases with real-world examples in one cell
7. Single cell for best practices and monitoring guidance

### 07_SQL_Editor.sql
**Type:** SQL file  
**Content:**
- SQL Warehouse setup guide
- **8 analytical queries:**
  1. Top 10 all-time drivers
  2. F1 growth over time
  3. National F1 success
  4. Driver career tiers
  5. Recent F1 activity
  6. Win concentration analysis
  7. Decade-by-decade comparison
  8. High-performance drivers
- Visualization suggestions for each query (chart types)
- Dashboard creation guide (step-by-step)
- Dashboard best practices (layout, performance, design, filters)
- Advanced SQL features (window functions, CTEs, pivoting)

**Cell Optimization (Target: 10-12 cells):**
1. Single comprehensive introduction to SQL analytics
2. Consolidated SQL Warehouse setup section
3. Group related queries by theme (e.g., drivers + performance, growth + trends)
4. Each SQL query includes visualization guidance in comments
5. Advanced SQL features demonstrated within the main queries
6. Single dashboard creation guide with visual layout example
7. Combined best practices for query performance and visualization

### 08_Dashboard_Placeholder.ipynb
**Type:** Markdown reference  
**Content:**
- Dashboard layout specification (4 rows)
- KPI cards for Row 1
- Main visualizations for Rows 2-3
- Trends for Row 4
- Import instructions placeholder
- Dashboard features (filters, refresh, permissions)
- Best practices summary

**Cell Optimization (Target: 6-8 cells):**
1. Combined introduction and dashboard purpose in one cell
2. Single visual dashboard layout specification with row descriptions
3. Comprehensive visualization guidance with examples in one cell
4. Dashboard features and interactivity in a consolidated cell
5. Import and sharing instructions in a single workflow cell
6. Best practices and performance tips in one formatted cell

### 09_Genie_Room.ipynb
**Type:** Markdown guide  
**Content:**
- Genie overview (natural language analytics)
- Genie Space creation guide (5 steps)
- Example questions by category:
  - Basic queries
  - Statistical analysis
  - Trends and patterns
  - Complex queries
- How Genie works (flow diagram)
- Features (conversation history, SQL transparency, visualizations, follow-ups)
- Best practices for better questions
- Teaching Genie about your data
- Genie vs SQL Editor vs Notebooks comparison
- Limitations and troubleshooting

**Cell Optimization (Target: 8-10 cells):**
1. Single rich introduction with Genie concept and value proposition
2. Comprehensive Genie Space creation guide in one visual workflow cell
3. Example questions grouped by complexity in a structured markdown cell
4. How Genie works with architecture diagram in a consolidated cell
5. Features and capabilities in one well-formatted cell
6. Best practices for effective queries combined with teaching tips
7. Tool comparison table in a single visual cell
8. Limitations and troubleshooting in one practical guidance cell

### 10_Databricks_One.ipynb
**Type:** Markdown guide  
**Content:**
- Databricks One overview (vs Genie)
- Access methods (keyboard shortcuts, UI button, inline help)
- **5 main capabilities:**
  1. Code generation
  2. Code explanation
  3. Debugging help
  4. Best practices guidance
  5. Documentation lookup
- Usage in notebooks (4 scenarios)
- Usage in SQL Editor
- Best practices for prompts
- Integration with platform features
- Example conversations (3 detailed)
- Advanced features (context awareness, learning mode, multi-turn)
- Privacy and security
- Comparison table (One vs Assistant vs Genie)
- Getting started exercises

**Cell Optimization (Target: 10-12 cells):**
1. Combined introduction and overview with access methods
2. Five main capabilities in a single visually structured cell
3. Usage examples for notebooks consolidated by theme
4. SQL Editor usage with examples in one comprehensive cell
5. Best practices for prompts in a single formatted guide
6. Integration features in one consolidated cell
7. Example conversations grouped in a single demonstration cell
8. Advanced features in one structured explanation cell
9. Privacy/security and tool comparison in a single informational cell
10. Getting started exercises as a single call-to-action cell

---

## 📋 Technical Requirements

### Data Pipeline (Notebooks 00 & 02)
```
Notebook 00: Setup
Volume Creation
↓
Download 3 CSVs from GitHub
↓
Save to Volume (/Volumes/main/default/f1_raw_data/)
↓
Create DLT schema directories
↓
Verify data and preview schemas

Notebook 02: Processing
Bronze Layer (COPY INTO)
- bronze_races
- bronze_drivers  
- bronze_results
↓
Silver Layer (cleaned)
- silver_races
- silver_drivers
- silver_results
↓
Gold Layer (analytics)
- gold_driver_standings
- gold_season_stats
```

### Code Standards
- **Python:** PEP 8 style, clear variable names
- **SQL:** Readable formatting, comments for complex logic
- **Magic commands:** Use `%sql` and `%md` appropriately
- **Error handling:** Include where necessary
- **Comments:** Explain why, not what
- **Consistency:** Maintain same coding style and patterns across all notebooks
- **Cell density:** Balance readability with cell count by combining related operations
- **Visual feedback:** Use print statements with status indicators (✓, ❌) for clear operation status

### Data Quality
- All Bronze loads must succeed
- Silver transformations handle nulls and invalid data
- Gold tables have proper aggregations
- Test queries included for validation

---

## 🎯 Success Criteria

### Must Have
✅ All 10 files created with specified content  
✅ Notebook 02 builds complete medallion in `main.default`  
✅ Uses Volume + COPY INTO pattern (not just `spark.read`)  
✅ Python + SQL only (no Scala/R)  
✅ All code is executable in fresh trial workspace  
✅ Clear placeholders for screenshots  
✅ README with trial signup link and instructions  

### Content Quality
✅ Simple explanations (non-technical users can follow)  
✅ Real-world patterns and best practices  
✅ Working code in every executable notebook  
✅ Clear progression through topics  
✅ Troubleshooting guidance included  

### Workshop Flow
✅ Notebooks numbered 01-10 for clear order  
✅ Each builds on previous concepts  
✅ Total runtime under 45 minutes for demo  
✅ Can be run independently if needed  

---

## 📖 README Requirements

### Sections to Include
1. **Quick Start** (trial signup + clone steps)
2. **What You'll Build** (architecture diagram)
3. **Skills You'll Learn** (bulleted list)
4. **Workshop Structure** (timing breakdown)
5. **Prerequisites** (minimal)
6. **Notebook Table** (all 10 with durations)
7. **Resources** (links to docs, training, community)
8. **Troubleshooting** (common issues + solutions)
9. **Data Source** (GitHub link)
10. **License** (educational use)

### Key Elements
- Prominent trial signup link at top
- 3-step getting started (signup, clone, run)
- Clear troubleshooting for common issues
- Links to Databricks resources
- Professional but friendly tone

---

## 🔧 Implementation Notes

### For Notebook 02 (Critical)
- Must use proper Jupyter notebook format (.ipynb)
- Must create Volume first
- Use `requests.get()` for CSV downloads
- Write files to `/Volumes/main/default/f1_raw_data/`
- COPY INTO syntax must be correct for CSV
- Include FORMAT_OPTIONS and COPY_OPTIONS
- Verify data loaded with COUNT queries
- All transformations in SQL cells where possible
- Proper markdown cells for explanations and structure

### For Screenshot Placeholders
Use this format consistently:
```markdown
**[Screenshot: Description of what will be shown]**
```

### For Code Blocks in Markdown
Use triple backticks with language:
```markdown
```python
# Python code
```

```sql
-- SQL code
```
```

### Testing Checklist
- [ ] Fresh trial workspace can run Notebook 02 start to finish
- [ ] All 8 tables created in `main.default`
- [ ] CSV files download successfully
- [ ] COPY INTO statements execute without errors
- [ ] Lineage visible in Catalog UI
- [ ] SQL queries in 07 return results
- [ ] DLT notebook has correct decorator syntax

---

## 📦 File Outputs

When complete, repository should contain:

```
/
├── README.ipynb
├── notebooks/
│   ├── 01_Platform_Tour.ipynb
│   ├── 02_Notebook_Tour.ipynb
│   ├── 03_Unity_Catalog_Demo.ipynb
│   ├── 04_Job_Creation.ipynb
│   ├── 05_Delta_Live_Pipeline.ipynb
│   ├── 06_AI_Agent_Bricks.ipynb
│   ├── 07_SQL_Editor.sql
│   ├── 08_Dashboard_Placeholder.ipynb
│   ├── 09_Genie_Room.ipynb
│   └── 10_Databricks_One.ipynb
└── .gitignore (optional)
```

All Jupyter notebooks (.ipynb) should be properly formatted with markdown and code cells 
for optimal experience in Databricks workspace.

---

## 📋 Notebook Format Requirements

### Proper Jupyter Notebook Structure
- All notebooks must have proper nbformat fields (nbformat=4, nbformat_minor=5)
- Must include metadata with kernelspec and language_info sections
- Structure must follow standard Jupyter format with cell metadata 

### SQL vs Python Usage
- Use native SQL cells (%sql) for SQL operations rather than spark.sql() in Python
- Use Python cells for complex logic (file operations, display functions, etc.)
- Create clear separation between languages based on strengths
- Maintain readability by using each language for its appropriate use case

### Multi-Language Showcase
- Demonstrate Databricks' multi-language capabilities
- Use %sql magic for SQL cells
- Use %md magic for markdown cells when appropriate
- Show proper transitions between languages

### Streamlining Content
- Focus on core concepts rather than complex data engineering
- Keep notebooks simple and educational
- Remove unnecessary print statements, use comments instead
- Ensure content is appropriate for the target audience and time constraints

---

## 📝 Best Practices Learned

### 🎨 Notebook Design & Structure
- **Clear introductions:** Start each notebook with a markdown title, brief description, and learning objectives
- **Visual organization:** Use emojis and formatting consistently to create visual hierarchy
- **Progress indicators:** End each notebook with a "Complete" section showing what was learned and next steps
- **Logical flow:** Structure content as a journey from simple to complex concepts
- **Clean outputs:** Include verification steps after major operations (e.g., table counts, data samples)

### 💻 Code Quality & Readability
- **SQL best practices:**
  - Use proper column aliases with AS for clarity
  - Include table aliases in multi-table operations
  - Format complex CASE statements with indentation
  - Add comments before complex logic blocks
  - Use CREATE OR REPLACE for idempotent execution
- **Python best practices:**
  - Handle exceptions with specific error messages
  - Use f-strings for readable string formatting
  - Include status indicators (✓, ❌) for visual feedback
  - Create utility functions for repeated operations
  - Use descriptive variable names that explain purpose

### 📊 Data Visualization & Presentation
- **Rich markdown:** Utilize markdown formatting (headers, lists, code blocks, tables)
- **ASCII diagrams:** Show data flow with simple text-based diagrams when appropriate
- **Placeholder conventions:** Consistent format for screenshot placeholders
- **Verify operations:** Include COUNT, DESCRIBE, or preview queries after data transformations
- **Sample outputs:** Show example query results to demonstrate value

### 🔄 Data Pipeline Patterns
- **Simple setup files:** Keep setup notebooks focused on just data preparation
- **Modular approach:** Separate data acquisition from processing and analytics
- **Lineage demo:** Create clear parent-child table relationships to showcase governance
- **Table metadata:** Add descriptive comments to tables for discoverability
- **Naming conventions:** Use consistent prefixes (bronze_, silver_, gold_) across the medallion

### 🚀 Workshop Delivery
- **Next steps guidance:** Always include what to do next at the end of each notebook
- **Time-boxed sections:** Break content into 5-15 minute executable segments
- **Independent execution:** Ensure notebooks can run standalone when necessary
- **Consistent formatting:** Maintain the same style across all notebooks
- **Escalating complexity:** Start simple and build to more complex concepts

### 📏 Cell Optimization Strategies
- **Combine related markdown:** Merge sequential markdown cells that cover related topics
- **Multi-step code cells:** Include multiple logical steps in a single code cell when they're closely related
- **Rich markdown over multiple cells:** Use a single well-formatted markdown cell instead of multiple simple ones
- **Purposeful comments:** Use in-code comments rather than separate markdown cells for simple explanations
- **Consolidated SQL:** Combine multiple simple SQL operations into a single comprehensive cell
- **Focused verification:** Limit verification steps to essential checkpoints, not after every operation
- **Visual economy:** Use emojis, headers and formatting to convey structure without requiring additional cells
- **Streamlined introductions:** Combine objectives, overview, and prerequisites into a single rich markdown cell
- **Comprehensive examples:** Show one thorough example rather than multiple similar ones
- **Strategic cell breaks:** Only start a new cell when there's a meaningful conceptual boundary

---

**Status:** Ready for implementation  
**Next Step:** Create all 10 files according to specifications above  
**Timeline:** Can be built in parallel, test Notebook 02 first