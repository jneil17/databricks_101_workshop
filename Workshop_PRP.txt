# Databricks 1.5-Hour Intro Workshop - Project Requirements

## ðŸŽ¯ Project Overview

**Goal:** Create 10 focused Databricks notebooks/files for a hands-on workshop using Formula 1 data.

**Duration:** 1.5 hours (30 min presentation + 45 min demo + 15 min Q&A)

**Environment:** Databricks trial workspace, `main.default` catalog

**Data Source:** https://github.com/toUpperCase78/formula1-datasets

## Core Principles

- **Languages:** Python and SQL only (no Scala or R)
- **Structure:** One topic per notebook
- **Approach:** SQL-first where appropriate
- **Patterns:** Use Volumes and COPY INTO (real-world)
- **Simplicity:** Anyone can follow

---

## ðŸ“š Deliverables: 10 Files

```
notebooks/
â”œâ”€â”€ 01_Platform_Tour.ipynb           (5 min - navigation guide - markdown cells only)
â”œâ”€â”€ 02_Notebook_Tour.ipynb           (15 min - complete pipeline build)
â”œâ”€â”€ 03_Unity_Catalog_Demo.ipynb      (5 min - governance & lineage)
â”œâ”€â”€ 04_Job_Creation.ipynb            (3 min - scheduling workflows)
â”œâ”€â”€ 05_Delta_Live_Pipeline.ipynb     (5 min - managed ETL)
â”œâ”€â”€ 06_AI_Agent_Bricks.ipynb         (3 min - AI overview - markdown cells only)
â”œâ”€â”€ 07_SQL_Editor.sql                (10 min - analytics queries)
â”œâ”€â”€ 08_Dashboard_Placeholder.ipynb   (reference - dashboard template - markdown cells only)
â”œâ”€â”€ 09_Genie_Room.ipynb              (3 min - natural language queries - markdown cells only)
â””â”€â”€ 10_Databricks_One.ipynb          (3 min - platform AI assistant - markdown cells only)
```

---

## ðŸ““ Notebook Specifications

### 01_Platform_Tour.md
**Type:** Markdown guide  
**Content:**
- Left sidebar navigation (Workspace, Catalog, Compute, Workflows)
- Data ingestion options (Lakehouse Federation, Fivetran, Partner Connect, Custom Scripts)
- ML features overview (Experiments, Models, Feature Store, AutoML)
- SQL Editor & Analytics intro
- Screenshot placeholders throughout

### 02_Notebook_Tour.ipynb
**Type:** Jupyter notebook (CRITICAL - Main Setup)  
**Content Structure:**
1. Serverless compute setup instructions
2. Multi-language demo (Python + SQL cells)
3. Create Volume: `main.default.f1_raw_data`
4. Download 3 CSVs (races, drivers, results) to Volume
5. **Bronze Layer:** Load CSVs using COPY INTO
   - `bronze_races`
   - `bronze_drivers`
   - `bronze_results`
6. **Silver Layer:** Clean and validate
   - `silver_races` (date types, filtering)
   - `silver_drivers` (full names, date types)
   - `silver_results` (position cleanup)
7. **Gold Layer:** Analytics tables
   - `gold_driver_standings` (aggregated stats)
   - `gold_season_stats` (yearly metrics)
8. Table operations demo (TABLE, VIEW, MATERIALIZED VIEW)
9. Auto Loader example (streaming ingestion)
10. Databricks Assistant usage guide

**Key Requirements:**
- Proper Jupyter notebook format with markdown and code cells
- Uses `requests` library for CSV downloads
- COPY INTO pattern for all Bronze loads
- SQL cells for data transformations
- Complete medallion architecture (8 tables total)
- All code must be runnable

### 03_Unity_Catalog_Demo.ipynb
**Type:** Jupyter notebook  
**Content:**
- Unity Catalog overview (3-level namespace, governance)
- Create 5 lineage demo tables with dependencies:
  1. `lineage_drivers_source`
  2. `lineage_results_source`
  3. `lineage_driver_performance` (joins sources)
  4. `lineage_career_stats` (aggregates)
  5. `lineage_championship_tiers` (classifies)
- Instructions for viewing lineage in Catalog UI
- Unity Catalog features overview
- Best practices for organization and security

### 04_Job_Creation.ipynb
**Type:** Jupyter notebook  
**Content:**
- Simple job task: refresh `job_driver_standings_daily`
- Create `job_run_log` table
- Log execution with timestamp and status
- Complete job creation guide:
  - Navigate to Workflows
  - Configure job settings
  - Set schedule (examples)
  - Notifications setup
  - Advanced options
- Common job patterns (ETL, multi-source, ML)
- Troubleshooting guide

### 05_Delta_Live_Pipeline.ipynb
**Type:** DLT Jupyter notebook  
**Content:**
- DLT decorators and patterns
- **Bronze:** Ingest drivers and results from Volume
- **Silver:** Clean with data quality expectations
  - `@dlt.expect_or_drop` for critical fields
  - `@dlt.expect` for warnings
- **Gold:** Aggregated analytics
  - `dlt_gold_driver_stats`
  - `dlt_gold_top_performers`
- DLT pipeline creation guide
- Features: expectations, dependencies, CDC, streaming
- DLT vs Jobs comparison
- Best practices

### 06_AI_Agent_Bricks.md
**Type:** Markdown guide  
**Content:**
- AI Agents overview (what they are, why use them)
- Key components (Vector Search, Foundation Models, Agent Framework, Playground)
- Simple F1 Q&A bot example
- Building steps (prepare data, create index, configure agent, test, deploy)
- Agent types (SQL, RAG, Function Calling, Multi-Agent)
- Use cases and examples
- Monitoring and best practices
- Screenshot placeholders

### 07_SQL_Editor.sql
**Type:** SQL file  
**Content:**
- SQL Warehouse setup guide
- **8 analytical queries:**
  1. Top 10 all-time drivers
  2. F1 growth over time
  3. National F1 success
  4. Driver career tiers
  5. Recent F1 activity
  6. Win concentration analysis
  7. Decade-by-decade comparison
  8. High-performance drivers
- Visualization suggestions for each query (chart types)
- Dashboard creation guide (step-by-step)
- Dashboard best practices (layout, performance, design, filters)
- Advanced SQL features (window functions, CTEs, pivoting)

### 08_Dashboard_Placeholder.md
**Type:** Markdown reference  
**Content:**
- Dashboard layout specification (4 rows)
- KPI cards for Row 1
- Main visualizations for Rows 2-3
- Trends for Row 4
- Import instructions placeholder
- Dashboard features (filters, refresh, permissions)
- Best practices summary

### 09_Genie_Room.md
**Type:** Markdown guide  
**Content:**
- Genie overview (natural language analytics)
- Genie Space creation guide (5 steps)
- Example questions by category:
  - Basic queries
  - Statistical analysis
  - Trends and patterns
  - Complex queries
- How Genie works (flow diagram)
- Features (conversation history, SQL transparency, visualizations, follow-ups)
- Best practices for better questions
- Teaching Genie about your data
- Genie vs SQL Editor vs Notebooks comparison
- Limitations and troubleshooting

### 10_Databricks_One.md
**Type:** Markdown guide  
**Content:**
- Databricks One overview (vs Genie)
- Access methods (keyboard shortcuts, UI button, inline help)
- **5 main capabilities:**
  1. Code generation
  2. Code explanation
  3. Debugging help
  4. Best practices guidance
  5. Documentation lookup
- Usage in notebooks (4 scenarios)
- Usage in SQL Editor
- Best practices for prompts
- Integration with platform features
- Example conversations (3 detailed)
- Advanced features (context awareness, learning mode, multi-turn)
- Privacy and security
- Comparison table (One vs Assistant vs Genie)
- Getting started exercises

---

## ðŸ“‹ Technical Requirements

### Data Pipeline (Notebook 02)
```
Volume Creation
â†“
Download 3 CSVs from GitHub
â†“
Bronze Layer (COPY INTO)
- bronze_races
- bronze_drivers  
- bronze_results
â†“
Silver Layer (cleaned)
- silver_races
- silver_drivers
- silver_results
â†“
Gold Layer (analytics)
- gold_driver_standings
- gold_season_stats
```

### Code Standards
- **Python:** PEP 8 style, clear variable names
- **SQL:** Readable formatting, comments for complex logic
- **Magic commands:** Use `%sql` and `%md` appropriately
- **Error handling:** Include where necessary
- **Comments:** Explain why, not what

### Data Quality
- All Bronze loads must succeed
- Silver transformations handle nulls and invalid data
- Gold tables have proper aggregations
- Test queries included for validation

---

## ðŸŽ¯ Success Criteria

### Must Have
âœ… All 10 files created with specified content  
âœ… Notebook 02 builds complete medallion in `main.default`  
âœ… Uses Volume + COPY INTO pattern (not just `spark.read`)  
âœ… Python + SQL only (no Scala/R)  
âœ… All code is executable in fresh trial workspace  
âœ… Clear placeholders for screenshots  
âœ… README with trial signup link and instructions  

### Content Quality
âœ… Simple explanations (non-technical users can follow)  
âœ… Real-world patterns and best practices  
âœ… Working code in every executable notebook  
âœ… Clear progression through topics  
âœ… Troubleshooting guidance included  

### Workshop Flow
âœ… Notebooks numbered 01-10 for clear order  
âœ… Each builds on previous concepts  
âœ… Total runtime under 45 minutes for demo  
âœ… Can be run independently if needed  

---

## ðŸ“– README Requirements

### Sections to Include
1. **Quick Start** (trial signup + clone steps)
2. **What You'll Build** (architecture diagram)
3. **Skills You'll Learn** (bulleted list)
4. **Workshop Structure** (timing breakdown)
5. **Prerequisites** (minimal)
6. **Notebook Table** (all 10 with durations)
7. **Resources** (links to docs, training, community)
8. **Troubleshooting** (common issues + solutions)
9. **Data Source** (GitHub link)
10. **License** (educational use)

### Key Elements
- Prominent trial signup link at top
- 3-step getting started (signup, clone, run)
- Clear troubleshooting for common issues
- Links to Databricks resources
- Professional but friendly tone

---

## ðŸ”§ Implementation Notes

### For Notebook 02 (Critical)
- Must use proper Jupyter notebook format (.ipynb)
- Must create Volume first
- Use `requests.get()` for CSV downloads
- Write files to `/Volumes/main/default/f1_raw_data/`
- COPY INTO syntax must be correct for CSV
- Include FORMAT_OPTIONS and COPY_OPTIONS
- Verify data loaded with COUNT queries
- All transformations in SQL cells where possible
- Proper markdown cells for explanations and structure

### For Screenshot Placeholders
Use this format consistently:
```markdown
**[Screenshot: Description of what will be shown]**
```

### For Code Blocks in Markdown
Use triple backticks with language:
```markdown
```python
# Python code
```

```sql
-- SQL code
```
```

### Testing Checklist
- [ ] Fresh trial workspace can run Notebook 02 start to finish
- [ ] All 8 tables created in `main.default`
- [ ] CSV files download successfully
- [ ] COPY INTO statements execute without errors
- [ ] Lineage visible in Catalog UI
- [ ] SQL queries in 07 return results
- [ ] DLT notebook has correct decorator syntax

---

## ðŸ“¦ File Outputs

When complete, repository should contain:

```
/
â”œâ”€â”€ README.md
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Platform_Tour.md
â”‚   â”œâ”€â”€ 02_Notebook_Tour.ipynb
â”‚   â”œâ”€â”€ 03_Unity_Catalog_Demo.ipynb
â”‚   â”œâ”€â”€ 04_Job_Creation.ipynb
â”‚   â”œâ”€â”€ 05_Delta_Live_Pipeline.ipynb
â”‚   â”œâ”€â”€ 06_AI_Agent_Bricks.md
â”‚   â”œâ”€â”€ 07_SQL_Editor.sql
â”‚   â”œâ”€â”€ 08_Dashboard_Placeholder.md
â”‚   â”œâ”€â”€ 09_Genie_Room.md
â”‚   â””â”€â”€ 10_Databricks_One.md
â””â”€â”€ .gitignore (optional)
```

All Jupyter notebooks (.ipynb) should be properly formatted with markdown and code cells 
for optimal experience in Databricks workspace.

---

## ðŸ“‹ Notebook Format Requirements

### Proper Jupyter Notebook Structure
- All notebooks must have proper nbformat fields (nbformat=4, nbformat_minor=5)
- Must include metadata with kernelspec and language_info sections
- Structure must follow standard Jupyter format with cell metadata 

### SQL vs Python Usage
- Use native SQL cells (%sql) for SQL operations rather than spark.sql() in Python
- Use Python cells for complex logic (file operations, display functions, etc.)
- Create clear separation between languages based on strengths
- Maintain readability by using each language for its appropriate use case

### Multi-Language Showcase
- Demonstrate Databricks' multi-language capabilities
- Use %sql magic for SQL cells
- Use %md magic for markdown cells when appropriate
- Show proper transitions between languages

### Streamlining Content
- Focus on core concepts rather than complex data engineering
- Keep notebooks simple and educational
- Remove unnecessary print statements, use comments instead
- Ensure content is appropriate for the target audience and time constraints

---

**Status:** Ready for implementation  
**Next Step:** Create all 10 files according to specifications above  
**Timeline:** Can be built in parallel, test Notebook 02 first